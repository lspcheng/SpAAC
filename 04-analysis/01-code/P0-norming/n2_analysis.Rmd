---
title: "n2"
output: html_document
---

# Preamble
## Packages
```{r setup}
# Load libraries and custom functions
if (file.exists("project_functions.R")){
  source("project_functions.R")
  
} else { # try one directory up
  source("../project_functions.R")
}

# Text Analysis
library(countrycode)
library(maps)
library(tidytext)

# Plots
# library(hrbrthemes)
```

## Pipeline Structure
```{r}
# Fill in file structure info (e.g. using getwd())
NAME <- 'n2_analysis' ## Name of the R file (w/o file extension!)
PHASE <- 'P0-norming' ## Name of the project phase (if relevant)
PROJECT <- 'SpAAC' ## Name of project
```

```{r}
# Get project directory path & subfolder status from working dir
PROJECT_DIR <- str_extract(getwd(), paste0("^(.*?)",PROJECT,"/"))

if (basename(getwd()) != PHASE) {SUBFOLDER <- basename(getwd())} else {SUBFOLDER <- NA}

# Get pipeline path names
if (dir.exists(file.path(PROJECT_DIR, '04-analysis', '02-pipeline'))){
  if (is.na(SUBFOLDER)){
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, NAME)
  } else {
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, SUBFOLDER, NAME)
  }
} else {
  pipeline <- file.path('.', 'pipeline', PHASE, NAME)
}

# Create pipeline folders
if (!dir.exists(pipeline)) {
  dir.create(pipeline, recursive=TRUE)
  for (folder in c('out', 'store', 'temp')){
    dir.create(file.path(pipeline, folder))
  }
}
```

```{r}
# Basic reference paths
stim_data_path <- file.path(PROJECT_DIR, '02-materials', '02-stimuli', PHASE) 
ext_data_path <- file.path(PROJECT_DIR, '03-data', '01-external', PHASE) 
int_data_path <- file.path(PROJECT_DIR, '03-data', '02-internal', PHASE) 
manual_analysis_path <- file.path(PROJECT_DIR, '04-analysis', '03-manual', PHASE) # 001-code / 003-manual
```

# Process Main Data
```{r}
load(file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, "n2", "store", "n2_responses.RData"))
load(file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, "n2", "store", "n2_responses_selected.RData"))
load(file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, "n2", "store", "n2_subj_info.RData"))
```

## Summarize & Explore

### Descriptors
Try adding sentiments to each word entry: https://www.tidytextmining.com/sentiment.html
(See also: https://m-clark.github.io/text-analysis-with-R/sentiment-analysis.htm)
```{r}
library(tidytext)
n2_descriptors <-
  n2_responses_selected %>%
  filter(Question=="Impressions") %>%
  select(-Question:-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(Response)) %>%
  mutate(Response = mgsub(Response, c("-"), c(""), fixed=TRUE)) %>%
  mutate(Response = mgsub(Response, c("inetlligent", "independant"), c("intelligent", "independent"), fixed=TRUE)) %>% # typos
  # Add counts
  add_count(Voice, name="total_Voice_n") %>% add_count(Response, name="total_word_n") %>% 
    add_count(Voice, Response, name="Voice_word_n") %>%
  # Add sentiments
  rename(word=Response) %>% 
  left_join(get_sentiments("bing")) %>% left_join(get_sentiments("afinn"))  %>%
  rename(Response=word, sentiment_score=value) %>%
  # mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment),
         # sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score))
  mutate(sentiment = case_when((is.na(sentiment) & (sentiment_score > 0)) ~ "positive",
                               (is.na(sentiment) & (sentiment_score < 0)) ~ "negative",
                               TRUE ~ sentiment))
n2_descriptors
```
```{r}
# Check for certain types of patterns to clean
# n2_descriptors %>%  filter(Response == str_match(Response, ".* .*"))
```


#### Sentiment
```{r}
# Summarize sentiment data
n2_descriptors %>% count(sentiment)
n2_descriptors %>% count(sentiment_score)
n2_descriptors %>% quick_summarize(sentiment_score, na.rm=TRUE)
```

```{r}
#By Voice
n2_descriptors %>% drop_na(sentiment) %>% add_count(Voice, name="group_n") %>% 
  count(Voice, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n) %>%
  pivot_wider(id_cols=Voice, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum)

n2_descriptors %>% group_by(Voice)  %>% quick_summarize(sentiment_score, na.rm=TRUE) %>% rename_with( ~ gsub("_score", "", .x))
```

#### Sets
```{r}
# Total descriptor words
n2_descriptors %>% count(name="total_words")

# Total unique/different words
n2_descriptors %>% count(Response) %>% count(name="unique_words")
```

```{r}
# Top words used
n2_descriptors %>% count(Response, sort=TRUE)

# Top words used for a certain Voice
n2_descriptors %>% count(Voice, Response, sort=TRUE)

```
```{r}
# Show all
n2_descriptors %>% count(Response, sort=TRUE) %>% 
  ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")

# Only show >1
n2_descriptors %>% count(Response, sort=TRUE) %>% filter(n>1) %>% 
  ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")
```
### Age
Get proportion selections per age category for each Voice.
```{r}
n2_age <- n2_responses_selected %>% 
  filter(Question=="Age") %>% 
  pivot_wider(id_cols = c(PROLIFIC_PID, Voice), names_from = Question_Part, values_from = Response) %>% 
  mutate(across(Teens:`Over 40`, ~ as.numeric(as.character(.x)))) 
n2_age
# View crosstab counts
# n2_age %>% group_by(Voice) %>% summarize(across(where(is.numeric), sum))
```


```{r}
n2_age_byVoice <- n2_age %>% 
  # Get proportions out of total responses (props add up to over 100 b/c multiple selections)
  add_count(Voice) %>% group_by(Voice, n) %>% 
  summarize(across(Teens:`Over 40`, sum)) %>% 
  mutate(across(Teens:`Over 40`, ~ .x/n)) %>%
  pivot_longer(Teens:`Over 40`, names_to = "Age", values_to = "prop") %>%
  mutate(Age=gsub("Teens", "10s", Age)) %>%
  # Drop values where none selected
  filter(prop>0) %>%  arrange(-prop, .by_group=TRUE) 

# View heatmap of persona_group selections per voice
# all
n2_age_byVoice %>%
  ggplot(aes(Voice, Age)) + gg_theme() +
  geom_tile(aes(fill = prop), colour = "white") +
  geom_text(aes(label = round(prop, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red")

# Select only highest values
n2_age_byVoice <- n2_age_byVoice %>%
  # filter(prop>=0.75) # Option 1: Keep values where more than 75% of participants selected
  slice_max(prop) # Option 2: Keep only highest value (including ties)
n2_age_byVoice <- n2_age_byVoice %>% 
  full_join(
    # Get age consensus based on majority decision
   n2_age_byVoice %>% pivot_wider(Voice:n, names_from = Age, values_from = prop) %>% 
   # relocate(`10s`, .before=`20s`) %>%
   mutate(across(2:last_col(), ~ ifelse(!is.na(.x), cur_column(), NA) )) %>%
   unite(perceived_age, 3:last_col(), sep=',', na.rm=TRUE)
  ) %>% ungroup
n2_age_byVoice
```


### (+) GrewUp
```{r}
n2_grewup <-
  n2_responses_selected %>% filter(Question=="GrewUp") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = str_trim(tolower(Response))) %>%
  mutate(Response = gsub("(.*)\\s*,\\s*usa", "\\1", Response)) %>% # keep everything before ', usa'
  mutate(Response = mgsub(Response, c("philipinnes|phillippines", "^america$|^us$|^united states$|^u\\.s\\.$", "(?:^|\\W)(ca|cali|califonia)(?:$|\\W)", "(?:^|\\W)ny(?:$|\\W)", "(?:^|\\W)az(?:$|\\W)", "wisconson"), c("philippines", "usa", "california", "new york", "arizona", "wisconsin"))) %>% 
  # Add categorization into US or Asia or Europe
  mutate(Region = case_when(
    Response %in% c("san francisco", "los angeles", "boston", "beverly hills", "san fernando valley", "san gabriel", "las vegas", "bay area", "portland") ~ "USA",
    str_detect(Response, paste0("(?:^|\\W)", paste(tolower(state.name), collapse = '|'), "(?:$|\\W)")) ~ "USA",
    str_detect(Response, "(?:^|\\W)(mid(\\s*|-*)west|pacific northwest|(east|west)\\s*coast|(the )*(west|south|southwest)|(west|east)ern states)(?:$|\\W)|american suburbia|(?:^|\\W)united states(?:$|\\W)|(?:^|\\W)(in|north|northeastern|southwest) (america|us)(?:$|\\W)|(?:^|\\W)in the us$") ~ "USA", 
    str_detect(Response, "(?:^|\\W)usa(?:$|\\W)") ~ "USA", # if USA or <other>, defaults to US
    str_detect(Response,"(?:^|\\W)canada(?:$|\\W)") ~ "Canada",
    str_detect(Response, "(?<!\\w)(asia(n*))(?!\\w)") ~ "Asia",
    str_detect(Response, "(?<!\\w)(europe|eu|slavic)(?!\\w)") ~ "Europe",
    TRUE ~ NA
  )) 
n2_grewup <- n2_grewup %>%
  mutate(CountryContinent = countrycode(sourcevar = Response,
                             origin = "country.name",
                             destination = "continent")) %>%
  cbind(CityCountry = world.cities[match(n2_grewup$Response, tolower(world.cities$name)), ][[2]]) %>%
  
  mutate(Response_Cat = coalesce(coalesce(Region, CountryContinent), CityCountry), .after=Response) %>%
  mutate(across(where(is.character), as.factor))


# Check summary
summary(n2_grewup)

# Check categorization
n2_grewup %>% filter(Response_Cat == "USA") %>% count(Response_Cat, Response, sort=TRUE)
n2_grewup %>% filter(Response_Cat == "Asia") %>% count(Response_Cat, Response, sort=TRUE)
n2_grewup %>% filter(!(Response_Cat == "USA" | Response_Cat == "Asia")) %>% count(Response_Cat, Response, sort=TRUE)
# Check for NAs (uncategorized)
# n2_grewup %>% filter(is.na(Region) & is.na(CountryContinent) & is.na(CityCountry) )
n2_grewup %>% filter(is.na(Response_Cat) )
```

```{r}
# Check for errors
# world.cities %>% filter(country.etc=="USA") %>% mutate(name=tolower(name)) %>% filter(str_detect(name, "washington"))
```

```{r}
# Check data overall
# Specific Labels
n2_grewup %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")

# Categorized Labels
n2_grewup %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
```
All personae categories sometimes identified as USA

```{r}
# Check Location category of photos per voice
n2_grewup %>%  count(Voice, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

```
```{r}
n2_grewup %>% count(Voice, Response_Cat)
```

```{r}
# View heatmap of responses per voice
# all
n2_grewup %>% count(Voice, Response_Cat) %>% mutate(prop = n/18) %>% 
  mutate(Response_Cat = ifelse(is.na(Response_Cat), "Other", as.character(Response_Cat))) %>%
  mutate(Response_Cat = fct_relevel(Response_Cat, "USA", "Canada", "Europe", "Asia", "Other")) %>%
  ggplot(aes(x=Voice, y=fct_rev(Response_Cat))) + gg_theme() +
  geom_tile(aes(fill = prop), colour = "white") +
  geom_text(aes(label = round(prop, 2)), col="white") +
  scale_fill_gradient(low = "lightgrey", high = "red") + #, limits=c(0,0.6), breaks=seq(0.1,0.5,0.1))
  labs(title="Free Response", y="Location") + theme_bw()

ggsave(file=file.path(pipeline, "temp", "grewup_heatmap.png"))
```


**Review Comments:**


**Selection Criteria:**


### (+) Ethnicity
```{r}
n2_ethnicity <-
  n2_responses_selected %>% filter(Question=="Ethnicity") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(str_trim(Response))) %>%
  mutate(Response = mgsub(Response, c("china|chiense", "flippino|philipino|philipno|pilipino|from the philippines", "portugese", "african american"), c("chinese", "filipino", "portuguese", "black"))) %>% 

  # Add categorization into race
  mutate(Asian = case_when(
    str_detect(Response, "(?:^|\\W)(asia(n*)|chinese|filipin(o|a))(?:$|\\W)") ~ "Asian",
    str_detect(Response, "(?:^|\\W)(hong kong|singapore|japan|korean|taiwan|vietnam|thai|hmong|lao|malaysian)") ~ "Asian",
    TRUE ~ NA
  )) %>%
  mutate(White = case_when(
    str_detect(Response, "(?:^|\\W)(white|european|caucasian|anglo|english|finnish|german|irish|italian|portuguese|scandinavian|scottish|swedish|danish|russian|greek|slavic|british|french|norwegian)(?:$|\\W)") ~ "White",
    TRUE ~ NA
  )) %>%
  mutate(American = ifelse(str_detect(Response, "(?:^|\\W)american(?:$|\\W)"), "American", NA)) %>%
  # mutate(Other = case_when(
    # str_detect(Response, "(?:^|\\W)(mexi|hispanic)") ~ "Other",
    # str_detect(Response, "(?:^|\\W)(half|and|mixed)(?:$|\\W)") ~ "Mixed",
    # TRUE ~ NA )) %>%
  mutate(Response_Cat = coalesce(coalesce(coalesce(Asian, White), American), "Other"), .after=Response) %>% # if Asian mixed, Asian
  mutate(across(where(is.character), as.factor))
# n2_ethnicity

# Check for NAs (uncategorized)
n2_ethnicity %>% filter(is.na(Response_Cat) )

# Check summary
summary(n2_ethnicity)

# Check categorization
n2_ethnicity %>% filter(Response_Cat == "Asian") %>% count(Response_Cat, Response, sort=TRUE)
n2_ethnicity %>% filter(Response_Cat == "White") %>% count(Response_Cat, Response, sort=TRUE)
n2_ethnicity %>% filter(!(Response_Cat == "Asian" | Response_Cat == "White")) %>% count(Response_Cat, Response, sort=TRUE)
```

```{r}
# Check data overall
# Specific Labels
n2_ethnicity %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")

# Categorized Labels
n2_ethnicity %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
```

```{r}
# Check  category of photos per voice
n2_ethnicity %>% count(Voice, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```
```{r}
# View heatmap of responses per voice
# all
n2_ethnicity %>% count(Voice, Response_Cat) %>% mutate(prop = n/18) %>% 
  mutate(Response_Cat = ifelse(is.na(Response_Cat), "Other", as.character(Response_Cat))) %>%
  mutate(Response_Cat = fct_relevel(Response_Cat, "White", "American", "Asian", "Other")) %>%
  ggplot(aes(x=Voice, y=fct_rev(Response_Cat))) + gg_theme() +
  geom_tile(aes(fill = prop), colour = "white") +
  geom_text(aes(label = round(prop, 2)), col="white") +
  scale_fill_gradient(low = "lightgrey", high = "red") + #, limits=c(0,0.6), breaks=seq(0.1,0.5,0.1))
  labs(title="Free Response", y="Ethnicity") + theme_bw()

ggsave(file=file.path(pipeline, "temp", "ethnicity_heatmap.png"))
```


**Review Comments:**

**Selection Criteria:**

To pick photos: Check a specific photo's words
```{r}
n2_ethnicity %>% filter(Voice=="S01") %>% select(Voice, Response_Cat, Response, PROLIFIC_PID)

```

### (+) Speech
```{r}
n2_speech <- 
  n2_responses_selected %>% filter(Question=="Speech") %>% select(-Question_Part) %>%
    # text cleaning
  mutate(Response = tolower(str_trim(Response)))  %>%
  # Consider: shorten long statements by extracting the keyword (e.g. straightforward)
# Add categorization into accentedness
  mutate(OtherAccent = case_when(
    str_detect(Response, "(?:^|\\W)((british|english|uk|southern( drawl)*|mid(\\s*|-*)west(ern)*|country|spanish|german|european) accent)(?:$|\\W)") ~ "OtherAccent",
    TRUE ~ NA
  )) %>%
  mutate(Accented = case_when(
    # str_detect(Response, "(?:^|\\W)(different to a|non) native speaker(?:$|\\W)") ~ "Accented",
    str_detect(Response, "(?:^|\\W)(accented|(with|have) a(.*\\s)accent|(broken|poor(-ish)*) english)(?:$|\\W)") ~ "Accented",
    str_detect(Response, "(?:^|\\W)(slight|strong|aapi|hawaiian|asian|foreign|heritage('s)*|chinese) accent(?:$|\\W)") ~ "Accented",
    TRUE ~ NA
  )) %>%
  mutate(Unaccented = case_when(
    str_detect(Response, "(?:^|\\W)(standard|unaccented|without an accent|no accent|(american|canadian|neutral|west coast) accent|california\\s.*accent|(typical|like an|average) american|(california) style|valley girl|normal|perfect english|native (speaker|english))(?:$|\\W)") ~ "Unaccented",
    # str_detect(Response, "(?:^|\\W)((fluent|clear) english)(?:$|\\W)") ~ "Unaccented",
    TRUE ~ NA
  )) %>%

  mutate(Response_Cat = coalesce(coalesce(coalesce(OtherAccent, Accented), Unaccented), "Other"), .after=Response) %>%
  mutate(across(where(is.character), as.factor))
# n2_speech

# Check for NAs (uncategorized)
n2_speech %>% filter(is.na(Response_Cat) ) %>% select(Response:last_col())

# Check summary
summary(n2_speech)

# Check categorization
n2_speech %>% filter(Response_Cat == "Unaccented") %>% count(Response_Cat, Response, sort=TRUE)
n2_speech %>% filter(Response_Cat == "Accented") %>% count(Response_Cat, Response, sort=TRUE)
n2_speech %>% filter(!(Response_Cat == "Unaccented" | Response_Cat == "Accented")) %>% count(Response_Cat, Response, sort=TRUE)
```
NOTE: Two of the "accented" may be categorized as "unaccented"and I fixed it. Just discuss this rather than show categories either way.
S01 = accented and non native speaker
S11 = they speak with an asian accent, different to a native speaker

Accentedness:
```{r}
# Check data overall
# Specific Labels
n2_speech %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")

# Categorized Labels
n2_speech %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
```

```{r}
# Check category of photos per condition
n2_speech %>% count(Voice, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

```
```{r}
# View heatmap of responses per voice
# all
n2_speech %>% count(Voice, Response_Cat) %>% mutate(prop = n/18) %>% 
  mutate(Response_Cat = ifelse(is.na(Response_Cat), "Other", as.character(Response_Cat))) %>%
  mutate(Response_Cat = fct_relevel(Response_Cat, "Other", "Unaccented", "Accented")) %>%
  ggplot(aes(x=Voice, y=fct_rev(Response_Cat))) + gg_theme() +
  geom_tile(aes(fill = prop), colour = "white") +
  geom_text(aes(label = round(prop, 2)), col="white") +
  scale_fill_gradient(low = "lightgrey", high = "red") + #, limits=c(0,0.6), breaks=seq(0.1,0.5,0.1))
  labs(title="Free Response", y="Accentedness") + theme_bw()

ggsave(file=file.path(pipeline, "temp", "accentedness_heatmap.png"))
```


```{r}
# Check specific photos/labels
n2_speech %>% mutate(Response=as.character(Response)) %>% 
  filter(Voice == "S11")

# n2_speech %>% mutate(Response=as.character(Response)) %>% 
#   filter(str_detect(Response, "(?:^|\\W)(south)"))
```


#### Styles
```{r}
n2_speech <- n2_speech %>%
  # Other styles
  mutate(Style = case_when(
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(soft|softly|soft spoken|quiet|mild|high|sweet|feminine)") ~ "Delicate", # possibly polite
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(deliberate|formal|polite)") ~ "Reserved",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(casual|conversational|slang)") ~ "Casual",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(fast|quick|rapid)") ~ "Fast",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(slow|drawl)") ~ "Slow",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(direct|straight|straightforward|clear manner|concise)") ~ "Direct",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(nasal|nasally)") ~ "Nasal",
    TRUE ~ NA
  ))

# Check Styles data
n2_speech %>% filter(Style == "Delicate") %>% count(Style, Response, sort=TRUE)
n2_speech %>% filter(Style == "Reserved") %>% count(Style, Response, sort=TRUE)
n2_speech %>% filter(Style == "Fast") %>% count(Style, Response, sort=TRUE)
n2_speech %>% filter(Style == "Direct") %>% count(Style, Response, sort=TRUE)
n2_speech %>% filter(Style == "Nasal") %>% count(Style, Response, sort=TRUE)


# Categorized Labels
n2_speech %>% count(Style, sort=TRUE) %>% add_count(name="n_unique")
n2_speech %>% count(Voice, Style, sort=TRUE) %>% pivot_wider(names_from=Voice, values_from = n)
```



### (+) Ratings
```{r}
n2_ratings <-
  n2_responses %>%
  select(PROLIFIC_PID, Question_Number, Question, Question_Part, Voice, Response) %>%
  filter(Question=="Ratings") %>% select(-Question) %>% mutate(Response =  as.numeric(as.character(Response))) %>%
  # Convert scales and responses to consistent direction
  separate(Question_Number, into=c("Question_Number", "Version"), sep="-") %>%
separate(Question_Number, into=c(NA, "Rating_Group"), sep="\\.") %>% 
  mutate(Rating_Group = case_when(Rating_Group == "8" ~ "Style",
                                  Rating_Group == "9" ~ "Traits",
                                  Rating_Group == "10" ~ "Culture",
                                  TRUE ~ NA)) %>%
  separate(Question_Part, into=c("Left", "Right"), sep=":") %>%
  mutate(Rating_Scale = case_when(Rating_Group == "Style" & Version == "A" ~ Left,
                                  Rating_Group == "Traits" & Version == "A" ~ Right,
                                  Rating_Group == "Culture" & Version == "A" ~ Left,
                                  Rating_Group == "Style" & Version == "B" ~ Right,
                                  Rating_Group == "Traits" & Version == "B" ~ Left,
                                  Rating_Group == "Culture" & Version == "B" ~ Right,
                                  TRUE ~ NA), .before=(Rating_Group))  %>%
  mutate(Rating_Value = case_when(Rating_Group == "Style" & Version == "A" ~ abs((Response)-8),
                                  Rating_Group == "Traits" & Version == "B" ~ abs((Response)-8),
                                  Rating_Group == "Culture" & Version == "A" ~ abs((Response)-8),
                                  TRUE ~ Response), .before=(Response)) %>%
  select(-Version, -Left, -Right) %>% relocate(c(Rating_Group,Rating_Scale), .before=Rating_Value) %>%
  # Add z-score transformed ratings by participant (participant-normalized)
  group_by(PROLIFIC_PID) %>% 
  mutate(Rating_ZScore = scale(Rating_Value, center=TRUE, scale=TRUE), .before=Rating_Value) %>%
  mutate(Response_ZScore = scale(Response, center=TRUE, scale=TRUE)) %>% 
  ungroup() %>%
  mutate(across(where(is.character), as.factor))

summary(n2_ratings)
n2_ratings
```
Summary stats
```{r}
# Check summary stats of data overall
n2_ratings %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
# n2_ratings %>% summarize(min=min(Rating_ZScore), max=max(Rating_ZScore), mean=mean(Rating_ZScore), median=median(Rating_ZScore))

# by rating group
n2_ratings %>% group_by(Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

# by Voice
n2_ratings %>% group_by(Voice, Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
n2_ratings %>% group_by(Voice, Rating_Scale) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

```

#### Tabularize
Means per
```{r}
# Check  category of photos per condition + sd
# Tables
n2_ratings %>% group_by(Rating_Group) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd)
  
n2_ratings %>% group_by(Rating_Group, Rating_Scale) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd)
```

#### Visualize
Parallel plots show the differences in score (=y, e.g., rating values) across score type (=x, e.g., rating scales) for each group (=series as represented by color and line, e.g., condition).

##### By Voice

Overview of ratings grouping scales under Rating_Group (Culture, Traits, Style)
```{r}
# Condition Mean Plots by Rating_Group
# Get plots: Zoomed in (Raw and ZScore)
parallel_plot(n2_ratings, x=Rating_Group, y=Rating_Value, group=Voice)
parallel_plot(n2_ratings, x=Rating_Group, y=Rating_ZScore, group=Voice)

# Get plots: Zoomed out (Raw and ZScore)
parallel_plot(n2_ratings, x=Rating_Group, y=Rating_Value, group=Voice, full_scale = TRUE)
parallel_plot(n2_ratings, x=Rating_Group, y=Rating_ZScore, group=Voice, full_scale = TRUE)
```
```{r}
# Condition Distribution + Median Violin Plots by RatingGroup
violin_plot(n2_ratings, x=Rating_Group, y=Rating_Value, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE)
violin_plot(n2_ratings, x=Rating_Group, y=Rating_ZScore, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE)

```

Overview of all scales shown individually, reordered into helpful viewing order
```{r}
# Condition Mean Parallel Plots by Rating_Scale
# Get plots: Zoomed in (Raw and ZScore)
n2_ratings_releveled <-
  n2_ratings %>% 
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale, 
  "American", "Native speaker of English", "American-accented", "Aligned with American culture",  # Culture
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent",               # Traits
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker"))                         # Style

parallel_plot(n2_ratings_releveled, x=Rating_Scale, y=Rating_Value, group=Voice, angle_axis_labels=TRUE) 
parallel_plot(n2_ratings_releveled, x=Rating_Scale, y=Rating_ZScore, group=Voice, angle_axis_labels=TRUE) + labs(title="Likert Scale", x="Rating Scale", y="Rating (Z-score)")

ggsave(file=file.path(pipeline, "temp", "ratings_parallelplot.png"))
```

```{r}
# Selected speakers 
n2_ratings_releveled %>%
  filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Rating_Value, group=Voice, angle_axis_labels=TRUE) 

n2_ratings_releveled %>%
  filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Rating_ZScore, group=Voice, angle_axis_labels=TRUE) 

```
```{r}
# Selected speakers 
n2_ratings_releveled %>%
  filter(!Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Rating_Value, group=Voice, angle_axis_labels=TRUE) 

n2_ratings_releveled %>%
  filter(!Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Rating_ZScore, group=Voice, angle_axis_labels=TRUE) 

```


```{r}
# Condition Distribution + Median Violin Plots by Rating_Scale
n2_ratings_releveled %>% filter(Rating_Group == "Culture") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n2_ratings_releveled %>% filter(Rating_Group == "Traits") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n2_ratings_releveled %>% filter(Rating_Group == "Style") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)
```



### (+) Likelihood
```{r}
n2_likelihood <-
  n2_responses %>%
  select(PROLIFIC_PID, Question_Number, Question, Question_Part, Voice, Response) %>%
  filter(Question=="Likelihood") %>% select(-Question) %>% mutate(Response =  as.numeric(as.character(Response))) %>%
  # Convert scales and responses to consistent direction
  # separate(Question_Number, into=c("Question_Number", "Version"), sep="-") %>%
separate(Question_Number, into=c(NA, "Rating_Group"), sep="\\.") %>% 
  mutate(Rating_Group = case_when(Rating_Group == "11" ~ "GrewUpContinent",
                                  Rating_Group == "12" ~ "GrewUpRegion",
                                  Rating_Group == "13" ~ "RaceEthnicity",
                                  TRUE ~ NA)) %>%
  mutate(Rating_Scale=str_extract(Question_Part, "(?<=(in\\s|or\\s)).+(?=$)"), .after=Rating_Group) %>%  
  mutate(Rating_Scale = mgsub(Rating_Scale, c("the ", "US or in Canada"), c("", "elsewhere US/Canada"))) %>% 
  group_by(PROLIFIC_PID) %>% 
  mutate(Response_ZScore = scale(Response, center=TRUE, scale=TRUE)) %>% 
  ungroup() %>%
  mutate(across(where(is.character), as.factor)) %>% select(-Question_Part)

summary(n2_likelihood)
n2_likelihood
```
Summary stats
```{r}
# Check summary stats of data overall
n2_likelihood %>% quick_summarize(Response) %>% rename_with(~ gsub("Response", "rating", .x))
# n2_likelihood %>% summarize(min=min(Response_ZScore), max=max(Response_ZScore), mean=mean(Response_ZScore), median=median(Response_ZScore))

# by rating group
n2_likelihood %>% group_by(Rating_Group) %>% quick_summarize(Response) %>% rename_with(~ gsub("Response", "rating", .x))

# by Voice
n2_likelihood %>% group_by(Voice, Rating_Group) %>% quick_summarize(Response) %>% rename_with(~ gsub("Response", "rating", .x))
n2_likelihood %>% group_by(Voice, Rating_Scale) %>% quick_summarize(Response) %>% rename_with(~ gsub("Response", "rating", .x))

```


#### Tabularize
Means per
```{r}
# Check  category of photos per condition + sd
# Tables
n2_likelihood %>% group_by(Rating_Group) %>% 
  summarize(mean=mean(Response), sd=sd(Response)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd)
  
n2_likelihood %>% group_by(Rating_Group, Rating_Scale) %>% 
  summarize(mean=mean(Response), sd=sd(Response)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd)
```

#### Visualize
Parallel plots show the differences in score (=y, e.g., rating values) across score type (=x, e.g., rating scales) for each group (=series as represented by color and line, e.g., condition).

##### By Voice

Overview of ratings grouping scales under Rating_Group (Culture, Traits, Style)
```{r}
# Condition Mean Plots by Rating_Group
# Get plots: Zoomed in (Raw and ZScore)
parallel_plot(n2_likelihood, x=Rating_Group, y=Response, group=Voice)
parallel_plot(n2_likelihood, x=Rating_Group, y=Response_ZScore, group=Voice)

# Get plots: Zoomed out (Raw and ZScore)
parallel_plot(n2_likelihood, x=Rating_Group, y=Response, group=Voice, full_scale = TRUE)
parallel_plot(n2_likelihood, x=Rating_Group, y=Response_ZScore, group=Voice, full_scale = TRUE)
```
```{r}
# Condition Distribution + Median Violin Plots by RatingGroup
violin_plot(n2_likelihood, x=Rating_Group, y=Response, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE)
violin_plot(n2_likelihood, x=Rating_Group, y=Response_ZScore, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE)

```

Overview of all scales shown individually, reordered into helpful viewing order
```{r}
# Condition Mean Parallel Plots by Rating_Scale
# Get plots: Zoomed in (Raw and ZScore)
n2_likelihood_releveled <-
  n2_likelihood %>% 
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale, 
  "US/Canada", "Asia", "Europe",  # GrewUpContinent
  "California", "Midwest", "Southern US", "elsewhere US/Canada",                # GrewUpRegion
  "White American", "Asian American","Latin/Hispanic American", "Black American"))                         # RaceEthnicity

parallel_plot(n2_likelihood_releveled, x=Rating_Scale, y=Response, group=Voice, angle_axis_labels=TRUE) 
parallel_plot(n2_likelihood_releveled, x=Rating_Scale, y=Response_ZScore, group=Voice, angle_axis_labels=TRUE) + labs(title="Likert Scale", x="Rating Scale", y="Rating (Z-score)")

ggsave(file=file.path(pipeline, "temp", "likelihood_parallelplot.png"))
```

```{r}
# Selected speakers: Targets
n2_likelihood_releveled %>%
  filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Response, group=Voice, angle_axis_labels=TRUE) 

n2_likelihood_releveled %>%
  filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  parallel_plot(x=Rating_Scale, y=Response_ZScore, group=Voice, angle_axis_labels=TRUE) 

```
```{r}
# Selected speakers: Filler
n2_likelihood_releveled %>%
  filter(!(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18"))) %>% 
  parallel_plot(x=Rating_Scale, y=Response, group=Voice, angle_axis_labels=TRUE) 

n2_likelihood_releveled %>%
  filter(!(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18"))) %>% 
  parallel_plot(x=Rating_Scale, y=Response_ZScore, group=Voice, angle_axis_labels=TRUE) 

```

```{r}
# Condition Distribution + Median Violin Plots by Rating_Scale
n2_likelihood_releveled %>% filter(Rating_Group == "GrewUpContinent") %>%
  violin_plot(x=Rating_Scale, y=Response, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n2_likelihood_releveled %>% filter(Rating_Group == "GrewUpRegion") %>%
  violin_plot(x=Rating_Scale, y=Response, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n2_likelihood_releveled %>% filter(Rating_Group == "RaceEthnicity") %>%
  violin_plot(x=Rating_Scale, y=Response, group=Voice, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)
```


### (+) Face
```{r}
n2_face <- n2_responses %>%
  select(PROLIFIC_PID, Question_Number, Question, Question_Part, Recode_Value=Question_Part_Number, Voice, Response) %>%
  filter(Question=="Face") %>% 
  separate(Question_Number, into=c("Question_Number", "List.Version"), sep="-") %>%
  select(-Question_Part, -Question_Number) %>%
  # separate(Question_Number, into=c(NA, "Rating_Group"), sep="\\.") %>% 
  left_join(n2_photosel_codes) %>% 
  # select only columns to use
  select(PROLIFIC_PID, Voice, List.Version, Response, Persona_Group, Persona_Code, Critical.Filler, Condition, Image=Code) %>%
  mutate(Response=as.numeric(as.character(Response)))
n2_face
```

```{r}
# Check number of times an Persona_Group was selected per voice
n2_face_byVoice <- 
  n2_face %>% add_count(Voice) %>% mutate(n_presented=n/6) %>%
  group_by(Voice, Persona_Group, n_presented) %>% summarize(n_selected=sum(Response)) %>% mutate(prop_selected = n_selected/n_presented) %>% ungroup() %>%
  mutate(Persona_Group = fct_relevel(Persona_Group, "WhiteA", "WhiteB", "WhiteC", "AsianC", "AsianB", "AsianA"))

# view as crosstab (counts)
n2_face_byVoice %>% 
  pivot_wider(id_cols = c(Voice), names_from = c(Persona_Group), values_from = n_selected) 
# view as crosstab (prop)
n2_face_byVoice %>% 
  pivot_wider(id_cols = c(Voice), names_from = c(Persona_Group), values_from = prop_selected) 

# view by most to least common choice per voice
n2_face_byVoice %>% slice_max(prop_selected)
n2_face_byVoice %>% slice_min(prop_selected)
n2_face_byVoice %>% arrange(Voice, -prop_selected)
```

#### Visualize
```{r}
# View heatmap of persona_group selections per voice
# all
n2_face_byVoice %>%
  ggplot(aes(x=Voice, y=fct_rev(Persona_Group))) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.6), breaks=seq(0.1,0.5,0.1)) + 
  labs(title="Face Selection", y="Persona Group") + theme_bw()

ggsave(file=file.path(pipeline, "temp", "face_heatmap.png"))


# filler/non-target
n2_face_byVoice %>% filter(!(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18"))) %>% 
  ggplot(aes(x=Voice, y=fct_rev(Persona_Group))) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.6), breaks=seq(0.1,0.5,0.1)) + 
  labs(title="Face Selection", y="Persona Group") + theme_bw()

# target
n2_face_byVoice %>% filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  ggplot(aes(x=Voice, y=fct_rev(Persona_Group))) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.6), breaks=seq(0.1,0.5,0.1)) + 
  labs(title="Face Selection", y="Persona Group") + theme_bw()
```

```{r}
# (Temp) Check number of times an Image was selected per voice
n2_face_byVoiceImage <- 
  n2_face %>% add_count(Voice) %>% mutate(n_presented=n/6) %>%
  group_by(Voice, Persona_Group, Image, n_presented) %>% summarize(n_selected=sum(Response)) %>% mutate(prop_selected = n_selected/n_presented) %>% ungroup() %>%
  mutate(Persona_Group = fct_relevel(Persona_Group, "WhiteA", "WhiteB", "WhiteC", "AsianC", "AsianB", "AsianA")) %>%
  unite(Persona_Image, c(Persona_Group, Image), remove=FALSE) %>%
  mutate(Persona_Image = fct_relevel(Persona_Image, 
                                     "WhiteA_WAW-58", "WhiteA_WAW-23", "WhiteA_WAW-12",
                                     "WhiteB_WAW-56", "WhiteB_WAW-4", "WhiteB_WAW-1", 
                                     "WhiteC_WAW-57", "WhiteC_WAW-51", "WhiteC_WAW-41",
                                     "AsianC_AAW-55", "AsianC_AAW-36", "AsianC_AAW-30",
                                     "AsianB_AAW-33", "AsianB_AAW-28", "AsianB_AAW-11",
                                     "AsianA_AAW-63", "AsianA_AAW-27", "AsianA_AAW-25"

                                     ))

# view as crosstab (counts)
n2_face_byVoiceImage %>%
  pivot_wider(id_cols = c(Voice), names_from = c(Persona_Group, Image), values_from = n_selected) 

# View heatmap of persona_group selections per voice
# all
n2_face_byVoiceImage %>%
  ggplot(aes(Voice, Persona_Image)) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.5), breaks=seq(0.1,0.5,0.1))

# filler/non-target
n2_face_byVoiceImage %>% filter(!(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18"))) %>% 
  ggplot(aes(Voice, Persona_Image)) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.5), breaks=seq(0.1,0.5,0.1))

# target
n2_face_byVoiceImage %>% filter(Voice %in% c("S01", "S03", "S06", "S10", "S16", "S18")) %>% 
  ggplot(aes(Voice, Persona_Image)) + gg_theme() +
  geom_tile(aes(fill = prop_selected), colour = "white") +
  geom_text(aes(label = round(prop_selected, 2)), col="white") +
  scale_fill_gradient(low = "white", high = "red", limits=c(0,0.5), breaks=seq(0.1,0.5,0.1))
```


### Check By-Subject Patterns
```{r}
# n2_subj_p2_network # add in maybe

n2_bysubj_data <- 
  n2_subj_info %>% 
  full_join(
    n2_descriptors %>% rename(descriptors_response = Response)
  ) %>% 
  full_join(n2_age) %>%
  full_join(
    n2_grewup %>% select(-Question) %>% rename(grewup_response = Response, grewup_cat = Response_Cat)
  ) %>%
  full_join(
    n2_ethnicity %>% select(-Question) %>% rename(ethnicity_response = Response, ethnicity_cat = Response_Cat)
  ) %>%
  full_join(
    n2_speech %>% select(-Question) %>% rename(speech_response = Response, speech_cat = Response_Cat)
  ) %>% 
  full_join(n2_ratings) %>% select(-Response:-Response_ZScore) %>% rename(ratings_group = Rating_Group, ratings_scale = Rating_Scale, ratings_zscore = Rating_ZScore, ratings_value = Rating_Value) %>%
  full_join(n2_likelihood) %>% rename(likelihood_group = Rating_Group, likelihood_scale = Rating_Scale, likelihood_zscore = Response_ZScore, likelihood_value = Response)

# check data
n2_bysubj_data
colnames(n2_bysubj_data)
```
```{r}
# Get plot functions: Numeric
target_plot_num <- function(df, grouping_column, current_column, angle_axis_labels=FALSE){
  plt_df <- df %>% select(PROLIFIC_PID:Voice_Number, !!current_column) %>% distinct()
  
  plot <- plt_df %>% 
    ggplot(aes(x=plt_df[[grouping_column]], y=plt_df[[current_column]], fill=plt_df[[grouping_column]])) + gg_theme() + 
    stat_summary(geom="col", alpha=0.7) + stat_summary() + labs(x=grouping_column, y=current_column, fill=grouping_column)
  if (angle_axis_labels == TRUE){
    plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
      theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
    }
  plot
}

# Get plot functions: Categorical
target_plot_cat <- function(df, grouping_column, current_column, angle_axis_labels=FALSE){
  grouping_column <- as.symbol(grouping_column) 
  current_column <- as.symbol(current_column)
  grouping_column <- enquo(grouping_column)
  current_column <- enquo(current_column)
  
  plt_df <- df %>% select(PROLIFIC_PID:Voice_Number, !!current_column) %>% distinct()
  plt_df <- plt_df %>% group_by(!!grouping_column, Voice) %>% add_count(!!grouping_column) 
  plt_df <- plt_df %>% count(n, !!current_column) %>% mutate(prop=nn/n)

  plot <- plt_df %>%  ggplot(aes(x=(!!current_column), y=prop, fill=(!!grouping_column))) + gg_theme() +
    stat_summary(geom="col", alpha=0.7, position = position_dodge(0.95)) + 
    stat_summary(position = position_dodge(0.95)) +
    labs(x=current_column, fill=grouping_column)
  if (angle_axis_labels == TRUE){
    plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
      theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)
    }
  plot
}
```

#### By-Gender
Prep
```{r}
# Get target grouped df
n2_bysubj_data_gender <- n2_bysubj_data %>% filter(GenderCat!="Prefer not to answer") %>% mutate(GenderCat = gsub("^Non.*", "NB", GenderCat)) %>% mutate(GenderCat = fct_relevel(GenderCat, "Man", "Woman", "NB")) %>% group_by(GenderCat)  %>% 
  # reorder factors
  mutate(ratings_scale = fct_relevel(ratings_scale,
  "American", "Native speaker of English", "American-accented", "Aligned with American culture", 
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent", 
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker")) %>%
    mutate(likelihood_scale = fct_relevel(likelihood_scale, 
  "US/Canada", "Asia", "Europe",  # GrewUpContinent
  "California", "Midwest", "Southern US", "elsewhere US/Canada",                # GrewUpRegion
  "White American", "Asian American","Latin/Hispanic American", "Black American")) 
```

```{r}
# By subject demographic summary: Age
grouping_column <- "GenderCat"
current_column <- "Age"
# Summary Table
quick_summarize(n2_bysubj_data_gender, current_column, na.rm=TRUE)
# Summary Plot
target_plot_num(n2_bysubj_data_gender, grouping_column, current_column)
```

```{r}
# By subject demographic summary: Asian Network
grouping_column <- "GenderCat"
current_column <- "prop_AsianCC"
# Summary Table
quick_summarize(n2_bysubj_data_gender, current_column, na.rm=TRUE)
# Summary Plot
target_plot_num(n2_bysubj_data_gender, grouping_column, current_column)

current_column <- "prop_AsianBC"
# Summary Table
quick_summarize(n2_bysubj_data_gender, current_column, na.rm=TRUE)
# Summary Plot
target_plot_num(n2_bysubj_data_gender, grouping_column, current_column)
```

```{r}
# By subject summary: Descriptors
grouping_column <- "GenderCat"
current_column <- "sentiment_score"
# Summary Table
quick_summarize(n2_bysubj_data_gender, current_column, na.rm=TRUE)
# Summary Plot
target_plot_num(n2_bysubj_data_gender, grouping_column, current_column)
```
```{r}
# By subject summary: Grewup
grouping_column <- "GenderCat"
current_column <- "grewup_cat"
# Summary Plot
target_plot_cat(n2_bysubj_data_gender, grouping_column, current_column, angle_axis_labels=TRUE)
```
```{r}
# By subject summary: Ethnicity
grouping_column <- "GenderCat"
current_column <- "ethnicity_cat"
# Summary Plot
target_plot_cat(n2_bysubj_data_gender, grouping_column, current_column, angle_axis_labels=TRUE)
```


```{r}
# By subject summary: Speech
grouping_column <- "GenderCat"
current_column <- "speech_cat"
# Summary Plot
target_plot_cat(n2_bysubj_data_gender, grouping_column, current_column, angle_axis_labels=TRUE)
```

```{r}
# By subject summary: Speech
grouping_column <- "GenderCat"

# Summary Plot
n2_bysubj_data_gender %>% select(PROLIFIC_PID:Voice_Number, ratings_group:ratings_zscore) %>% distinct() %>%
  filter(GenderCat!="NB") %>%
  parallel_plot(x=ratings_scale, y=ratings_zscore, group=GenderCat, angle_axis_labels=TRUE, viridis=FALSE)  +
  labs(title="All Voices: Ratings")

n2_bysubj_data_gender %>% select(PROLIFIC_PID:Voice_Number, likelihood_group:likelihood_zscore) %>% distinct() %>%
  filter(GenderCat!="NB") %>%
  parallel_plot(x=likelihood_scale, y=likelihood_zscore, group=GenderCat, angle_axis_labels=TRUE, viridis=FALSE)  +
  labs(title="All Voices: Likelihood")

```


## Process
### Get By-Voice Data
Get general screening filters per Voice, merge into one.
```{r}
n2_percepts_general <- 
  n2_descriptors %>% group_by(Voice) %>% summarize(sentiment_score=mean(sentiment_score, na.rm=TRUE)) %>%
  full_join(
    n2_descriptors %>% drop_na(sentiment) %>% add_count(Voice, name="group_n") %>% 
  count(Voice, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n, -n) %>%
  pivot_wider(id_cols=Voice, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum) %>%
  rename(positive_prop=positive, negative_prop=negative)
  ) %>%
  full_join(
    n2_age_byVoice %>% select(Voice, perceived_age)
  )

# n2_occupation
# n2_activities
n2_percepts_general
```

Get persona screening filters per Voice, merge into one.
```{r}
n2_percepts_personae <- 
  n2_grewup %>% count(Voice, Response_Cat) %>% drop_na(Response_Cat) %>% add_count(Voice, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Voice, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Voice, from_asia_prop=Asia, from_us_prop=USA) %>%
  full_join(
    n2_ethnicity %>% count(Voice, Response_Cat) %>% add_count(Voice, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Voice, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Voice, asian_prop=Asian, white_prop=White)
  ) %>%
  full_join(
    n2_speech %>% count(Voice, Response_Cat) %>% add_count(Voice, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Voice, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Voice, accented_prop=Accented, unaccented_prop=Unaccented)
  ) %>%
  full_join(
    n2_face %>% count(Voice, Persona_Group, Response) %>% add_count(Voice, wt=n) %>% mutate(nn=nn/6, prop=n/nn) %>%
      filter(Response==1) %>%  pivot_wider(id_cols=c(Voice), names_from = Persona_Group, values_from = prop, values_fill=0, names_glue = "{Persona_Group}_prop")
  )
n2_percepts_personae

```


Get mean general+persona rating scores per Voice, to (potentially) put through a PCA.
```{r}
# By-Voice means: Get across-participant means per Rating_Scale for each Voice 
n2_ratings_means <- n2_ratings_releveled %>% group_by(Voice, Rating_Group, Rating_Scale) %>%
  summarize(Rating_Value=mean(Rating_Value), Rating_ZScore = mean(Rating_ZScore)) %>% ungroup()

# Get factors as columns
# Raw rating values
n2_ratings_vars_raw <-
   n2_ratings_means %>%  pivot_wider(id_cols=Voice, names_from = Rating_Scale, values_from =  Rating_Value)

# Zscores
n2_ratings_vars_zscore <-
  n2_ratings_means %>%  pivot_wider(id_cols=Voice, names_from = Rating_Scale, values_from = Rating_ZScore)

# View data
n2_ratings_means
n2_ratings_vars_raw
n2_ratings_vars_zscore
```

```{r}
# By-Voice means: Get across-participant means per Rating_Scale for each Voice 
n2_likelihood_means <- n2_likelihood_releveled %>% group_by(Voice, Rating_Group, Rating_Scale) %>%
  summarize(Response=mean(Response), Response_ZScore = mean(Response_ZScore)) %>% ungroup()

# Get factors as columns
# Raw rating values
n2_likelihood_vars_raw <-
   n2_likelihood_means %>%  pivot_wider(id_cols=Voice, names_from = Rating_Scale, values_from =  Response)

# Zscores
n2_likelihood_vars_zscore <-
  n2_likelihood_means %>%  pivot_wider(id_cols=Voice, names_from = Rating_Scale, values_from = Response_ZScore)

# View data
n2_likelihood_means
n2_likelihood_vars_raw
n2_likelihood_vars_zscore
```


Check correlations between measures to assess collinearity.

Function to get correlation info:
```{r, fig.width=8, fig.height=7.5}
get_correlations <- function(df, columns_to_keep){
  # NOTE: Columns to keep cannot be name of column, only indices or function (e.g. last_col())
  corr_matrix <-
    df %>% select(columns_to_keep) %>%
    cor(., method="pearson")  # Alt: method="spearman"
  print(corr_matrix)
  # # View the correlation matrix
  corr_df <- corr_matrix %>% as_tibble(rownames = "var") # independent variables correlation matrix 
  print(corr_df)
  # # Visualize
  corr_plot <- corrplot(corr_matrix, method='number',is.corr = T)
  print(corr_plot)
}
```

Raw rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n2_ratings_vars_raw, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```
The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear. (OUTDATED: though Friendly and Likable are similar (>0.8) as well.)

ZScored rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n2_ratings_vars_zscore, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```

The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear.



### Finalize By-Voice Data
#### Merge Scores
```{r}
# Merge finalized ratings data with other processed and finalized data per Voice
# Without PCA
n2_byVoice_data <- n2_ratings_vars_zscore %>% full_join(n2_likelihood_vars_zscore) %>% full_join(n2_percepts_personae) %>% full_join(n2_percepts_general) %>%
  distinct()

n2_byVoice_data
```

#### Calculate Scores
Add other relevant variables needed for selection of personae photos.
```{r}
# Add relevant selected composite persona variables
# Get average of "American background/upbringing" ratings, vs "American culture/identity" rating
n2_byVoice_data <- n2_byVoice_data %>% 
  mutate(perceived_american_raised = (American + `Native speaker of English` + `American-accented`)/3 ,
        perceived_american_aligned = `Aligned with American culture`, .after=Voice)  %>%
  mutate(perceived_american_diff = perceived_american_aligned - perceived_american_raised, .after=Voice)
```

##### Visualize
```{r}
# Visualize
n2_byVoice_data %>% 
  ggplot(aes(x=perceived_american_raised, y=perceived_american_aligned, color=Voice)) + 
  geom_point() + gg_theme()

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=perceived_american_raised, y=perceived_american_aligned, label=Voice, group=Voice, show_points=TRUE)
```

```{r}
# Visualize Asian vs. White by Task
# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=asian_prop, y=white_prop, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=`Asian American`, y=`White American`, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=`AsianA_prop`, y=`WhiteA_prop`, label=Voice, group=Voice, show_points=TRUE)
```

```{r}
# Visualize Asian by Task
# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=asian_prop, y=`Asian American`, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=asian_prop, y=`AsianA_prop`, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=`Asian American`, y=`AsianA_prop`, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=asian_prop, y=`AsianC_prop`, label=Voice, group=Voice, show_points=TRUE)

# Labelled scatterplot
n2_byVoice_data %>% labeled_scatterplot(x=`Asian American`, y=`AsianC_prop`, label=Voice, group=Voice, show_points=TRUE)

# # Labelled scatterplot
# n2_byVoice_data %>% labeled_scatterplot(x=`AsianA_prop`, y=`AsianC_prop`, label=Voice, group=Voice, show_points=TRUE)
```

Free Response (`asian_prop`)
- vs. Slider Scale
  - Most voices are roughly similar, correlation-wise. Generally higher likelilhood ratings for Asian American than mentioned in free response.
  - Two with visually/notably higher slider scores for Asian than free response was S10 and S01
- vs. Face Selection
  - For AsianA faces, visually/notably higher Asian face selections than free responses was S08, S18, and again S01
  - For AsianC faces, visually/notably higher Asian face selections  than free responses was ~S03, ~S05, ~S08, ~S18; but mostly S10 and S01 jump out (and S11 is notably low)

Slider Scale (`Asian American`)
- vs. Face Selection
  - For AsianA faces, roughly correlated. Some higher might be S08, S18, and S04, while some lower might be S07, S10
  - For AsianC faces, visually/notably higher Asian face selections than slider ratings was S03, S05, and mostly S10 and S01. Some who look notably lower are S16, S07, and possibly S11.

Face Selection (`AsianA_prop`; `AsianC_prop`)


```{r}
# Asian Impression scores by Task
# Created selected long byVoice data, scaled/z-scored
n2_byVoice_sl <- n2_byVoice_data %>% select(Voice, `Asian Free` = asian_prop, `AsAm Rating` = `Asian American`, `AsianA Face` = AsianA_prop, `AsianB Face` = AsianB_prop, `AsianC Face` = AsianC_prop) %>% 
  mutate(across(is.numeric, scale)) %>% 
  pivot_longer(cols = 2:last_col(), names_to = "measure", values_to = "value") %>% mutate(measure = fct_relevel(measure, "Asian Free", "AsAm Rating", "AsianA Face", "AsianB Face", "AsianC Face")) 
  
n2_byVoice_sl %>% group_by(Voice) %>% summarize(mean_measure=mean(value)) %>% arrange(mean_measure) %>% pull(Voice)

# Scaled scores faceted by voice 
n2_byVoice_sl %>% mutate(Voice = fct_relevel(Voice, "S06", "S18", "S07", "S05", "S03", "S08", "S04", "S16", "S10", "S01", "S11")) %>%
  ggplot(aes(x=measure, y=value)) + gg_theme() +
  geom_point() + geom_col(alpha=0.5) + facet_wrap(~Voice) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5)) + 
  theme(plot.margin = margin(0,0,0,40, unit = "pt")) 

```


```{r}
# For Reference: Column names
colnames(n2_byVoice_data)
```



### Identify Outliers
#### Percept Categories
```{r}
get_outliers <- function(df, col) {
  col <- enquo(col)
  df %>% select(1:3, !!col) %>% mutate(mean=mean(!!col), sd=sd(!!col)) %>% mutate(lower = mean-(3*sd), upper=mean+(3*sd)) %>%
    mutate(outlier=case_when(
      !!col < lower | !!col > upper ~ TRUE,
      TRUE ~ FALSE
    ))
}
```

Based on coded text responses.
```{r}
# Ethnicity
n2_byVoice_data %>% get_outliers(asian_prop) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(white_prop) %>% filter(outlier==TRUE)
# by Condition
# Check outliers beyond 3SD
n2_byVoice_data %>% get_outliers(asian_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least Asian Asian
n2_byVoice_data %>% select(1:3, asian_prop:white_prop) %>%  slice_min(asian_prop, n=4)
# NOTE: Seems like least "Asian" are the Southeast Asian/Pacific Islander interpreted faces

```
OUTLIER NOTES: 
- AAW-47 - 0.75  below lower 3SD boundary for asian_prop (perceived as much less asian)
- Then, AAW-45 and AAW-12

```{r}
# GrewUp
# Check outliers beyond 3SD
n2_byVoice_data %>% get_outliers(from_asia_prop) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# by Condition
n2_byVoice_data %>% get_outliers(from_asia_prop) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# NOTE: None
n2_byVoice_data %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least Asia Asian
n2_byVoice_data %>% select(1:3, from_asia_prop:from_us_prop)  %>% slice_min(from_asia_prop, n=4)
# Check least US Asian
n2_byVoice_data %>% select(1:3, from_asia_prop:from_us_prop) %>% slice_min(from_us_prop, n=4)
# NOTE: 

# Check most US White
n2_byVoice_data %>% select(1:3, from_asia_prop:from_us_prop) %>% slice_max(from_us_prop, n=4)
# Check least US White
n2_byVoice_data %>% select(1:3, from_asia_prop:from_us_prop) %>% slice_min(from_us_prop, n=4)
# NOTE: 
```

```{r}
# Speech
# Check outliers beyond 3SD
n2_byVoice_data %>% get_outliers(accented_prop) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(unaccented_prop) %>% filter(outlier==TRUE)
# by Condition
n2_byVoice_data %>% get_outliers(accented_prop) %>% filter(outlier==TRUE)
# NOTE: None
n2_byVoice_data %>% get_outliers(unaccented_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least accented/unaccented Asian
n2_byVoice_data %>% select(1:3, accented_prop:unaccented_prop) %>% slice_min(accented_prop, n=4)
n2_byVoice_data %>% select(1:3, accented_prop:unaccented_prop) %>% slice_min(unaccented_prop, n=4)
# NOTE: 

# Check most unaccented White
n2_byVoice_data %>% select(1:3, accented_prop:unaccented_prop) %>% 
  slice_max(unaccented_prop, n=4) # filter(unaccented_prop > 0.2) 
# Check least unaccented White
n2_byVoice_data %>% select(1:3, accented_prop:unaccented_prop) %>% 
  slice_min(unaccented_prop, n=4) # filter(unaccented_prop < 0.2)
# NOTE: Confusing to interpret because unaccented also include descriptors like "California style" and "standard"
```


```{r}
# Descriptors
# Check outliers beyond 3SD
n2_byVoice_data %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# by Condition
n2_byVoice_data %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# NOTE: None
n2_byVoice_data %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least positive photos
n2_byVoice_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(sentiment_score, n=4)
n2_byVoice_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(positive_prop, n=4)
```
OUTLIER NOTES:
- AAW-57 is perceived extra negatively, below lower 3SD boundary for both positive_prop and sentiment_score

##### Visualize
```{r}
# Ethnicity x Grewup x Speech

n2_byVoice_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, color=Voice, label=Voice)) + geom_text(alpha=0.7) + gg_theme() +
  scale_x_continuous(limits=c(-0.1, 1.1), breaks = c(0,0.25, 0.5, 0.75, 1))

n2_byVoice_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, size=accented_prop, color=Voice)) + geom_point(alpha=0.7) + gg_theme()

```


```{r}
# Descriptor Sentiment
n2_byVoice_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Voice)) + geom_point() + gg_theme()

n2_byVoice_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Voice, label=Voice)) + geom_text() + gg_theme() 
```

OUTLIERS EXCLUSION:
- AAW-47 (perceived as much less Asian than other AAW photos)
- AAW-57 (perceived as much more negative than any other photo)


#### Ratings
* Use the Mahalanobis distance to assess outliers/matches via visual inspection and selecting a threshold cut-off

```{r}
# Get mahalnobis distances Function
get_mdist <- function(scores_df, index_df, cutoff=10){
  # Finding the center point 
center  = colMeans(scores_df)
# Finding the covariance matrix
cov     = cov(scores_df)
# Calculate Mahalnobis distance + identify outliers
mdist_df <- index_df %>% 
  cbind(m_dist = mahalanobis(scores_df, center, cov))  %>%
  mutate(outlier = ifelse(m_dist > cutoff, TRUE, FALSE))
  # mutate(pvalue = pchisq(m_dist, df=3, lower.tail=FALSE)) #pval<0.001 outlier
return(mdist_df)
}
```

Check for outliers with 3*SD for numerical ratings
```{r}
# Get rating colnames
# n2_ratings_vars_pca %>% select(American:Intelligent) %>% colnames()

# Check for outliers
n2_byVoice_data %>% get_outliers(American) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Native speaker of English`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`American-accented`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Aligned with American culture`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Feminine) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Clear speaker`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Cool) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Casual) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Nerdy) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Slow speaker`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Enthusiastic) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Confident) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Friendly) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Likeable) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Attractive) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(Intelligent) %>% filter(outlier==TRUE)

# # NOTE: None
```

```{r}
n2_byVoice_data %>% get_outliers(`US/Canada`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Asia`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Europe`) %>% filter(outlier==TRUE)

n2_byVoice_data %>% get_outliers(`California`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Midwest`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Southern US`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`elsewhere US/Canada`) %>% filter(outlier==TRUE)

n2_byVoice_data %>% get_outliers(`White American`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Asian American`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Latin/Hispanic American`) %>% filter(outlier==TRUE)
n2_byVoice_data %>% get_outliers(`Black American`) %>% filter(outlier==TRUE)

# # NOTE: None
```
#### Dimensions
Check for outliers with Mahalanobis distances for numerical ratings
```{r}
# # All rating variables
# scores_df <- n2_ratings_vars_pca %>% select(-Condition:-Voice) %>% 
#   select(American:Intelligent) 
# index_df <- n2_ratings_vars_pca %>% select(Condition:Voice)
# 
# # Get mahalanobis distances
# mdist_df <- get_mdist(scores_df, index_df)
# 
# # Most different considering all variables
# # See results
# mdist_df %>% arrange(desc(m_dist))
# # Get list of potential outliers to check
# mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Voice, m_dist) %>% inner_join(n2_ratings_vars_pca) %>% select(-Voice_Cat)
```


```{r}
# # All orthogonal/unique variables
# scores_df <- n2_ratings_vars_pca %>% select(-Condition:-Voice) %>% 
#   select(Dim1, Dim2, Dim3, Dim4) 
# index_df <- n2_ratings_vars_pca %>% select(Condition:Voice)
# 
# # Get mahalnobis distances
# mdist_df <- get_mdist(scores_df, index_df)
# 
# # Most different considering all variables
# # See results
# mdist_df %>% arrange(desc(m_dist))
# # Get list of potential outliers to check
# mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Voice, m_dist) %>% inner_join(n2_ratings_vars_pca) %>% select(-Voice_Cat)
```

```{r}
# # All orthogonal/unique variables, excluding American
# scores_df <- n2_ratings_vars_pca %>% select(-Condition:-Voice) %>% 
#   select(Dim2, Dim3, Dim4) #, Feminine, Casual, Intelligent, Nerdy, `Clear speaker`)
# index_df <- n2_ratings_vars_pca %>% select(Condition:Voice)
# 
# # Get mahalnobis distances
# mdist_df <- get_mdist(scores_df, index_df)
# 
# # Most different (across conditions) excluding American
# # See results
# mdist_df %>% arrange(desc(m_dist))
# # Get list of potential outliers to check
# mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Voice, m_dist) %>% inner_join(n2_ratings_vars_pca) %>% select(-Voice_Cat)
```
```{r}
# # Most similar (across conditions) excluding American
# # See results
# mdist_df %>% arrange((m_dist))
# # Get list of potential matches to check
# mdist_ranks <- mdist_df %>% select(Condition, Voice, m_dist) %>% inner_join(n2_ratings_vars_pca) %>% select(-Voice_Cat) %>% 
#   mutate(m_dist_bins = case_when(
#     m_dist < 1 ~ "0-1", # 0-1
#     m_dist < 2 ~ "1-2", # 1-2
#     m_dist < 4 ~ "2-4", # 2-4
#     m_dist < 6 ~ "4-6", # 4-6
#     m_dist >= 6 ~ "6+", # 6+
#     TRUE ~ NA
#   ), .after=m_dist) %>%
#   arrange((m_dist)) %>% group_by(Condition) %>%
#   mutate(ingroup_ranks = order(order(m_dist, decreasing=FALSE)), .after=m_dist_bins) #%>% arrange(ingroup_ranks)
# mdist_ranks
# 
# # Option 1: Pivot to matched rank columns, Voice value for condition based on rank
# mdist_ranks %>%  pivot_wider(id_cols = ingroup_ranks, names_from = Condition, values_from = c(Voice, m_dist))
# 
# # Option 2: Bin columns, Voice value for condition based on rank
# mdist_ranks %>% count(Condition, m_dist_bins) %>% pivot_wider(names_from = Condition, values_from = n)
# mdist_ranks %>% pivot_wider(id_cols = c(m_dist_bins, Voice), names_from = Condition, values_from = c(m_dist))
```
