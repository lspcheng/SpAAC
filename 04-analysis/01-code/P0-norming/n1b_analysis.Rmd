---
title: "n1b"
output: html_document
---

# Preamble
## Packages
```{r setup}
# Load libraries and custom functions
if (file.exists("project_functions.R")){
  source("project_functions.R")
  
} else { # try one directory up
  source("../project_functions.R")
}

# Text Analysis
library(countrycode)
library(maps)
library(tidytext)

# Plots
library(hrbrthemes)

# Functions
quick_summarize <- function(df, col, na.rm=FALSE){
  col = enquo(col)
  df %>%  summarize(across(!!col, list(median= ~ median(.x, na.rm=na.rm), mean= ~ mean(.x, na.rm=na.rm),
                                       sd= ~ sd(.x, na.rm=na.rm), min= ~ min(.x, na.rm=na.rm), 
                                       max= ~ max(.x, na.rm=na.rm))))
}

```


## Pipeline Structure
```{r}
# Fill in file structure info (e.g. using getwd())
NAME <- 'n1b_analysis' ## Name of the R file (w/o file extension!)
PHASE <- 'P0-norming' ## Name of the project phase (if relevant)
PROJECT <- 'SpAAC' ## Name of project
```

```{r}
# Get project directory path & subfolder status from working dir
PROJECT_DIR <- str_extract(getwd(), paste0("^(.*?)",PROJECT,"/"))

if (basename(getwd()) != PHASE) {SUBFOLDER <- basename(getwd())} else {SUBFOLDER <- NA}

# Get pipeline path names
if (dir.exists(file.path(PROJECT_DIR, '04-analysis', '02-pipeline'))){
  if (is.na(SUBFOLDER)){
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, NAME)
  } else {
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, SUBFOLDER, NAME)
  }
} else {
  pipeline <- file.path('.', 'pipeline', PHASE, NAME)
}

# Create pipeline folders
if (!dir.exists(pipeline)) {
  dir.create(pipeline, recursive=TRUE)
  for (folder in c('out', 'store', 'temp')){
    dir.create(file.path(pipeline, folder))
  }
}
```

```{r}
# Basic reference paths
stim_data_path <- file.path(PROJECT_DIR, '02-materials', '02-stimuli', PHASE) 
ext_data_path <- file.path(PROJECT_DIR, '03-data', '01-external', PHASE) 
int_data_path <- file.path(PROJECT_DIR, '03-data', '02-internal', PHASE) 
manual_analysis_path <- file.path(PROJECT_DIR, '04-analysis', '03-manual', PHASE) # 001-code / 003-manual
```

# Process Main Data
```{r}
load(file.path(pipeline, "store", "n1b_responses_selected.RData"))
```

## Summarize & Explore
### Recognize
Sanity check that no people were recognized.
```{r}
n1b_responses_selected %>% filter(Question=="Recognize") %>% count(Response)
```
### (WIP) Descriptors
Try adding sentiments to each word entry: https://www.tidytextmining.com/sentiment.html
(See also: https://m-clark.github.io/text-analysis-with-R/sentiment-analysis.htm)
```{r}
library(tidytext)
n1b_descriptors <-
  n1b_responses_selected %>%
  filter(Question=="Impressions") %>%
  select(-Question:-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(Response)) %>%
  mutate(Response = mgsub(Response, c("-"), c(""), fixed=TRUE)) %>%
  mutate(Response = mgsub(Response, c("inetlligent", "independant"), c("intelligent", "independent"), fixed=TRUE)) %>% # typos
  # Add counts
  add_count(Image, name="total_image_n") %>% add_count(Response, name="total_word_n") %>% 
    add_count(Image, Response, name="image_word_n") %>%
  # Add sentiments
  rename(word=Response) %>% 
  left_join(get_sentiments("bing")) %>% left_join(get_sentiments("afinn"))  %>%
  rename(Response=word, sentiment_score=value) %>%
  # mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment),
         # sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score))
  mutate(sentiment = case_when((is.na(sentiment) & (sentiment_score > 0)) ~ "positive",
                               (is.na(sentiment) & (sentiment_score < 0)) ~ "negative",
                               TRUE ~ sentiment))
n1b_descriptors
```
```{r}
# Check for certain types of patterns to clean
# n1b_descriptors %>%  filter(Response == str_match(Response, ".* .*"))
```

#### Sentiment
```{r}
# Summarize sentiment data
n1b_descriptors %>% count(sentiment)
n1b_descriptors %>% count(sentiment_score)
n1b_descriptors %>% quick_summarize(sentiment_score, na.rm=TRUE)

# By group
n1b_descriptors %>% drop_na(sentiment) %>% add_count(Condition, name="group_n") %>% count(Condition, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n)
n1b_descriptors %>% group_by(Condition)  %>% quick_summarize(sentiment_score, na.rm=TRUE) %>% rename_with( ~ gsub("_score", "", .x))
```

```{r}
#By image
n1b_descriptors %>% drop_na(sentiment) %>% add_count(Image, name="group_n") %>% 
  count(Image, Condition, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n) %>%
  pivot_wider(id_cols=Image:Condition, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum)

n1b_descriptors %>% group_by(Image)  %>% quick_summarize(sentiment_score, na.rm=TRUE) %>% rename_with( ~ gsub("_score", "", .x))
```

#### Sets
```{r}
# Total descriptor words
n1b_descriptors %>% count(name="total_words")

# Total unique/different words
n1b_descriptors %>% count(Response) %>% count(name="unique_words")

# Total unique/different words per condition
n1b_descriptors %>% count(Condition,Response) %>% count(Condition, name="unique_words_per_condition")
```

```{r}
# Top words used
n1b_descriptors %>% count(Response, sort=TRUE)

# Top words used for a certain image
n1b_descriptors %>% count(Image, Response, sort=TRUE)

# Top words used for a certain condition's images
n1b_descriptors %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

```{r}
ethAsian_words <- n1b_descriptors %>% filter(Condition=="ethAsian") %>% count(Condition, Response, sort=TRUE)
msAsian_words <- n1b_descriptors %>% filter(Condition=="msAsian") %>% count(Condition, Response, sort=TRUE)
msWhite_words <- n1b_descriptors %>% filter(Condition=="msWhite") %>% count(Condition, Response, sort=TRUE)

# Shared words
Asian_words <- intersect(select(ethAsian_words, Response), select(msAsian_words,Response)) 
shared_words <- intersect(Asian_words, select(msWhite_words, Response)) 
ms_words <- intersect(select(msWhite_words, Response), select(msAsian_words,Response))
nonmsA_words <- intersect(select(msWhite_words, Response), select(ethAsian_words,Response))

shared_words %>% rename("Words in all personae"=Response)
nonmsA_words %>% setdiff(.,shared_words) %>% rename("Words only in both msWhite and ethAsian"=Response)
ms_words %>% setdiff(.,shared_words) %>% rename("Words only in Mainstream personae (both msWhite and msAsian)"=Response)
Asian_words %>% setdiff(.,shared_words) %>% rename("Words only in Asian personae (both ethAsian and msAsian)"=Response)
```
```{r}
# Different words
ethAsian_words_only <- setdiff(select(ethAsian_words, Response), select(msAsian_words, Response)) %>% setdiff(., select(msWhite_words, Response)) 
msAsian_words_only <- setdiff(select(msAsian_words, Response), select(ethAsian_words, Response)) %>% setdiff(., select(msWhite_words, Response)) 
msWhite_words_only <- setdiff(select(msWhite_words, Response), select(ethAsian_words, Response)) %>% setdiff(., select(msAsian_words, Response)) 

ethAsian_words_only %>% rename("Words in ethAsian only"=Response)
msAsian_words_only%>% rename("Words in msAsian only"=Response)
msWhite_words_only %>% rename("Words in msWhite only"=Response)
```

```{r}
# Show individual condition top words (excluding single occurances)
ethAsian_words %>% filter(n>1) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")
msAsian_words %>% filter(n>1) %>%ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msAsian adjectives", x= "Responses", y="No. of occurences")
msWhite_words %>% filter(n>1) %>%ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msWhite adjectives", x= "Responses", y="No. of occurences")
```
```{r}
# Show individual condition unique top words (excluding single occurances)
ethAsian_words %>% filter(n>1) %>% filter(Response %in% pull(ethAsian_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")
msAsian_words %>% filter(n>1)  %>% filter(Response %in% pull(msAsian_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msAsian adjectives", x= "Responses", y="No. of occurences")
msWhite_words %>% filter(n>1)  %>%  filter(Response %in% pull(msWhite_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msWhite adjectives", x= "Responses", y="No. of occurences")
```




### Age
Get proportion selections per age category for each image.
```{r}
 n1b_age <- n1b_responses_selected %>% 
  filter(Question=="Age") %>% 
  pivot_wider(id_cols = c(PROLIFIC_PID, Condition, Image), names_from = Question_Part, values_from = Response) %>% 
  mutate(across(Teens:`Over 40`, ~ as.numeric(as.character(.x)))) %>% 
  # Get proportions out of total responses (props add up to over 100 b/c multiple selections)
  add_count(Image) %>% group_by(Condition, Image, n) %>% 
  summarize(across(Teens:`Over 40`, sum)) %>% 
  mutate(across(Teens:`Over 40`, ~ .x/n)) %>%
  pivot_longer(Teens:`Over 40`, names_to = "Age", values_to = "prop") %>%
  mutate(Age=gsub("Teens", "10s", Age)) %>%
  # Drop values where none selected
  filter(prop>0) %>%  arrange(-prop, .by_group=TRUE) %>% 
  filter(prop>=0.75) # Option 1: Keep values where more than 75% of participants selected
  # slice_max(prop) # Option 2: Keep only highest value (including ties)
n1b_age <- n1b_age %>%
  full_join(
    # Get age consensus based on majority decision
   n1b_age %>% pivot_wider(Image:n, names_from = Age, values_from = prop) %>% 
   # relocate(`10s`, .before=`20s`) %>%
   mutate(across(2:last_col(), ~ ifelse(!is.na(.x), cur_column(), NA) )) %>%
   unite(perceived_age, 3:last_col(), sep=',', na.rm=TRUE)
  ) %>% ungroup
n1b_age
```

### Occupation
```{r}
n1b_occupation <-
  n1b_responses_selected %>% filter(Question=="Occupation") %>% 
  # text cleaning
  mutate(Response = tolower(Response)) 
  
n1b_occupation %>% count(Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_occupation %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

### Activities
```{r}
n1b_activities <-
  n1b_responses_selected %>% filter(Question=="Activities") %>% 
  # text cleaning
  mutate(Response = tolower(Response)) 
  # Split responses by comma, get multiple long responses per row/image, then count
  
n1b_activities %>% count(Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_activities %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

### (+) GrewUp
```{r}
n1b_grewup <-
  n1b_responses_selected %>% filter(Question=="GrewUp") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = str_trim(tolower(Response))) %>%
  mutate(Response = gsub("(.*)\\s*,\\s*usa", "\\1", Response)) %>% # keep everything before ', usa'
  mutate(Response = mgsub(Response, c("philipinnes|phillippines", "america|^us$|^in the us$|^united states$", "(?:^|\\W)(ca|cali)(?:$|\\W)", "(?:^|\\W)ny(?:$|\\W)", "(?:^|\\W)az(?:$|\\W)", "wisconson"), c("philippines", "usa", "california", "new york", "arizona", "wisconsin"))) %>% 
  # Add categorization into US or Asia or Europe
  mutate(Region = case_when(
    Response %in% c("san francisco", "los angeles", "boston", "beverly hills", "san fernando valley", "las vegas") ~ "USA",
    str_detect(Response, paste0("(?:^|\\W)", paste(tolower(state.name), collapse = '|'), "(?:$|\\W)")) ~ "USA",
    str_detect(Response, "(?:^|\\W)(mid(\\s*|-*)west|pacific northwest|(east|west) coast|the west|(west|east)ern states)(?:$|\\W)") ~ "USA", 
    str_detect(Response, "(?:^|\\W)usa(?:$|\\W)") ~ "USA", # if USA or <other>, defaults to US
    str_detect(Response,"(?:^|\\W)canada(?:$|\\W)") ~ "Canada",
    str_detect(Response, "(?<!\\w)(asia(n*))(?!\\w)") ~ "Asia",
    str_detect(Response, "(?<!\\w)europe(?!\\w)") ~ "Europe",
    TRUE ~ NA
  )) 
n1b_grewup <- n1b_grewup %>%
  mutate(CountryContinent = countrycode(sourcevar = Response,
                             origin = "country.name",
                             destination = "continent")) %>%
  cbind(CityCountry = world.cities[match(n1b_grewup$Response, tolower(world.cities$name)), ][[2]]) %>%
  
  mutate(Response_Cat = coalesce(coalesce(Region, CountryContinent), CityCountry), .after=Response) %>%
  mutate(across(where(is.character), as.factor))


# Check summary
summary(n1b_grewup)

# Check categorization
n1b_grewup %>% filter(Response_Cat == "USA") %>% count(Response_Cat, Response, sort=TRUE)
n1b_grewup %>% filter(Response_Cat == "Asia") %>% count(Response_Cat, Response, sort=TRUE)
n1b_grewup %>% filter(!(Response_Cat == "USA" | Response_Cat == "Asia")) %>% count(Response_Cat, Response, sort=TRUE)
# Check for NAs (uncategorized)
# n1b_grewup %>% filter(is.na(Region) & is.na(CountryContinent) & is.na(CityCountry) )
n1b_grewup %>% filter(is.na(Response_Cat) )
```

```{r}
# Check for errors
# world.cities %>% filter(country.etc=="USA") %>% mutate(name=tolower(name)) %>% filter(str_detect(name, "washington"))
```

```{r}
# Check data overall
# Specific Labels
n1b_grewup %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_grewup %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_grewup %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_grewup %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```
All personae categories sometimes identified as USA

msWhite only USA (or Canada), Europe
msAsian and ethAsian only USA (or Canada) or Asian. More msAsian = USA, and more ethAsian = Asia

```{r}
# Check Location category of photos per condition

n1b_grewup %>% filter(Condition=="msWhite") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_grewup %>% filter(Condition=="msAsian") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_grewup %>% filter(Condition=="ethAsian") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```

**Review Comments:**
msWhite photos were all identified as growing up in USA (or Canada) at least once (only other responses were Europe [n=2]; none were Asia).

msAsian photos were all identified as growing up in USA (or Canada) at least once, while some were also identified as from Asia (none were Europe)

ethAsian photos were variably identified as growing up in the USA or Asia (none were Europe)

**Selection Criteria:**
- msAsian must be at least identified as USA (TO CONSIDER: Mostly, select those ONLY USA because including Asia could bleed into ethAsian category, but this might depend more on the ratings)
- ethAsian must be identified as USA + Asia (not ONLY Asia, b/c that would be Asia-Asian; not ONLY USA, because that would be too msAsian)

### (+) Ethnicity
```{r}
n1b_ethnicity <-
  n1b_responses_selected %>% filter(Question=="Ethnicity") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(str_trim(Response))) %>%
  mutate(Response = mgsub(Response, c("china|chiense", "flippino|philipino|philipno"), c("chinese", "filipino"))) %>% 

  # Add categorization into race
  mutate(Asian = case_when(
    str_detect(Response, "(?:^|\\W)(asia(n*)|chinese|filipin(o|a))(?:$|\\W)") ~ "Asian",
    str_detect(Response, "(?:^|\\W)(hong kong|singapore|japan|korean|taiwan|vietnam|thai|hmong|lao|malaysian)") ~ "Asian",
    TRUE ~ NA
  )) %>%
  mutate(White = case_when(
    str_detect(Response, "(?:^|\\W)(white|european|caucasian|anglo|english|finnish|german|irish|italian|scandinavian|scottish|swedish)(?:$|\\W)") ~ "White",
    TRUE ~ NA
  )) %>%
  mutate(American = ifelse(str_detect(Response, "(?:^|\\W)american(?:$|\\W)"), "American", NA)) %>%
  # mutate(Other = case_when(
    # str_detect(Response, "(?:^|\\W)(mexi|hispanic)") ~ "Other",
    # str_detect(Response, "(?:^|\\W)(half|and|mixed)(?:$|\\W)") ~ "Mixed",
    # TRUE ~ NA )) %>%
  mutate(Response_Cat = coalesce(coalesce(coalesce(Asian, White), American), "Other"), .after=Response) %>% # if Asian mixed, Asian
  mutate(across(where(is.character), as.factor))
# n1b_ethnicity

# Check for NAs (uncategorized)
n1b_ethnicity %>% filter(is.na(Response_Cat) )

# Check summary
summary(n1b_ethnicity)

# Check categorization
n1b_ethnicity %>% filter(Response_Cat == "Asian") %>% count(Response_Cat, Response, sort=TRUE)
n1b_ethnicity %>% filter(Response_Cat == "White") %>% count(Response_Cat, Response, sort=TRUE)
n1b_ethnicity %>% filter(!(Response_Cat == "Asian" | Response_Cat == "White")) %>% count(Response_Cat, Response, sort=TRUE)
```

```{r}
# Check data overall
# Specific Labels
n1b_ethnicity %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_ethnicity %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_ethnicity %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_ethnicity %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```

```{r}
# Check  category of photos per condition
n1b_ethnicity %>% filter(Condition=="msWhite") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_ethnicity %>% filter(Condition=="msAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_ethnicity %>% filter(Condition=="ethAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```
**Review Comments:**
msWhite photos were all identified as White (or American) at least once (Other responses [n=2]).

msAsian photos were all identified as Asian only (exception: one photo also Other = 'hispanic')

ethAsian photos were  all identified as Asian only

**Selection Criteria:**
- msWhite must be identified as White only
- msAsian and ethAsian must be identified as Asian only
  - PLUS: should additional check East vs. Southeast asian representation within msAsian and ethAsian so that it isn't the difference (e.g., Southeast Asian in msAsian but not ethAsian)

To pick photos: Check a specific photo's words
```{r}
n1b_ethnicity %>% filter(Image=="AAW-12") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)
n1b_ethnicity %>% filter(Image=="AAW-36") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)
n1b_ethnicity %>% filter(Image=="AAW-47") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)

```

### (+) Speech
```{r}
n1b_speech <- 
  n1b_responses_selected %>% filter(Question=="Speech") %>% select(-Question_Part) %>%
    # text cleaning
  mutate(Response = tolower(str_trim(Response)))  %>%
  # Consider: shorten long statements by extracting the keyword (e.g. straightforward)
# Add categorization into accentedness
  mutate(OtherAccent = case_when(
    str_detect(Response, "(?:^|\\W)((english|southern( drawl)*|mid(\\s*|-*)west(ern)*) accent)(?:$|\\W)") ~ "OtherAccent",
    TRUE ~ NA
  )) %>%
  mutate(Accented = case_when(
    str_detect(Response, "(?:^|\\W)(accented|(with|have) a(.+\\s)accent|(aapi|hawaiian|asian|foreign|heritage('s)*|chinese) accent|(broken|poor(-ish)*) english)(?:$|\\W)") ~ "Accented",
    TRUE ~ NA
  )) %>%
  mutate(Unaccented = case_when(
    str_detect(Response, "(?:^|\\W)(standard|unaccented|without an accent|no accent|(american|canadian|california|neutral) accent|typical american|(california) style|normal|perfect english|native speaker)(?:$|\\W)") ~ "Unaccented",
    # str_detect(Response, "(?:^|\\W)((fluent|clear) english)(?:$|\\W)") ~ "Unaccented",
    TRUE ~ NA
  )) %>%

  mutate(Response_Cat = coalesce(coalesce(coalesce(OtherAccent, Unaccented), Accented), "Other"), .after=Response) %>% 
  mutate(across(where(is.character), as.factor))
# n1b_speech

# Check for NAs (uncategorized)
n1b_speech %>% filter(is.na(Response_Cat) ) %>% select(Response:last_col())

# Check summary
summary(n1b_speech)

# Check categorization
n1b_speech %>% filter(Response_Cat == "Unaccented") %>% count(Response_Cat, Response, sort=TRUE)
n1b_speech %>% filter(Response_Cat == "Accented") %>% count(Response_Cat, Response, sort=TRUE)
n1b_speech %>% filter(!(Response_Cat == "Unaccented" | Response_Cat == "Accented")) %>% count(Response_Cat, Response, sort=TRUE)
```

Accentedness:
```{r}
# Check data overall
# Specific Labels
n1b_speech %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_speech %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```

```{r}
# Check category of photos per condition
n1b_speech %>% filter(Condition=="msWhite") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_speech %>% filter(Condition=="msAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_speech %>% filter(Condition=="ethAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```

```{r}
# Check specific photos/labels
n1b_speech %>% mutate(Response=as.character(Response)) %>% 
  filter(str_detect(Response, "(?:^|\\W)(south)"))
```
NOTE: WAW-1 was mentioned by 2 participants as possibly having a southern accent == good for my distractor stimuli (or avoid, because this photo will behave differently?)

#### Styles
```{r}
n1b_speech <- n1b_speech %>%
  # Other styles
  mutate(Style = case_when(
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(softly|soft spoken|quiet|mild|high)") ~ "Delicate", # possibly polite
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(deliberate|formal|polite)") ~ "Reserved",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(casual|conversational|slang)") ~ "Casual",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(fast|quick|rapid)") ~ "Fast",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(slow|drawl)") ~ "Slow",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(direct|straight|straightforward|clear manner|concise)") ~ "Direct",
    TRUE ~ NA
  ))

# Check Styles data
n1b_speech %>% filter(Style == "Delicate") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Reserved") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Fast") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Direct") %>% count(Style, Response, sort=TRUE)


# Categorized Labels
n1b_speech %>% count(Style, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Style, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```



### (+) Ratings
```{r}
n1b_ratings <-
  n1b_responses %>%
  select(PROLIFIC_PID, Question_Number, Question, Question_Part, Condition, Image_Cat, Image, Response) %>%
  filter(Question=="Ratings") %>% select(-Question) %>% mutate(Response =  as.numeric(as.character(Response))) %>%
  # Convert scales and responses to consistent direction
  separate(Question_Number, into=c("Question_Number", "Version"), sep="-") %>%
separate(Question_Number, into=c(NA, "Rating_Group"), sep="\\.") %>% 
  mutate(Rating_Group = case_when(Rating_Group == "8" ~ "Style",
                                  Rating_Group == "9" ~ "Traits",
                                  Rating_Group == "10" ~ "Culture",
                                  TRUE ~ NA)) %>%
  separate(Question_Part, into=c("Left", "Right"), sep=":") %>%
  mutate(Rating_Scale = case_when(Rating_Group == "Style" & Version == "A" ~ Left,
                                  Rating_Group == "Traits" & Version == "A" ~ Right,
                                  Rating_Group == "Culture" & Version == "A" ~ Left,
                                  Rating_Group == "Style" & Version == "B" ~ Right,
                                  Rating_Group == "Traits" & Version == "B" ~ Left,
                                  Rating_Group == "Culture" & Version == "B" ~ Right,
                                  TRUE ~ NA), .before=(Rating_Group))  %>%
  mutate(Rating_Value = case_when(Rating_Group == "Style" & Version == "A" ~ abs((Response)-8),
                                  Rating_Group == "Traits" & Version == "B" ~ abs((Response)-8),
                                  Rating_Group == "Culture" & Version == "A" ~ abs((Response)-8),
                                  TRUE ~ Response), .before=(Response)) %>%
  select(-Version, -Left, -Right) %>% relocate(c(Rating_Group,Rating_Scale), .before=Rating_Value) %>%
  # Add z-score transformed ratings by participant (participant-normalized)
  group_by(PROLIFIC_PID) %>% 
  mutate(Rating_ZScore = scale(Rating_Value, center=TRUE, scale=TRUE), .before=Rating_Value) %>%
  mutate(Response_ZScore = scale(Response, center=TRUE, scale=TRUE)) %>% 
  ungroup() %>%
  mutate(across(where(is.character), as.factor))

summary(n1b_ratings)
n1b_ratings
```
Summary stats
```{r}
# Check summary stats of data overall
n1b_ratings %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
# n1b_ratings %>% summarize(min=min(Rating_ZScore), max=max(Rating_ZScore), mean=mean(Rating_ZScore), median=median(Rating_ZScore))

# by rating group
n1b_ratings %>% group_by(Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

# by condition
n1b_ratings %>% group_by(Condition, Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
n1b_ratings %>% group_by(Condition, Rating_Scale) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

# by image
n1b_ratings %>% group_by(Image, Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
n1b_ratings %>% group_by(Image, Rating_Scale) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

```

Means per
```{r}
# Check  category of photos per condition + sd
# Tables
n1b_ratings %>% group_by(Condition, Rating_Group) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd) %>%
  pivot_wider(names_from=Condition, values_from = `Mean (SD)`)
  
n1b_ratings %>% group_by(Condition, Rating_Group, Rating_Scale) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd) %>%
  pivot_wider(names_from=Condition, values_from = `Mean (SD)`)
```



Plot Functions:
```{r}
# Parallel Plot Function
# FUNCTION REF: https://stackoverflow.com/questions/52340768/using-a-custom-function-with-tidyverse 

parallel_plot <- function(df, x, y, group, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <-
    df %>% 
    group_by((!!group), (!!x)) %>% 
    
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group), group=(!!group)))  +
    stat_summary(fun.data=mean_se, geom="pointrange", position=position_dodge(), alpha=1) +
    stat_summary(fun.data=mean_se, geom="line", position=position_dodge(), alpha=1) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    theme_minimal()
    
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}

# Violin Plot Function
# FUNCTION REF: https://stackoverflow.com/questions/52340768/using-a-custom-function-with-tidyverse 

violin_plot <- function(df, x, y, group, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <-
    df %>% group_by((!!group), (!!x)) %>% 
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group)))  +
    geom_violin(alpha=0.5, position=position_dodge(0.95)) +
    geom_boxplot(fill="white", alpha=0.9, width=0.2, position=position_dodge(0.95)) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    scale_fill_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    theme_minimal()
  
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
     theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}
```


Parallel plots show the differences in score (=y, e.g., rating values) across score type (=x, e.g., rating scales) for each group (=series as represented by color and line, e.g., condition).

#### By Condition

Overview of ratings grouping scales under Rating_Group (Culture, Traits, Style)
```{r}
# Condition Mean Plots by Rating_Group
# Get plots: Zoomed in (Raw and ZScore)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition)

# Get plots: Zoomed out (Raw and ZScore)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition, full_scale = TRUE)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition, full_scale = TRUE)
```
```{r}
# Condition Distribution + Median Violin Plots by RatingGroup
violin_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE)
violin_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE)

```

Overview of all scales shown individually, reordered into helpful viewing order
```{r}
# Condition Mean Parallel Plots by Rating_Scale
# Get plots: Zoomed in (Raw and ZScore)
n1b_ratings_releveled <-
  n1b_ratings %>% 
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale, 
  "American", "Native speaker of English", "American-accented", "Aligned with American culture",  # Culture
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent",               # Traits
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker"))                         # Style

parallel_plot(n1b_ratings_releveled, x=Rating_Scale, y=Rating_Value, group=Condition, angle_axis_labels=TRUE) 
parallel_plot(n1b_ratings_releveled, x=Rating_Scale, y=Rating_ZScore, group=Condition, angle_axis_labels=TRUE) 
```

Notes:
- msAsian shouldn't be less attractive—pick matching where possible
- msWhite shouldn't be less likable—pick matching where possible
- msAsian shouldn't be more Friendly

- I think Casual shouldn't differ between MS groups, if possible
- I think Feminine shouldn't differ for msAsian only, if possible (but maybe that is part of the persona look?)


```{r}
# Condition Distribution + Median Violin Plots by Rating_Scale
n1b_ratings_releveled %>% filter(Rating_Group == "Culture") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n1b_ratings_releveled %>% filter(Rating_Group == "Traits") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n1b_ratings_releveled %>% filter(Rating_Group == "Style") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)
```

#### By Image
```{r}
# Add all images mean plots to find outliers to drop (or matched pairs to keep)

```


```{r}
# Parallel Plot exploration
current_condition = "msWhite"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)

current_condition = "msAsian"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)

current_condition = "ethAsian"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)
```


```{r}
# Add image direct comparision plots


```

## Process
### Get By-Image Data
Get general screening filters per image, merge into one.
```{r}
n1b_percepts_general <- 
  n1b_descriptors %>% group_by(Image) %>% summarize(sentiment_score=mean(sentiment_score, na.rm=TRUE)) %>%
  full_join(
    n1b_descriptors %>% drop_na(sentiment) %>% add_count(Image, name="group_n") %>% 
  count(Image, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n, -n) %>%
  pivot_wider(id_cols=Image, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum) %>%
  rename(positive_prop=positive, negative_prop=negative)
  ) %>%
  full_join(
    n1b_age %>% select(Image, perceived_age)
  )

# n1b_occupation
# n1b_activities
n1b_percepts_general
```

Get persona screening filters per image, merge into one.
```{r}
n1b_percepts_personae <- 
  n1b_grewup %>% count(Image, Response_Cat) %>% drop_na(Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, from_asia_prop=Asia, from_us_prop=USA) %>%
  full_join(
    n1b_ethnicity %>% count(Image, Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, asian_prop=Asian, white_prop=White)
  ) %>%
  full_join(
    n1b_speech %>% count(Image, Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, accented_prop=Accented, unaccented_prop=Unaccented)
  )
n1b_percepts_personae

```


Get mean general+persona rating scores per image, to put through a PCA.
```{r}
# By-Image means: Get across-participant means per Rating_Scale for each Image 
n1b_ratings_means <- n1b_ratings_releveled %>% group_by(Condition, Image_Cat, Image, Rating_Group, Rating_Scale) %>%
  summarize(Rating_Value=mean(Rating_Value), Rating_ZScore = mean(Rating_ZScore)) %>% ungroup()

# Get factors as columns
# Raw rating values
n1b_ratings_vars_raw <-
   n1b_ratings_means %>%  pivot_wider(id_cols=Condition:Image, names_from = Rating_Scale, values_from =  Rating_Value)

# Zscores
n1b_ratings_vars_zscore <-
  n1b_ratings_means %>%  pivot_wider(id_cols=Condition:Image, names_from = Rating_Scale, values_from = Rating_ZScore)

# View data
n1b_ratings_means
n1b_ratings_vars_raw
n1b_ratings_vars_zscore
```

Check correlations between measures to assess collinearity.

Function to get correlation info:
```{r, fig.width=8, fig.height=7.5}
get_correlations <- function(df, columns_to_keep){
  # NOTE: Columns to keep cannot be name of column, only indices or function (e.g. last_col())
  corr_matrix <-
    df %>% select(columns_to_keep) %>%
    cor(., method="pearson")  # Alt: method="spearman"
  print(corr_matrix)
  # # View the correlation matrix
  corr_df <- corr_matrix %>% as_tibble(rownames = "var") # independent variables correlation matrix 
  print(corr_df)
  # # Visualize
  corr_plot <- corrplot(corr_matrix, method='number',is.corr = T)
  print(corr_plot)
}
```

Raw rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n1b_ratings_vars_raw, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```
The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear. (OUTDATED: though Friendly and Likable are similar (>0.8) as well.)

ZScored rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n1b_ratings_vars_zscore, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```

The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear.


### Run PCA
Notes for PCA Interpretation
```{r}
#############################################
## (1) Checking the number of dimensions

# Parallel Analysis
# Standard way to decide on the number of factors or components needed in an FA or PCA.

# Eigenvalues & percent variance accounted for
# One guideline is to only include dimensions that have an eigenvalue of at least 1, but note that...
# This guideline only applies if using correlation matrix (i.e. scaled units; scale.unit = T)
# Because we don't want a factor that accounts for less than what a single variable accounts for (single variable=1)

# Scree Plot
# One guideline is to check starting with the "elbow" value, plus or minus 1
# Check for where there is an "elbow" where the plot bends, such that subsequent factors don't contribute much

## (2) Interpreting the output factor values

# Factor matrix (raw eigenvectors = the factor score coefficients; sometimes called the factor, but not factor scores)
# To interpret, focus on the most extreme factor values (or loadings, below)
# Higher values means that those variables contribute more to the specific dimension/component/factor
# Dimensions/components/factors can be interpreted based on which variables contribute more

# Factor loadings (eigenvectors scaled by the square root of their associated eigenvalues)
# Provide similar information about which variables contribute to each dimension/component/factor, but also
# Can be interpreted as correlations between each variable and the factor
# One guideline treats all values less than 0.3 as 0, thus drops them from consideration (irrelevant for that factor) 

# Rotated factor matrix
# Orthogonally rotates factor matrix for ease of interpretation of each dimension

#############################################
## (3) Checking the individual coordinate scores
# Individual coordinate scores (principle coordinates)
# Same as factor scores for each subject and dimension (weighted sum of all of a subjects raw scores, where the weights are the eigenvector values)
# i.e. values are calculated from the normalized variable scores (Z-scores) multiplied by the eigenvector weights, then summed
```

#### Select Data
Select data for PCA.
```{r}
# Use Zscores to represent responses without participant bias in scale range 
#   (e.g., never selecting 1 means their lowest value is represented by 2, but their 2 can be equated to another person's 1)
# Use Raw scores to represent responses as representing true value 
#   (e.g., never selecting 1 means the true perceptual range represented by the photos never goes down to 1 for this person, and their 1 is equated to another person's 1)

current_source_data <- n1b_ratings_vars_zscore # Alt: n1b_ratings_vars_raw

## Maximal set
image_vars_pca <- current_source_data %>% select(-Condition:-Image)
image_vars_pca
```

#### Process PCA
Determine number of components via Parallel analysis.
```{r }
## Relevant libraries
# `PCA` command from `FactoMineR` library (see index for more info)
# `paran` command from `paran` library
# `Varimax` command from `GPArotations` library (https://stats.stackexchange.com/questions/59213/how-to-compute-varimax-rotated-principal-components-in-r)

## (1) Run Parallel Analysis with `paran`
# Standard way to decide on the number of factors or components needed in an FA or PCA.
# Prints out a scree plot as well, with the randomized line + unadjusted line
paran(image_vars_pca,
      graph = TRUE, color = TRUE, 
      col = c("black", "red", "blue"), lty = c(1, 2, 3), lwd = 1, legend = TRUE, 
      file = "", width = 640, height = 640, grdevice = "png", seed = 0)
```
Parallel analysis suggests 2 components retained.
Scree plots suggest ~3 components, based on the location of the elbow. Could try 2, 3, or 4 components.
Eigenvalues suggest 2 components, as only the first two comps have a value above 1.
The _difference in_ eigenvalues suggests 3 components, as up to the third comp has a difference greater than 1. (See: https://stats.stackexchange.com/questions/450752/understanding-how-many-components-to-include-for-pca)
Interpretability indicates 3 components, such that FrCA serves as its own component (Dim3).

```{r}
## (2) Run PCA with `FactoMineR`
# ncp = number of components; adjust after checking the parallel analysis output

# FactoMineR PCA Commands
#score_PCA        # lists commands
#score_PCA$var    # variables
#score_PCA$ind    # individuals
#score_PCA$call   # summary stats

# Conduct PCA with scaling/standardizing
score_PCA <- PCA(image_vars_pca, scale.unit = T, ncp =3, graph=T)

## Relevant Raw PCA Output
# Eigenvalues & percent variance accounted for
eigenvalues <- score_PCA$eig
as_tibble(eigenvalues, rownames="components")

# Eigenvectors (=Factor matrix, factor score coefficients, principal directions, principal axes; sometimes called the factor, but NOT factor scores; these are called loadings by some but its incorrect).
eigenvectors <- score_PCA$var$coord
as_tibble(eigenvectors, rownames="Score")

# Factor scores for each subject and dimension (also: Individual coordinate scores; principle coordinates)
rawScores <- score_PCA$ind$coord
as_tibble(rawScores, rownames="Item")

# Factor loadings (=loadings, correlation loadings, or scaled factor coefficients; eigenvectors scaled by the square root of their associated eigenvalues)
# Calculate factor loadings using the output eigenvectors and eigenvalues (i.e. divide each eigenvector column value by the appropriate eigenvalue square root).
rawLoadings <- sweep(eigenvectors,MARGIN=2,STATS=sqrt(eigenvalues[1:ncol(eigenvectors),1]),FUN="/") # margin 1=rows, 2=cols
as_tibble(rawLoadings, rownames="Score")
```




```{r lbqdata-pca2-3}
## (3) Conduct rotation on the PCA factor loadings with `GPArotation`
# Rotations are typically done on the retained component factor loadings, not on all components nor on the eigenvectors
# Performed for ease of interpretation, maximizing factor loadings
rotLoadings <- Varimax(rawLoadings, normalize=T)$loadings
as_tibble(rotLoadings, rownames="Score") 

# Recover Rotation matrix from loadings
# Because the rotLoadings are calculated from rawLoadings %*% rotMatrix, can recover rotMatrix by rotLoadings "divided" by rawLoadings, which in matrix multiplication is multiplying by the inverse (transpose) 
# Note: For some reason, can't call Varimax(rawLoadings)$rotmat (just get NULL); this recreates the same matrix from Varimax(rawLoadings)
rotMatrixL <- t(rawLoadings) %*% rotLoadings
as_tibble(rotMatrixL, rownames="Dimensions")

# Calculate rotated factor scores
# The formula simply multiplies the normalized variable scores with the rotation matrix to get rotated factor scores
# First, z-score the raw scores using base R scale()
# Then, matrix multiply the matrix of zScores with the rotation matrix
# Result is a matrix with columns=components and rows=each subject
zScores <- scale(rawScores)
rotScores <- zScores %*% rotMatrixL
as_tibble(rotScores, rownames="Item")

```

```{r}
# For comparison, a different rotation function. Requires normalization. Still a little different but similar enough. Provides rotmat and SS loadings. 
## Rotate eigenvectors (not typical, according to Stats notes and StackExchange, but reasonable-looking SS loadings+% variance)
# stats::varimax(eigenvectors)
## Rotate factor loadings (typical, but unreasonable(?)-looking SS loadings+% variance)
stats::varimax(rawLoadings)
```

#### Factor Loadings
```{r}
# Replace all factor loading values under |0.3| with 0 for better readability

# Raw Loadings
as_tibble(rawLoadings, rownames="Variable") %>% 
  mutate(across(where(is.numeric), ~ ifelse(abs(.x)<0.3, 0, .x)))

# Rotated Loadings
as_tibble(rotLoadings, rownames="Variable") %>% 
  mutate(across(where(is.numeric), ~ ifelse(abs(.x)<0.3, 0, .x)))
```

#### PCA: Raw Plots
```{r lbqdata-pca2-4, echo=F}
## (4) Data Visualization of Raw Scores with `factoextra`

# Plot individual factor scores
fviz_pca_ind(score_PCA, col.ind = "#00AFBB", repel = TRUE)

# Biplot, including individual scores and factor vectors
fviz_pca_biplot(score_PCA, label = "all", col.ind = "#00AFBB", col.var="black", ggtheme = theme_minimal())
```

#### PCA: Rotated Plots
```{r plbqdata-pca2-5, echo=F}
## (5) Manual Plots of Rotated Scores with `ggplot`

## Create dataframes of the rotated factor loading and factor score matrices

# Convert rotated factor loadings matrix to data frame; add variable number
rotLoadingsData <- as.data.frame(rotLoadings)
rotLoadingsData <- mutate(rotLoadingsData, variable = row.names(rotLoadings))
rotLoadingsData <- mutate(rotLoadingsData, variable = factor(variable))
#rotLoadingsData

# Convert rotated factor score matrix to data frame; add subject number
rotScoreData <- as.data.frame(rotScores)
rotScoreData <- mutate(rotScoreData, subject = 1:n())
rotScoreData

## Create base plots
# Loading plot
loadingplot <- ggplot(rotLoadingsData, aes(x=Dim.1, y=Dim.2))+
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4, y=Dim.2*4, label=variable), color="red",check_overlap=T) +
  scale_x_continuous(lim=c(-2.5, 2.5),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-2, 3),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Variables - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))
loadingplot


# Scatter plot of Individual factor scores
dimplot = ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  scale_x_continuous(lim=c(-2.5, 2.5),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Individuals - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))
dimplot

## Merge loading and score plot = Biplot

# Biplot of factor loadings + ind factor scores
ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  
  # Overlay loading plot (i.e. arrows)
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4.5, y=Dim.2*4.5, label=variable), color="red",check_overlap=T, nudge_y = 0)+

  # scale_x_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  # scale_y_continuous(lim=c(-4.5, 4.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Biplot - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))


```
~ PILOT NOTES as sample ~
NOTE: Both ZScore rotated and unrotated are very similar (for pilot data; however, big difference for Raw ratings)

The results of the Zscore rotated PCA with three components retained suggest that:
* Dim1 is the "culture" or "American" component, with all four of those American-related ratings being highly correlated.

* Dim2 is the "social attractiveness" or "likeability" component, linked to positive traits like Attractive, Likeable (strongest), Friendly, Confident, and Enthusiastic (when rotated), as well as confidence-related stylistic features like Cool and Fast speaker (i.e., non-Slow speaker)

* The other features that act more like their factors rather than a grouped/similar/collinear unit are:
  * Intelligent
  * Nerdy (vs Enthusiastic, when not rotated)
  * Casual
  * Feminine — definitely seems more like a third dimension
  

Based on the biplots, where is the best slice to take matched participants from? 
* Since Dim1 is the relevant distinguishing component, we'd want to take different personae across this dimension — i.e., this should differ significantly across Personae conditions (ethAsian vs. msAsian / msWhite)
* Since Dim2 (and others) are the "flavor" non-target characteristics to ensure social perceptual similarity on, those should be similar, maybe towards the mean around 0 (i.e., visually, along the horizontal x axis line is where we should pick from).

In addition, use the Ethnicity, GrewUp, and Speech scores to screen for outliers or non-matching aspects. So, we don't want to include even if they might match in ratings but: 
* e.g., differ in being East Asian vs. Southeast Asian or 
* e.g., differ in where they are from like always California vs. never California

#### Combine Data
```{r}
# Merge PCA factor scores back to image number and raw scores
n1b_ratings_vars_pca <-
  n1b_ratings_vars_zscore %>%
  cbind(
    as_tibble(rotScores, rownames="Item"), . # alternatively rawScores
  ) %>%
relocate(Condition:Image,
       Dim1=Dim.1, Dim2=Dim.2, Dim3=Dim.3 #, Dim4=Dim.4
       ) %>% select(-Item)
n1b_ratings_vars_pca
```

```{r}
# Summary stats of each PC/Dimension
quick_summarize(n1b_ratings_vars_pca, Dim1)
quick_summarize(n1b_ratings_vars_pca, Dim2)
quick_summarize(n1b_ratings_vars_pca, Dim3)
# quick_summarize(n1b_ratings_vars_pca, Dim4)
```

#### Visualize Data
Function for labelled scatterplot of variables
```{r}
labeled_scatterplot <- function(df, x, y, label, group, show_points=TRUE, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  label = enquo(label)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <- df %>% group_by((!!group), (!!x)) %>% 
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group), label=(!!label)))
  if (show_points == TRUE){
    plot <- plot + geom_point(alpha=0.7)
  }
  plot <- plot +
    geom_text(alpha=0.9, nudge_x = 0.2, nudge_y = 0.2) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    scale_fill_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    gg_theme() + theme_minimal()
  
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
     theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}
```

```{r}
# # Get variable names
# colnames(n1b_ratings_vars_pca)
# 
# # Double check raw zscore correlations for select variables
# n1b_ratings_vars_pca %>% filter(Condition=="ethAsian") %>%
#   ggplot(aes(y=American, x=`American-accented`)) +  geom_point() + geom_smooth(method="lm") + theme_minimal()
# 
# n1b_ratings_vars_pca %>% filter(Condition=="msAsian") %>%
#   ggplot(aes(y=American, x=Likeable)) +  geom_point() + geom_smooth(method="lm") + theme_minimal()
```

```{r}
# # Double check PCA correlations
# n1b_ratings_vars_pca %>%
#   ggplot() +
#   # facet_wrap(~Image) +
#   geom_point(aes(y=Dim1, x=American)) +
#   geom_point(aes(y=Dim1, x=`Native speaker of English`), col="blue", alpha=0.7) +
#   geom_point(aes(y=Dim1, x=`American-accented`), col="red", alpha=0.7) +
#   geom_point(aes(y=Dim1, x=`Aligned with American culture`), col="orange", alpha=0.7) +
#   theme_minimal()

```

Get plot of main Dimensions with the labelled images to review the selections below
```{r}
# Main Dimensions
# Specific Images colored by Condition
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim2, label=Image, group=Condition, show_points=FALSE)
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim3, label=Image, group=Condition, show_points=FALSE)
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim4, label=Image, group=Condition, show_points=FALSE)

n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim2, y=Dim3, label=Image, group=Condition, show_points=FALSE)
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim2, y=Dim4, label=Image, group=Condition, show_points=FALSE)

# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim3, y=Dim4, label=Image, group=Condition, show_points=FALSE)

```
```{r}
# Other
# Specific Images colored by Condition
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Feminine, label=Image, group=Condition, show_points=FALSE)
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Intelligent, label=Image, group=Condition, show_points=FALSE)
```

### Merge By-Image Data
```{r}
# Merge finalized ratings data with other processed and finalized data per image
n1b_byimage_data <-  n1b_ratings_vars_pca %>% full_join(n1b_percepts_personae) %>% full_join(n1b_percepts_general)
n1b_byimage_data
```
```{r}
# For Reference: Column names
colnames(n1b_byimage_data)
```


## Assess

### Check Outliers: Percepts
```{r}
get_outliers <- function(df, col) {
  col <- enquo(col)
  df %>% select(1:3, !!col) %>% mutate(mean=mean(!!col), sd=sd(!!col)) %>% mutate(lower = mean-(3*sd), upper=mean+(3*sd)) %>%
    mutate(outlier=case_when(
      !!col < lower | !!col > upper ~ TRUE,
      TRUE ~ FALSE
    ))
}
```

Based on coded text responses.
```{r}
# Ethnicity
n1b_byimage_data %>% get_outliers(asian_prop) %>% filter(outlier==TRUE)
n1b_byimage_data %>% get_outliers(white_prop) %>% filter(outlier==TRUE)
# by Condition
# Check outliers beyond 3SD
n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(asian_prop) %>% filter(outlier==TRUE)
# NOTE: None
n1b_byimage_data %>% filter(Condition=="msWhite") %>% get_outliers(white_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least Asian Asian
n1b_byimage_data %>% select(1:3, asian_prop:white_prop) %>% filter(Condition!="msWhite") %>% slice_min(asian_prop, n=4)
# NOTE: Seems like least "Asian" are the Southeast Asian/Pacific Islander interpreted faces

# Check least White White
n1b_byimage_data %>% select(1:3, asian_prop:white_prop) %>% filter(Condition=="msWhite") %>% slice_min(white_prop, n=4)
# NOTE: Possibly some responses were instead "American", which was coded differently, so not super informative
```


```{r}
# GrewUp
# Check outliers beyond 3SD
n1b_byimage_data %>% get_outliers(from_asia_prop) %>% filter(outlier==TRUE)
n1b_byimage_data %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# by Condition
n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(from_asia_prop) %>% filter(outlier==TRUE)
n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# NOTE: None
n1b_byimage_data %>% filter(Condition=="msWhite") %>% get_outliers(from_us_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least Asia Asian
n1b_byimage_data %>% select(1:3, from_asia_prop:from_us_prop) %>% filter(Condition!="msWhite") %>% slice_min(from_asia_prop, n=4)
# Check least US Asian
n1b_byimage_data %>% select(1:3, from_asia_prop:from_us_prop) %>% filter(Condition!="msWhite") %>% slice_min(from_us_prop, n=4)
# NOTE: 

# Check most US White
n1b_byimage_data %>% select(1:3, from_asia_prop:from_us_prop) %>% filter(Condition=="msWhite") %>% slice_max(from_us_prop, n=4)
# Check least US White
n1b_byimage_data %>% select(1:3, from_asia_prop:from_us_prop) %>% filter(Condition=="msWhite") %>% slice_min(from_us_prop, n=4)
# NOTE: 
```

```{r}
# Speech
# Check outliers beyond 3SD
n1b_byimage_data %>% get_outliers(accented_prop) %>% filter(outlier==TRUE)
n1b_byimage_data %>% get_outliers(unaccented_prop) %>% filter(outlier==TRUE)
# by Condition
n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(accented_prop) %>% filter(outlier==TRUE)
# NOTE: None
n1b_byimage_data %>% filter(Condition=="msWhite") %>% get_outliers(unaccented_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least accented/unaccented Asian
n1b_byimage_data %>% select(1:3, accented_prop:unaccented_prop) %>% filter(Condition!="msWhite") %>% slice_min(accented_prop, n=4)
n1b_byimage_data %>% select(1:3, accented_prop:unaccented_prop) %>% filter(Condition!="msWhite") %>% slice_min(unaccented_prop, n=4)
# NOTE: 

# Check most unaccented White
n1b_byimage_data %>% select(1:3, accented_prop:unaccented_prop) %>% filter(Condition=="msWhite") %>% 
  slice_max(unaccented_prop, n=4) # filter(unaccented_prop > 0.2) 
# Check least unaccented White
n1b_byimage_data %>% select(1:3, accented_prop:unaccented_prop) %>% filter(Condition=="msWhite") %>% 
  slice_min(unaccented_prop, n=4) # filter(unaccented_prop < 0.2)
# NOTE: Confusing to interpret because unaccented also include descriptors like "California style" and "standard"
```


```{r}
# Descriptors
# Check outliers beyond 3SD
n1b_byimage_data %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# by Condition
n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# NOTE: None
n1b_byimage_data %>% filter(Condition=="msWhite") %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# NOTE: None

# Check least positive photos
# n1b_byimage_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(sentiment_score, n=4)
n1b_byimage_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(positive_prop, n=4)
```



```{r}
# Ethnicity x Grewup x Speech
n1b_byimage_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, size=accented_prop, color=Condition)) + geom_point(alpha=0.7) + gg_theme()
n1b_byimage_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, color=Condition, label=Image)) + geom_text(alpha=0.7) + gg_theme() +
  scale_x_continuous(limits=c(-0.1, 1.1), breaks = c(0,0.25, 0.5, 0.75, 1))

# Descriptor Sentiment
n1b_byimage_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Condition)) + geom_point() + gg_theme()
n1b_byimage_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Condition, label=Image)) + geom_text() + gg_theme() 
```

### Find Outliers: Ratings
Check for Outliers with Mahalanobis distances for numerical ratings
```{r}
# # Check outliers beyond 3SD
# n1b_byimage_data %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# # by Condition
# n1b_byimage_data %>% filter(Condition!="msWhite") %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# # NOTE: None
# n1b_byimage_data %>% filter(Condition=="msWhite") %>% get_outliers(positive_prop) %>% filter(outlier==TRUE)
# # NOTE: None
# 
# # Check least positive photos
# # n1b_byimage_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(sentiment_score, n=4)
# n1b_byimage_data %>% select(1:3, sentiment_score:positive_prop) %>% slice_min(positive_prop, n=4)
```


* Use the Mahalanobis distance to assess outliers/matches via visual inspection and selecting a threshold cut-off

```{r}
# Get mahalnobis distances Function
get_mdist <- function(scores_df, index_df, cutoff=10){
  # Finding the center point 
center  = colMeans(scores_df)
# Finding the covariance matrix
cov     = cov(scores_df)
# Calculate Mahalnobis distance + identify outliers
mdist_df <- index_df %>% 
  cbind(m_dist = mahalanobis(scores_df, center, cov))  %>%
  mutate(outlier = ifelse(m_dist > cutoff, TRUE, FALSE))
  # mutate(pvalue = pchisq(m_dist, df=3, lower.tail=FALSE)) #pval<0.001 outlier
return(mdist_df)
}
```

```{r}
# All rating variables
scores_df <- n1b_ratings_vars_pca %>% select(-Condition:-Image) %>% 
  select(American:Intelligent) 
index_df <- n1b_ratings_vars_pca %>% select(Condition:Image)

# Get mahalanobis distances
mdist_df <- get_mdist(scores_df, index_df)

# Most different considering all variables
# See results
mdist_df %>% arrange(desc(m_dist))
# Get list of potential outliers to check
mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat)
```


```{r}
# All orthogonal/unique variables
scores_df <- n1b_ratings_vars_pca %>% select(-Condition:-Image) %>% 
  select(Dim1, Dim2, Dim3) #, Dim4) #, Feminine, Casual, Intelligent, Nerdy, `Clear speaker`)
index_df <- n1b_ratings_vars_pca %>% select(Condition:Image)

# Get mahalnobis distances
mdist_df <- get_mdist(scores_df, index_df)

# Most different considering all variables
# See results
mdist_df %>% arrange(desc(m_dist))
# Get list of potential outliers to check
mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat)
```

```{r}
# All orthogonal/unique variables, excluding American
scores_df <- n1b_ratings_vars_pca %>% select(-Condition:-Image) %>% 
  select(Dim2, Dim3) #, Dim4) #, Feminine, Casual, Intelligent, Nerdy, `Clear speaker`)
index_df <- n1b_ratings_vars_pca %>% select(Condition:Image)

# Get mahalnobis distances
mdist_df <- get_mdist(scores_df, index_df)

# Most different (across conditions) excluding American
# See results
mdist_df %>% arrange(desc(m_dist))
# Get list of potential outliers to check
mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat)
```
```{r}
# Most similar (across conditions) excluding American
# See results
mdist_df %>% arrange((m_dist))
# Get list of potential matches to check
mdist_ranks <- mdist_df %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat) %>% 
  mutate(m_dist_bins = case_when(
    m_dist < 1 ~ "0-1", # 0-1
    m_dist < 2 ~ "1-2", # 1-2
    m_dist < 4 ~ "2-4", # 2-4
    m_dist < 6 ~ "4-6", # 4-6
    m_dist >= 6 ~ "6+", # 6+
    TRUE ~ NA
  ), .after=m_dist) %>%
  arrange((m_dist)) %>% group_by(Condition) %>%
  mutate(ingroup_ranks = order(order(m_dist, decreasing=FALSE)), .after=m_dist_bins) #%>% arrange(ingroup_ranks)
mdist_ranks

# Option 1: Pivot to matched rank columns, image value for condition based on rank
mdist_ranks %>%  pivot_wider(id_cols = ingroup_ranks, names_from = Condition, values_from = c(Image, m_dist))

# Option 2: Bin columns, image value for condition based on rank
mdist_ranks %>% count(Condition, m_dist_bins) %>% pivot_wider(names_from = Condition, values_from = n)
mdist_ranks %>% pivot_wider(id_cols = c(m_dist_bins, Image), names_from = Condition, values_from = c(m_dist))
```

### Get Narrowed Conditions

Select 4-5 photos 
* Based on Dim1 scores difference
* Based on outliers (mahalanobis scores)


TODO:
- Pull in screener categories; filter out
- How to get most similar rows across categories? Mahalanobis but smallest values
  - TODO: Try with fewer unique cols, just the most relevant/orthogonal seeming


```{r}
# # WIP simplified slice_mean function
# slice_mean <- function(df, col, n, mean_group=NULL, filter_group=NULL, filter_string=NULL){
#   col <- enquo(col)
#   if (!is.null(mean_group)){    mean_group <- enquo(mean_group)  }
#   if (!is.null(filter_group)){    filter_group <- enquo(filter_group)  }
#   if (is.null(mean_group)){
#     df <- df %>% mutate(mean=round(mean(!!col),7), .before=!!col) 
#   } else {
#     df <- df %>% group_by(!!mean_group) %>% mutate(mean=round(mean(!!col),7), .before=!!col) %>% ungroup()
#   }
#   if (!is.null(filter_group)){
#     df <- df %>% filter(!!filter_group==filter_string)
#   }
#   df %>% mutate(dist_mean = abs(mean-!!col), .before=!!col) %>%
#   slice_min(dist_mean, n=n)
# }
# 
# n1b_byimage_data %>% slice_mean(Dim1, n=5)
# n1b_byimage_data %>% slice_mean(Dim1, n=5, mean_group=Condition)
# n1b_byimage_data %>% slice_mean(Dim1, n=5, filter_group=Condition, filter_string="msAsian")
# n1b_byimage_data %>% slice_mean(Dim1, n=5, mean_group=Condition, filter_group=Condition, filter_string="msAsian")

```

```{r}
# rightmost = msWhite
n1b_msWhite <- n1b_byimage_data %>% filter(Condition=="msWhite") %>% slice_max(Dim1, n=10)

# leftmost = ethAsian
n1b_ethAsian <- n1b_byimage_data %>% filter(Condition=="ethAsian") %>% slice_min(Dim1, n=5)

# Options for msAsian
# leftmost within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% filter(Condition=="msAsian") %>% slice_min(Dim1, n=5)
# rightmost within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% filter(Condition=="msAsian") %>% slice_max(Dim1, n=5)
# closest to group mean  within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% 
  group_by(Condition) %>% mutate(mean=round(mean(Dim1),7)) %>% ungroup() %>% mutate(dist_mean = abs(mean-Dim1)) %>%
  filter(Condition=="msAsian") %>%
  slice_min(dist_mean, n=5) %>% select(-mean:-dist_mean)
# closest to grand mean  within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% 
  mutate(mean=round(mean(Dim1),7)) %>% mutate(dist_mean = abs(mean-Dim1)) %>%
  filter(Condition=="msAsian") %>%
  slice_min(dist_mean, n=5) %>% select(-mean:-dist_mean)

n1b_selected_images <-  n1b_msWhite %>% rbind(n1b_ethAsian) %>% rbind(n1b_msAsian)
n1b_selected_images
```
 
```{r}
# Condition Mean Parallel Plots by Rating_Scale
n1b_selected_images %>% pivot_longer(American:Intelligent, names_to = "Rating_Scale", values_to = "Rating_ZScore") %>%
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale,
  "American", "Native speaker of English", "American-accented", "Aligned with American culture",  # Culture
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent",               # Traits
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker"))    %>%                      # Style
  parallel_plot(x=Rating_Scale, y=Rating_ZScore, group=Condition, angle_axis_labels=TRUE) 

# Dimensions
n1b_selected_images %>% pivot_longer(Dim1:Dim4, names_to = "Dimension", values_to = "Score") %>%
  parallel_plot(x=Dimension, y=Score, group=Condition, angle_axis_labels=FALSE) 
```
 
 
### Examine Distinctiveness / Test Differences
Function to get linear model output and pairwise comparisons
```{r}
# LM Output Comparisons Function
# **Reference Code:** 
# - https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html 
# - https://broom.tidymodels.org/reference/tidy.summary_emm.html 

lm_emms_pairs <- function(df, formula){
  # run linear model
  lm_out <- lm(formula, data=df)
  print( summary(lm_out) )
  print( lm_df <- tidy(lm_out) )
  
  # get marginal averages (estimated marginal means / Least-squares means)
  emms <- emmeans(lm_out, "Condition")
  print( emms_df <- tidy(emms, conf.int = TRUE) )
  
  # get contrasts
  # (?) tidy(contrast(emms)) # contrast from grand mean(?)
  # pwcs = contrast between paired groups; equivalent to: contrast(emms, method="pairwise") 
  #   +  bonferroni adjusted for multiple comparisons
  pwcs <- tidy(pairs(emms)) %>% mutate(bfrn.adj.p.value = tidy(pairs(emms, adjust="bonferroni")) %>% pull(adj.p.value))
  print(pwcs)
  
  # get effect sizes
  print( tidy(eff_size(emms, sigma = sigma(lm_out), edf = Inf)) )
  
  # plot confidence intervals
  #Option 1
  print( plot(emms, comparisons = TRUE))
  #Option 2
  confint_plot <- ggplot(emms_df, aes(Condition, estimate)) +
    geom_point() + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=0.5) + gg_theme()
  print( confint_plot )
}

# Output:
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
# (*) not currently sure about the `edf` parameter, currently speifying Inf, narrowing confint unrealistically
```

#### Dim1
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim1 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim1 ~ Condition)
```
#### Dim2
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim2 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim2 ~ Condition)
```
#### Dim3
Good, Dim3 are NOT different across conditions
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim3 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim3 ~ Condition)
```
#### Dim4
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim4 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim4 ~ Condition)
```

#### Feminine
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Feminine ~ Condition)
lm_emms_pairs(n1b_selected_images, Feminine ~ Condition)
```
#### Feminine
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Intelligent ~ Condition)
lm_emms_pairs(n1b_selected_images, Intelligent ~ Condition)
```

#### Nerdy
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Nerdy ~ Condition)
lm_emms_pairs(n1b_selected_images, Nerdy ~ Condition)
```
#### Casual
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Casual ~ Condition)
lm_emms_pairs(n1b_selected_images, Casual ~ Condition)
```
#### Clear speaker
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, `Clear speaker` ~ Condition)
lm_emms_pairs(n1b_selected_images, `Clear speaker` ~ Condition)
```