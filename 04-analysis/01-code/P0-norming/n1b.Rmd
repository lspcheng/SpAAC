---
title: "n1b"
output: html_document
---

# Preamble
## Packages
```{r setup}
# Load libraries and custom functions
if (file.exists("project_functions.R")){
  source("project_functions.R")
  
} else { # try one directory up
  source("../project_functions.R")
}

# Text Analysis
library(countrycode)
library(maps)
library(tidytext)

# Plots
library(hrbrthemes)

# Functions
quick_summarize <- function(df, col, na.rm=FALSE){
  col = enquo(col)
  df %>%  summarize(across(!!col, list(median= ~ median(.x, na.rm=na.rm), mean= ~ mean(.x, na.rm=na.rm),
                                       sd= ~ sd(.x, na.rm=na.rm), min= ~ min(.x, na.rm=na.rm), 
                                       max= ~ max(.x, na.rm=na.rm))))
}

```

## Pipeline Structure
```{r}
# Fill in file structure info (e.g. using getwd())
NAME <- 'n1b' ## Name of the R file (w/o file extension!)
PHASE <- 'P0-norming' ## Name of the project phase (if relevant)
PROJECT <- 'SpAAC' ## Name of project
```

```{r}
# Get project directory path & subfolder status from working dir
PROJECT_DIR <- str_extract(getwd(), paste0("^(.*?)",PROJECT,"/"))

if (basename(getwd()) != PHASE) {SUBFOLDER <- basename(getwd())} else {SUBFOLDER <- NA}

# Get pipeline path names
if (dir.exists(file.path(PROJECT_DIR, '04-analysis', '02-pipeline'))){
  if (is.na(SUBFOLDER)){
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, NAME)
  } else {
    pipeline <- file.path(PROJECT_DIR, '04-analysis', '02-pipeline', PHASE, SUBFOLDER, NAME)
  }
} else {
  pipeline <- file.path('.', 'pipeline', PHASE, NAME)
}

# Create pipeline folders
if (!dir.exists(pipeline)) {
  dir.create(pipeline, recursive=TRUE)
  for (folder in c('out', 'store', 'temp')){
    dir.create(file.path(pipeline, folder))
  }
}
```

```{r}
# Basic reference paths
stim_data_path <- file.path(PROJECT_DIR, '02-materials', '02-stimuli', PHASE) 
ext_data_path <- file.path(PROJECT_DIR, '03-data', '01-external', PHASE) 
int_data_path <- file.path(PROJECT_DIR, '03-data', '02-internal', PHASE) 
manual_analysis_path <- file.path(PROJECT_DIR, '04-analysis', '03-manual', PHASE) # 001-code / 003-manual
```


# .
# Set-up

## Review Comments

On Qualtrics "Results" Tab (i.e., not the Data & Analysis tab), view the comments about the task to get a sense of what participants thought and if there are things to change for the next batch.

Can also do a quick review of the demographics.

## Export Files

**(1) Prolific**
On the Prolific study page, click on "Download Demographic data".

**(2) Qualtrics: Screener**
From Qualtrics Data & Analysis page, export the screener data file.

Select these basic options:
* Download all fields
* Use choice text

Select these advanced options:
* Split multi-value fields into columns

NOTE: Don't include the -99 because I'll be able to easily exclude the NAs instead.

**(3) Qualtrics: Main Survey**
From Qualtrics Data & Analysis page, export the data file.

Select these basic options:
* Download all fields
* Use numeric values (first) AND choice text (second)

Select these advanced options:
* Recode seen but unanswered multi-value fields as 0
* Recode seen but unanswered questions as -99
* Split multi-value fields into columns
* Export viewing order data for randomized surveys

NOTE: Must include the -99 because it will result in -99 for pages where nothing was selected, which I can then convert to 0 (else, it will be NA, which I couldn't separate from unseen NAs, i.e., unpresented blocks).

### (*C) Set Filenames
Pilot set (n=8) data files.
Main set (n=...) data files
Final set (n=10 + 3) data files

```{r}
# Set File paths
RUN <- "1-1_pilot"

pilot_prolific <- list.files(file.path(int_data_path, NAME, RUN),  pattern="prolific_export", full.names = TRUE)
pilot_screener <- list.files(file.path(int_data_path, NAME, RUN),  pattern="+Screener", full.names = TRUE)
pilot_data_num <- list.files(file.path(int_data_path, NAME, RUN),  pattern="_num", full.names = TRUE) 
pilot_data_text <- list.files(file.path(int_data_path, NAME, RUN),  pattern="_text", full.names = TRUE) 

RUN <- "1-2_main"
main_prolific <- list.files(file.path(int_data_path, NAME, RUN),  pattern="prolific_export", full.names = TRUE)
# main_prolific_1 <- file.path(int_data_path, NAME, RUN, "prolific_export_647674eff562724a9f3b8394.csv") # n1b — 2
# main_prolific_2 <- file.path(int_data_path, NAME, RUN, "prolific_export_647674ebfd3b2b5494abcf01.csv") # n1b — 3
# main_prolific_3 <- file.path(int_data_path, NAME, RUN, "prolific_export_6478d1379cc5fe472f276074.csv") # n1b — 4
# main_prolific_4 <- file.path(int_data_path, NAME, RUN, "prolific_export_647de66d14bd4b43fe5c7f15.csv") # n1b — 4
main_screener <- list.files(file.path(int_data_path, NAME, RUN),  pattern="+Screener", full.names = TRUE) 
main_data_num <-  list.files(file.path(int_data_path, NAME, RUN),  pattern="_num", full.names = TRUE)
main_data_text <- list.files(file.path(int_data_path, NAME, RUN),  pattern="_text", full.names = TRUE)

# # File paths for: RUN <- "1-3_final"
# final_prolific <- list.files(file.path(int_data_path, NAME, RUN),  pattern="prolific_export", full.names = TRUE)
# final_screener <- list.files(file.path(int_data_path, NAME, RUN),  pattern="+Screener", full.names = TRUE) 
# final_data_num <-  list.files(file.path(int_data_path, NAME, RUN),  pattern="_num", full.names = TRUE)
# final_data_text <- list.files(file.path(int_data_path, NAME, RUN),  pattern="_text", full.names = TRUE)

# Get Current File paths
RUN_LIST <- c("pilot", "main", "final") # "test"
CURRENT_RUN <-  "main" # RUN_LIST[2] # To get last value: RUN_LIST[length(RUN_LIST)]

if (CURRENT_RUN == "pilot"){
  current_prolific <- pilot_prolific
  current_screener <- pilot_screener
  current_data_num <- pilot_data_num
  current_data_text <- pilot_data_text
} else if (CURRENT_RUN == "main" ) {
  current_prolific <- main_prolific
  current_screener <- main_screener
  current_data_num <- main_data_num
  current_data_text <- main_data_text  
} else if (CURRENT_RUN == "final") {
  current_prolific <- final_prolific
  current_screener <- final_screener
  current_data_num <- final_data_num
  current_data_text <- final_data_text  
}

print(CURRENT_RUN)
print(current_prolific)
print(current_screener)
print(current_data_num)
print(current_data_text)
```

## (*T) Check Test Data 
Using simulated (pre-pilot, test) data, check data labels and prep code for cleaning data (columns, etc). During this iterative process, go back to Qualtrics survey and update codes and labels where necessary to ensure clean output data.
Pre-pilot set data files.
```{r}
# RUN <- "0-1_initialtest"
# 
# test_data_num <- file.path(int_data_path, NAME, RUN, "Visual+Style+Impressions_May+29,+2023_09.45_num.csv")
# test_data_text <- file.path(int_data_path, NAME, RUN, "Visual+Style+Impressions_May+29,+2023_09.45_text.csv")
```

Some of this code adapted for wrangling participant data.
```{r}
# # NOTE: Unfinished data doesn't appear in the export file until X days later. That's why the data may seem to change if I export new data later.
# 
# # Use numeric data for main task
# n1b_data <- read_csv(test_data_num)  %>% 
#   filter(Finished!=0) %>%
#   select(-StartDate:-UserLanguage) %>% # Remove metadata columns (first several). See raw data for columns Progress, Duration, Finished, RecordedDate
#   slice(-2)  # Remove unnecessary question header rows (1,2)
# n1b_data
# 
# # Use text data for subject data
# n1b_data_text <- read_csv(test_data_text)  %>% 
#   filter(Finished!="False") %>%
#   select(-StartDate:-UserLanguage) %>% # Remove metadata columns (first several)
#   slice(-2) # Remove unnecessary question header rows (1,2)
# n1b_data_text
```

```{r}
# # Get randomized images order list per subject
# # Check for even distribution
# n1b_data %>%
#   select(contains("Q2.5")) %>% select(contains("DO")) %>% slice(-1) %>%
#   mutate(subj=1:n(), .before=1) %>% # Add temp subj number for simulated data
#   pivot_longer(cols=2:last_col(), names_to=c(NA, NA, "Photo_Number"), names_sep="_", values_to = "DO") %>% type.convert() %>%
#   arrange(subj, DO) %>% drop_na(DO) %>%
#   pivot_wider(subj, names_from = DO, names_prefix = "Trial", values_from = Photo_Number)
#   
```

```{r}
# # Get informative question labels
# q_labels <- 
#   n1b_data %>%
#   select(`1_Q3.1_1`:`30_Q3.12`) %>% select(-contains("DO")) %>% # select only looped trials, excl, DisplayOrder
#      slice(1) %>%
#   pivot_longer(cols=everything(), names_to=c("Photo_Number", "Question_Number", "Question_Part_Number", NA), names_sep="_", values_to = "q_label") %>%
#   # Since number of dashes and <> location aren't consistent, need to remove everything between < > and extra -
#   mutate(q_label = gsub("<.*>", "", q_label)) %>%
#   mutate(q_label = gsub("^(\\s*-\\s*)", "", q_label)) %>%
# mutate(q_label = gsub("(-\\s+.*\\s+-)", "-", q_label)) %>%
#   separate(q_label, into= c("Question", "Question_Part"), sep = "-", extra="merge")
# q_labels 
```

```{r}
# # Get question data + merge with informative question labels
# test_data <- n1b_data %>%
#   select(`1_Q3.1_1`:`30_Q3.12`) %>% select(-contains("DO")) %>% # select only looped trials, excl, DisplayOrder
#   slice(-1) %>% # remove second header
#   mutate(subj=1:n(), .before=1) %>% # Add temp subj number for simulated data
#   pivot_longer(cols=2:last_col(), names_to=c("Photo_Number", "Question_Number", "Question_Part_Number", NA), names_sep="_", values_to = "responses") %>%
#   full_join(q_labels, .)
# test_data
```
```{r}
# # Check question labels for all questions (even those not shown to participants, are NA)
# test_data %>%  group_by(subj) %>%  count() #870
# test_data %>%  group_by(subj, Photo_Number) %>%  count() # 29
# test_data %>%  group_by(subj, Photo_Number, Question) %>%  count() 
# test_data %>%  group_by(subj, Photo_Number, Question, Question_Part) %>%  count()
```
```{r}
# # Get only data shown to participants (i.e., drop NA)
# test_data_clean <- test_data %>%
#   drop_na(responses)
# 
# # Check n of total questions by subj and photo
# # For rough check (not very meaningful)
# test_data_clean %>% group_by(subj) %>%  count()
# test_data_clean %>%  group_by(subj, Photo_Number) %>% count() %>% type_convert() %>% arrange(subj, Photo_Number) # 29
# test_data_clean %>% group_by(Photo_Number)%>% count() %>% type_convert() %>% arrange(Photo_Number)
# 
# # Check n of trials by subj and photo 
# # For confirming even presentation (randomization) of photos
# # Total presented/answered photos (end goal: 600)
# test_data_clean %>% select(subj, Photo_Number) %>% distinct() %>% count()
# # Total presented/answered photos per participant (process/end goal: 10 per subject)
# test_data_clean %>% select(subj, Photo_Number) %>% distinct() %>% count(subj)
# # Total presented/answered participants per photo (end goal: 20 per photo)
# test_data_clean %>% select(subj, Photo_Number) %>% distinct() %>% count(Photo_Number) %>% type_convert() %>% arrange(Photo_Number)

```




# Pre-Process Screener Data

## (*C) Read, Wrangle

```{r}
# Read in Prolific "Demographic data" from the study page
n1b_prolific_demo <- 
  plyr::ldply(pilot_prolific, read_csv) %>% mutate(Run=RUN_LIST[1]) %>%
  full_join(plyr::ldply(main_prolific, read_csv) %>% mutate(Run=RUN_LIST[2]))%>%
  # full_join(read_csv(final_prolific) %>% mutate(Run=RUN_LIST[3])) %>%
  mutate(Time_in_min=`Time taken`/60, .after=`Time taken`)  
n1b_prolific_demo
```

```{r}
n1b_screener_data <- read_csv(current_screener) %>%
# Remove metadata columns (first several)
  select(-StartDate:-UserLanguage) %>%
  # Remove uneccessary question header rows (1,2)
  slice(-1:-2) %>%
  unite("Ethnicity", Ethnicity_9:Ethnicity_14, sep=",", na.rm=TRUE, remove=FALSE) %>%
relocate(Device, .after = Ethnicity) %>%
  rename(`Participant id` = Prolific_ID)

  #TEMP: Remove returned/non-consent participants, bad participants, outliers while checking, before deleting
# n1b_screener_data <- n1b_screener_data %>% filter(!(PROLIFIC_PID%in% c("62a15df1dd6fa394ea04f063"))) <- REPLACE if needed
  
n1b_screener_data 
# View(screener_data)
```

```{r}
# Compare screener to prolific demographic
n1b_screener_v_prolific <-
  n1b_screener_data %>% full_join(n1b_prolific_demo, by="Participant id")  %>% 
  select(
    Run,
    `Participant id`,
    Time_in_min,
    State_Born, `U.s state/territory of birth`,
    State_Current, `Current u.s state of residence`,
    Ethnicity_screen=Ethnicity.x, Ethnicity_prolific=Ethnicity.y, `Ethnicity simplified` 
    #, .after=`Participant id`
  ) 
# Show full data
n1b_screener_v_prolific

# Check and Remove Run==NA rows, where no Prolific data (i.e., generally, returned participants)
n1b_screener_v_prolific_NA <- n1b_screener_v_prolific %>% filter(is.na(Run))
n1b_screener_v_prolific_NA

n1b_screener_v_prolific <- n1b_screener_v_prolific %>%  drop_na(Run)
n1b_screener_v_prolific
```


## Check, Summarize
Check the prescreener data for consistency with the main data.
### (*C) Sample Check
#### Duplicates
```{r}
# Check for duplicated participant id issues
n1b_screener_v_prolific %>% count(`Participant id`) %>% arrange(desc(n))
```

#### Time Taken
```{r}
#Time_in_min
n1b_screener_v_prolific %>% 
  # filter(Run == CURRENT_RUN) %>%
  summarize(mean_Time=mean(Time_in_min, na.rm=TRUE), median_Time=median(Time_in_min, na.rm=TRUE),
            min_Time=min(Time_in_min, na.rm=TRUE), max_Time=max(Time_in_min, na.rm=TRUE),
            sd_Time=sd(Time_in_min, na.rm=TRUE)) %>%
  mutate(low_cutoff=min_Time-sd_Time*3, high_cutoff=max_Time+sd_Time*3)
```
#### Region
```{r}
# Sample Counts: Region
n1b_screener_data %>%
  count(State_Current,State_Born)
```
#### Ethnicity
```{r}
# Sample Counts: Ethnicity
n1b_screener_data %>%
  count(Ethnicity)
```

#### Device
```{r}
# Sample Counts: Device
n1b_screener_data %>%
  count(Device)
```

####  (*C) By-Participant Check
```{r}
participant_list <- n1b_screener_v_prolific %>% filter(Run==CURRENT_RUN) %>%  pull(`Participant id`)
participant_list
```

```{r}
current_participant <- "64136da866ff27f7bf98cfe0" #participant_list[1] #"5f7d4a2c5db79d21c7d07240"

n1b_screener_data %>%  filter(PROLIFIC_PID == current_participant) #Prolific_ID

n1b_screener_v_prolific %>%  filter(`Participant id` == current_participant) #Prolific_ID
```



# Pre-Process Main Data
## Read
Read in exported data, and check what it looks like.

NOTE: Remove data from subjects who did not complete the study ("returned" on Prolific) or have been identified to be outliers, not following instructions, not fulfilling my participant requirements, etc. Outliers are identified in a later section below (Outlier Check), while participant requirements are checked in the data from the questionnaire.

```{r}
# Use numeric data for main task
  # NOTE: Unfinished data doesn't appear in the export file until X days later. That's why the data may seem to change if I export new data later.

n1b_data <- read_csv(current_data_num)  %>% 
  filter(Finished!=0) %>%
  # Remove metadata columns (first several). See raw data for columns Progress, Duration, Finished, RecordedDate + question header rows (1,2) + function cols
  select(-StartDate:-UserLanguage) %>% 
  slice(-2) %>% select(-Q1.2,-starts_with("Ex")) 
  # Remove returned/non-consent participants, bad participants, outliers.
n1b_data %>% colnames()


# Use text data for subject data
n1b_data_text <- read_csv(current_data_text)  %>% 
  # NOTE: Unfinished data doesn't appear in the export file until X days later. That's why the data may seem to change if I export new data later.
  filter(Finished!="False") %>%
  # Remove metadata columns (first several) + question header rows (1,2)
  select(-StartDate:-UserLanguage) %>% 
  slice(-2)  %>% select(-Q1.2,-starts_with("Ex")) 
n1b_data_text
```

```{r}
n1b_data %>% pull(PROLIFIC_PID)
```
Next, read in image/filename decoding information that was originally constructed in Google sheets (SpAAC: Visual Stimuli) and downloaded as CSV.
```{r}
# Read in image decoding file
n1b_codes <- read.csv(file.path(stim_data_path, NAME, "02-records","SpAAC Visual Stimuli - N1b_Stims.csv")) %>%
  mutate(Photo_Number = seq(1:n()), .before="Critical.Filler")
n1b_codes
```

## Wrangle


### Subject Data
Before moving on, check the Task short-answer reflections and Demographic data + Randomization data.
```{r}
# Extract relevant Info from 
n1b_subj <- n1b_data_text %>%
  # Get Part 1 end and Demographics
  select('Q4.3':'VERSION') %>% 
  janitor::row_to_names(1, remove_rows_above = FALSE) %>% # Uses first row as colnames
  type_convert()
n1b_subj
```

#### Conditions
Process randomization/condition information. Check that randomization is working (e.g., displaying randomly, evenly). Includes version and photo numbers by participant and trial.
```{r}
n1b_subj_cond <- n1b_subj %>% 
  select(PROLIFIC_PID, VERSION) %>%
  full_join(
    # Get randomized images order list per subject
    n1b_data %>%
    select(PROLIFIC_PID, contains("Q2.5_DO")) %>% slice(-1) %>%
    pivot_longer(cols=2:last_col(), names_to=c(NA, NA, "Photo_Number"),
                 names_sep="_", values_to = "DO") %>% type.convert() %>%
      arrange(PROLIFIC_PID, DO) %>% drop_na(DO) %>%
      pivot_wider(PROLIFIC_PID, names_from = DO, names_prefix = "Trial",
                  values_from = Photo_Number)
  )
n1b_subj_cond 

## UNCOMMENT to check a participant
# n1b_subj_cond %>%  filter(PROLIFIC_PID=="596fc4717008ef000109703d")
```


Process Part 1 End responses about the task.
```{r}
# View
(n1b_subj_p1 <- n1b_subj %>% select(PROLIFIC_PID, WhatTaskAbout:TaskNotice))
```
Process Part 2 responses about the personal demographics.
```{r}
n1b_subj_p2 <- n1b_subj %>% select(PROLIFIC_PID, Age:VERSION) %>%
  # Convert -99 to NAs
  mutate(across(where(is.numeric), ~na_if(., -99))) %>%
  mutate(across(where(is.character), ~na_if(., "-99"))) %>%
  rename_with(., ~ gsub(" - Selected Choice", "", .x, fixed=TRUE)) %>%
  # Clean text responses
  mutate(Gender=tolower(Gender), Ethnicity=tolower(Ethnicity)) %>%
  #mutate(Gender=gsub(" ", "", Gender), Ethnicity=gsub("-", " ", Ethnicity)) %>%
  mutate(Gender=mgsub(Gender, c("^female|^woman", "^male|^man"), c("f", "m"), fixed=FALSE)) %>%
  mutate(Ethnicity=mgsub(Ethnicity, c("han ", "(mandarin)", "and honk-kongese"), c("", "", "; hongkongese"), fixed=TRUE)) %>%
  mutate(Ethnicity=mgsub(Ethnicity, c("(\\s|\\-)american", "/"), c("", ", "), fixed=FALSE)) # remove american
n1b_subj_p2
```

#### Summary
#### (*R) Sample Check
```{r}
# Total Participant entries
n1b_subj_p2 %>% count()

# Total Participants per condition
n1b_subj_p2 %>% add_count(name="total_n") %>% count(VERSION, total_n, name="group_n") %>% 
  mutate(prop = round(group_n/total_n,2))
```
##### Age
```{r}
# Age
n1b_subj_p2 %>% quick_summarize(Age)
```
```{r}
# Visualize Age
n1b_subj_p2 %>%
  ggplot() +  gg_theme() +
  geom_histogram(aes(x=Age), binwidth=1, alpha=0.7)
```


##### Gender
```{r}
# Gender overall
n1b_subj_p2 %>% add_count(name="total_n") %>% count(Gender, GenderCat, total_n, name="group_n") %>% 
  mutate(prop = round(group_n/total_n,2))
```

```{r}
# Gender by Age, colored
n1b_subj_p2 %>%
  ggplot() +  gg_theme() +
  geom_histogram(aes(x=Age, fill=GenderCat), binwidth=1, alpha=0.7)

# Visualize Gender
n1b_subj_p2 %>%
  ggplot() +
  gg_theme() +
  geom_bar(aes(x=GenderCat, fill=GenderCat), alpha=0.7) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

##### Ethnicity
```{r}
# EthnicityCat
n1b_subj_p2 %>%
  count(EthnicityCat) %>%
  arrange(-n)

# Specific Ethnicity
n1b_subj_p2 %>%
  count(Ethnicity, EthnicityCat) %>%
  arrange(-n)

# Other: Ethnicity by Gender 
n1b_subj_p2 %>%
  count(Ethnicity, GenderCat) %>%
  arrange(-n) %>%
  pivot_wider(., Ethnicity, names_from = "GenderCat", values_from = "n") %>%
  mutate(across(where(is.integer), ~ coalesce(.x, 0L))) %>%
  group_by(Ethnicity) %>% mutate(total=sum(Woman, Man), .after=Ethnicity)
```
```{r}
# Visualize Ethnicity
# By Gender
n1b_subj_p2 %>%
  group_by(Ethnicity) %>% mutate(Ethnicity_n=n()) %>% ungroup() %>%
  ggplot() +  gg_theme() +
  geom_bar(aes(x=reorder(Ethnicity, -Ethnicity_n), fill=GenderCat, linetype=EthnicityCat), alpha=0.7) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(x="Ethnicity")

# With Ethnicity Cat
n1b_subj_p2 %>%
  group_by(Ethnicity) %>% mutate(Ethnicity_n=n()) %>% ungroup() %>%
  ggplot() +  gg_theme() +
  geom_bar(aes(x=reorder(Ethnicity, -Ethnicity_n), fill=EthnicityCat), alpha=0.7) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(x="Ethnicity") %>% scale_fill_viridis(discrete=TRUE)
```


##### Location
```{r}
# # Check location info for summary statement
n1b_subj_p2_loc <-
  n1b_subj_p2 %>%
  select(PROLIFIC_PID, starts_with("Location")) %>%
  mutate(across(everything(), as.character)) %>%
  rename_with(., ~ gsub(".*(\\d)", "\\1", .x)) %>% # Remove everything before the first digit, but keep the digit (via the parenthesis and \\1)
  pivot_longer(cols = 2:last_col(), names_to = c("Loc", "Info"), names_sep=" - ", values_to = "Response") %>%
  pivot_wider(id_cols=PROLIFIC_PID:Loc, names_from = Info, values_from = Response) 
# n1b_subj_p2_loc # <- UNCOMMENT to view
```

##### (WIP) Network
```{r}
n1b_subj_p2_network <- n1b_subj_p2 %>% select(PROLIFIC_PID, contains("Community") | contains("Contacts")) %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(cols = 2:last_col(), names_to = c("Question", "Ethnicity"), names_sep=" - ", values_to = "Response") %>%
  filter(!is.na(Response)) %>% filter(Response!="0") %>%
  pivot_wider(id_cols=c(PROLIFIC_PID, Ethnicity), names_from = Question, values_from = Response, values_fill='0') %>% type_convert()
n1b_subj_p2_network <- n1b_subj_p2_network %>%
  full_join(n1b_subj_p2_network %>% group_by(PROLIFIC_PID) %>% summarize(total_CloseContacts = sum(CloseContacts, na.rm=TRUE))
  ) %>%
  mutate(prop_CloseContactsLocal = CloseContactsLocal/100,
        prop_CloseContacts = round(CloseContacts/total_CloseContacts, 2),
         prop_RegularContacts = RegularContacts/100,
         prop_BroaderCommunity = BroaderCommunity/100) %>%
  select(PROLIFIC_PID:Ethnicity, n_CloseContacts=CloseContacts, prop_CloseContactsLocal, prop_CloseContacts, prop_RegularContacts, prop_BroaderCommunity) %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.x), 0, .x)))
n1b_subj_p2_network

summary(n1b_subj_p2_network)
```

```{r}
# View per Ethnicity data summary
# Q1 -- selections for SocialCommunityNetwork (i.e., all undropped rows)
n1b_subj_p2_network %>% count(Ethnicity)
n1b_subj_p2_network %>% add_count(PROLIFIC_PID) %>% count(PROLIFIC_PID,n,  Ethnicity) %>% select(-nn)

# Q3 -- CloseContactsLocal
n1b_subj_p2_network %>% group_by(Ethnicity) %>% quick_summarize(prop_CloseContactsLocal, na.rm=TRUE)

# Q2 -- CloseContacts
n1b_subj_p2_network %>% group_by(Ethnicity) %>% quick_summarize(prop_CloseContacts, na.rm=TRUE)

# Q4 -- Regular Contacts
n1b_subj_p2_network %>% group_by(Ethnicity) %>% quick_summarize(prop_RegularContacts, na.rm=TRUE)

# Q5 -- BroaderCommunity
n1b_subj_p2_network %>% group_by(Ethnicity) %>% quick_summarize(prop_BroaderCommunity, na.rm=TRUE)
```

```{r}
# Get Asian-specific experience
# Filter to only East and Southeast Asian experience
n1b_subj_p2_asian <- n1b_subj_p2_network %>% filter(Ethnicity=="East Asian"|Ethnicity=="Southeast Asian") %>%
  mutate(n_CloseContactsLocal = n_CloseContacts*prop_CloseContactsLocal, .after=n_CloseContacts)
n1b_subj_p2_asian

# Total N for Asian (absolute)
n1b_subj_p2_asian %>% group_by(PROLIFIC_PID) %>% summarize(n_CloseContacts = sum(n_CloseContacts), n_CloseContactsLocal = round(sum(n_CloseContactsLocal)))

# Total prop for Asian (relative)
n1b_asian_network <- n1b_subj_p2_asian %>%
  # Get local prop of total, not just of subcategory/row
  mutate(prop_LocalCloseContacts = prop_CloseContactsLocal * prop_CloseContacts, .after=prop_CloseContactsLocal) %>%
  # Get total props across both east and southeast asian = "Asian"
  group_by(PROLIFIC_PID) %>% summarize(prop_AsianLocalCC = sum (prop_LocalCloseContacts), prop_AsianCC = sum(prop_CloseContacts), prop_AsianRC = sum(prop_RegularContacts), prop_AsianBC = sum(prop_BroaderCommunity)) %>%
  # Get unweighted total Asian estimate
  mutate(prop_Asian = (prop_AsianCC + prop_AsianRC + prop_AsianBC)/3)
n1b_asian_network
```

```{r}
# Get sample summary
n1b_asian_network %>% summarize(across(where(is.numeric), mean))

# # Q3 -- CloseContactsLocal
n1b_asian_network  %>% quick_summarize(prop_AsianLocalCC, na.rm=TRUE)
n1b_asian_network  %>% quick_summarize(prop_AsianCC, na.rm=TRUE)
n1b_asian_network  %>% quick_summarize(prop_AsianRC, na.rm=TRUE)
n1b_asian_network  %>% quick_summarize(prop_AsianBC, na.rm=TRUE)
n1b_asian_network  %>% quick_summarize(prop_Asian, na.rm=TRUE)
```
```{r}
n1b_asian_network %>%
  ggplot(aes(x=prop_Asian, y=prop_AsianCC, size=prop_AsianRC)) + gg_theme() +
  geom_point(alpha=0.2) 

n1b_asian_network %>%
  ggplot(aes(x=prop_AsianCC, y=prop_AsianRC, size=prop_AsianBC)) + gg_theme() +
  geom_point(alpha=0.2) + 
  labs(title="Asian Network", x="Close Contacts", y="Regular Contacts", size="Broader Community")

n1b_asian_network %>%
  ggplot(aes(x=prop_AsianBC, y=prop_AsianRC, size=prop_AsianCC)) + gg_theme() +
  geom_point(alpha=0.2) + 
  labs(title="Asian Network", x="Broader Community", y="Regular Contacts", size="Close Contacts")

n1b_asian_network %>%
  ggplot(aes(x=prop_AsianBC, y=prop_AsianCC, size=prop_AsianRC)) + gg_theme() +
  geom_point(alpha=0.2) + 
  labs(title="Asian Network", x="Broader Community", y="Close Contacts", size="Regular Contacts") 
```
Notes: 
- Most people seem to have a minimum proportion of Asian Close Contacts at 0.25 (one exception of 0).
- All correlate somewhat, but Regular and Broader seem most connected, then Regular and Close
- A number of people at all Asian contacts across the board
- There seems to be a bit of a split between 50% or more Asian in the Broader community linked to more Asian regular Contacts and Close Contacts — i.e., almost nobody has under 50% Close/Regular Contacts when large Community 

- Possibly can use CloseContacts as an estimate of interactive exposure + RegularContacts OR BroaderCommunity for a more general estimate, if needed. Otherwise, one estimate would probably be enough

#### (*C) By-Participant Check
```{r}
participant_list <- n1b_screener_v_prolific %>% filter(Run==CURRENT_RUN) %>%  pull(`Participant id`)
participant_list
```
Run the following to screen participant data. Mainly, check that they fit the demographic criteria.

Pilot = 1:8
Main1 = 8:
```{r}
# Print all in range
for (i in 1:length(participant_list)){
  current_participant <- participant_list[i]
    
  # UNCOMMENT TO view subj data by subject
  # print(n1b_subj_p2 %>% filter(PROLIFIC_PID == current_participant))
  
  # View locations by subject
  print(
    n1b_subj_p2_loc %>%  filter(PROLIFIC_PID == current_participant) %>%
    filter(!(is.na(`from Age`)))
  )
}
```

```{r}
# Check single participant
current_participant <- "610c239f6459fc6ffbc31764" # OR e.g. participant_list[3]

# View subj data by subject
n1b_subj_p2 %>% filter(PROLIFIC_PID == current_participant)

# View locations by subject
n1b_subj_p2_loc %>%  filter(PROLIFIC_PID == current_participant) %>%
  filter(!(is.na(`from Age`)))
```
<!-- UPDATE THIS STATEMENT AS NEEDED:  -->
<!-- All participants were born in, spent the majority of their childhood in, and were living in California* at the time. -->
All participants were born in and/or grew up in and were living in California* at the time (based on the screener).
```{r}
# Notes on Participant Locations (if needed).

# 63d007388da4ef5b301762f9 -- declined to answer
# 6365408f2a24d08a0f2a66ae -- declined to answer
# 5e6cade3d92ffb26677dbd84 -- born in NY but moved to CA at ~6 months old
# 6277fd1a3a086f8af7867fa0 -- born in CA, moved around a bit, mainly growing up in georgia (age 6-18), now back in CA (18-19)
```


### Main Data

Next, get just the response data with informative question labels. To be able to extract info from not only header but also first row, isolate the first row via slice(). Then, wrangle to get all the relevant info.

After that, isolate the actual data via slice (2nd row onwards) then pivot_longer and merge in by-question information (note: the original pivot_longer should match on both datatables). Drop unseen/unshown questions/blocks (i.e. NAs). Then, merge in the image codes!

NOTE: Can drop Question_Number and Question_Part_Number if not useful as shorthand.

```{r}
# Get informative question labels from header and first row

n1b_response_info <-  n1b_data %>%
  select(`1_Q3.1_1`:`30_Q3.12`) %>% select(-contains("DO")) %>% # select only looped trials, excl, DisplayOrder
     slice(1) %>%
  pivot_longer(cols=everything(), names_to=c("Photo_Number", "Question_Number", "Question_Part_Number", NA), names_sep="_", values_to = "q_label") %>%
  # Since number of dashes and <> location aren't consistent, need to remove everything between < > and extra - rather than just separate() or str_extract()
  mutate(q_label = gsub("<.*>", "", q_label)) %>%
  mutate(q_label = gsub("^(\\s*-\\s*)", "", q_label)) %>%
mutate(q_label = gsub("(-\\s+.*\\s+-)", "-", q_label)) %>%
  separate(q_label, into= c("Question", "Question_Part"), sep = "-", extra="merge")
n1b_response_info

# Get question data + merge with informative question labels
n1b_responses <- n1b_data %>%
  select(PROLIFIC_PID, `1_Q3.1_1`:`30_Q3.12`) %>% select(-contains("DO")) %>% # select only looped trials, excl, DisplayOrder
  slice(-1) %>% # remove second header
  pivot_longer(cols=2:last_col(), names_to=c("Photo_Number", "Question_Number", "Question_Part_Number", NA), names_sep="_", values_to = "Response") %>%
  full_join(n1b_response_info, .) %>% relocate(PROLIFIC_PID) %>%
  
  mutate(Response=ifelse(Response==-99,NA,Response)) %>% drop_na(Response) %>% # Drop unpresented + unanswered qs
  mutate(Response = case_when(Question=="Recognize" & Response=="2" ~ "No",
                              Question=="Recognize" & Response=="1" ~ "Yes",
                              TRUE ~ Response)) %>%
  type_convert() %>% # Auto-Convert number character columns to numeric
  full_join(., n1b_codes) %>% select(-Code) %>%
  mutate(across(where(is.character), as.factor))
n1b_responses
summary(n1b_responses)
# View(n1b_responses)
```

#### Summary
Make a targeted dataframe with just the useful/relevant columns for easier viewing.
```{r}
# Select relevant columns only.
n1b_responses_selected <- n1b_responses %>%
  select(PROLIFIC_PID, Question, Question_Part, Condition, Image_Cat, Image, Photo_Number, Response) %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(Response = case_when(Question=="Recognize" & Response=="2" ~ "No",
                              Question=="Recognize" & Response=="1" ~ "Yes",
                              TRUE ~ Response)) 
n1b_responses_selected
# View(n1b_responses_selected)
```

#### Check
Check and explore the data here.

Error check, i.e., manually view strange things in the data to fix them above
```{r}
# E.g., filter(is.na(Image)) ; filter(Response==-99)
# n1b_responses %>% filter(Question == "Ratings")
# 
# n1b_responses_selected %>% filter(Question == "Ratings")  %>% count(PROLIFIC_PID) %>% arrange(n)
# 
# # View one participant/slice
# n1b_responses_selected %>%  filter(PROLIFIC_PID=="6366fd2bd9ad6300b4cbc7df") %>% filter(Question=="Ratings") %>% count(Photo_Number, Image)
  # View()
```

```{r}
# Notes:
# PROLIFIC_PID=="6366fd2bd9ad6300b4cbc7df --- one missing rating for photo 15 / AAW-47 on Attractiveness (no way to fix, but noted — that's why total Ratings is -1; e.g. rather than 3360 it's 3359)
```


##### (*R) By-Sample Check

Sanity Check: Check the number of by-subject and by-photo counts. Does randomization seem to be working alright?
Can reset the randomization counts based on these numbers, so that started and unfinished surveys don't affect the final counts.
```{r}
# Check n of trials by subj and photo 
# For confirming even presentation (randomization) of photos

# Total presented/answered photos (end goal: 600)
n1b_responses %>% select(PROLIFIC_PID, Photo_Number) %>% distinct() %>% count()
# Total presented/answered photos per participant (process/end goal: 10 per subject)
n1b_responses %>% select(PROLIFIC_PID, Photo_Number) %>% distinct() %>% count(PROLIFIC_PID)
# Total presented/answered participants per photo (end goal: 20 per photo)
n1b_responses %>% select(PROLIFIC_PID, Photo_Number) %>% distinct() %>% count(Photo_Number) %>% arrange(Photo_Number)

```
```{r}
# Conditions, sample sizes, sample means, time taken, outliers(?)
# summary(n1b_responses)
summary(n1b_responses_selected)
```

##### (*C) By-Participant Check
Get the participants list.
```{r}
participant_list <- n1b_screener_v_prolific %>% filter(Run==CURRENT_RUN) %>%  pull(`Participant id`)
participant_list
length(participant_list)
```

Run the following iteratively to screen participant data. Mainly, check that they seemed to answer the questions in good faith. If not, add notes below. If it's all fine, then accept their submission.
```{r}
current_participant <- "6277fd1a3a086f8af7867fa0" #participant_list[10] # 

current_responses <- n1b_responses %>% 
  select(PROLIFIC_PID, Question, Question_Part, Image, Response) %>% 
  filter(PROLIFIC_PID == current_participant) %>%
  mutate(Response = case_when(Question=="Recognize" & Response=="2" ~ "No",
                              Question=="Recognize" & Response=="1" ~ "Yes",
                              TRUE ~ Response)) 

current_responses %>% filter(Question=="Impressions") %>% pivot_wider(id_cols = c(PROLIFIC_PID, Image), names_from = Question_Part, values_from = Response)

# Non-numeric responses
current_responses %>% filter(Question=="Age") %>%
  filter(Response!=0) %>% select(PROLIFIC_PID, Image, Question_Part, Response)  %>%
  pivot_wider(id_cols = c(PROLIFIC_PID, Image), names_from = Question_Part, values_from = Response)

current_responses %>% filter(Question=="Occupation") %>% select(-contains("Question_Part"))
current_responses %>% filter(Question=="Activities") %>% select(-contains("Question_Part"))
current_responses %>% filter(Question=="GrewUp") %>% select(-contains("Question_Part"))
current_responses %>% filter(Question=="Ethnicity") %>% select(-contains("Question_Part"))
current_responses %>% filter(Question=="Speech") %>% select(-contains("Question_Part"))
current_responses %>% filter(Question=="Recognize") %>% select(-contains("Question_Part"))

# Rating responses
current_ratings <- current_responses %>% filter(Question=="Ratings") %>% select(-"Question") %>% select(-PROLIFIC_PID) %>%
  mutate(Scale=str_extract(Question_Part, "(?<=:).+(?=$)")) %>% type_convert()

current_ratings %>%
  filter(Question_Part == str_match(Question_Part, ".*American.*|.*Native.*")) %>% 
  select(Image, Scale, Response) %>% pivot_wider(id_cols = Image, names_from = Scale, values_from = Response)

current_ratings %>% summary()
current_ratings %>% group_by(Image) %>% quick_summarize(Response, na.rm=TRUE)
current_ratings %>% group_by(Scale) %>% quick_summarize(Response, na.rm=TRUE)
current_ratings %>% quick_summarize(Response, na.rm=TRUE)

print(current_participant)
```

```{r}
# Response Patterns Review Notes

```

## Summarize & Explore
### Recognize
Sanity check that no people were recognized.
```{r}
n1b_responses_selected %>% filter(Question=="Recognize") %>% count(Response)
```
### (WIP) Descriptors
Try adding sentiments to each word entry: https://www.tidytextmining.com/sentiment.html
(See also: https://m-clark.github.io/text-analysis-with-R/sentiment-analysis.htm)
```{r}
library(tidytext)
n1b_descriptors <-
  n1b_responses_selected %>%
  filter(Question=="Impressions") %>%
  select(-Question:-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(Response)) %>%
  mutate(Response = mgsub(Response, c("-"), c(""), fixed=TRUE)) %>%
  mutate(Response = mgsub(Response, c("inetlligent", "independant"), c("intelligent", "independent"), fixed=TRUE)) %>% # typos
  # Add counts
  add_count(Image, name="total_image_n") %>% add_count(Response, name="total_word_n") %>% 
    add_count(Image, Response, name="image_word_n") %>%
  # Add sentiments
  rename(word=Response) %>% 
  left_join(get_sentiments("bing")) %>% left_join(get_sentiments("afinn"))  %>%
  rename(Response=word, sentiment_score=value) %>%
  # mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment),
         # sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score))
  mutate(sentiment = case_when((is.na(sentiment) & (sentiment_score > 0)) ~ "positive",
                               (is.na(sentiment) & (sentiment_score < 0)) ~ "negative",
                               TRUE ~ sentiment))
n1b_descriptors
```
```{r}
# Check for certain types of patterns to clean
# n1b_descriptors %>%  filter(Response == str_match(Response, ".* .*"))
```

#### Sentiment
```{r}
# Summarize sentiment data
n1b_descriptors %>% count(sentiment)
n1b_descriptors %>% count(sentiment_score)
n1b_descriptors %>% quick_summarize(sentiment_score, na.rm=TRUE)

# By group
n1b_descriptors %>% drop_na(sentiment) %>% add_count(Condition, name="group_n") %>% count(Condition, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n)
n1b_descriptors %>% group_by(Condition)  %>% quick_summarize(sentiment_score, na.rm=TRUE) %>% rename_with( ~ gsub("_score", "", .x))
```

```{r}
#By image
n1b_descriptors %>% drop_na(sentiment) %>% add_count(Image, name="group_n") %>% 
  count(Image, Condition, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n) %>%
  pivot_wider(id_cols=Image:Condition, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum)

n1b_descriptors %>% group_by(Image)  %>% quick_summarize(sentiment_score, na.rm=TRUE) %>% rename_with( ~ gsub("_score", "", .x))
```

#### Sets
```{r}
# Total descriptor words
n1b_descriptors %>% count(name="total_words")

# Total unique/different words
n1b_descriptors %>% count(Response) %>% count(name="unique_words")

# Total unique/different words per condition
n1b_descriptors %>% count(Condition,Response) %>% count(Condition, name="unique_words_per_condition")
```

```{r}
# Top words used
n1b_descriptors %>% count(Response, sort=TRUE)

# Top words used for a certain image
n1b_descriptors %>% count(Image, Response, sort=TRUE)

# Top words used for a certain condition's images
n1b_descriptors %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

```{r}
ethAsian_words <- n1b_descriptors %>% filter(Condition=="ethAsian") %>% count(Condition, Response, sort=TRUE)
msAsian_words <- n1b_descriptors %>% filter(Condition=="msAsian") %>% count(Condition, Response, sort=TRUE)
msWhite_words <- n1b_descriptors %>% filter(Condition=="msWhite") %>% count(Condition, Response, sort=TRUE)

# Shared words
Asian_words <- intersect(select(ethAsian_words, Response), select(msAsian_words,Response)) 
shared_words <- intersect(Asian_words, select(msWhite_words, Response)) 
ms_words <- intersect(select(msWhite_words, Response), select(msAsian_words,Response))
nonmsA_words <- intersect(select(msWhite_words, Response), select(ethAsian_words,Response))

shared_words %>% rename("Words in all personae"=Response)
nonmsA_words %>% setdiff(.,shared_words) %>% rename("Words only in both msWhite and ethAsian"=Response)
ms_words %>% setdiff(.,shared_words) %>% rename("Words only in Mainstream personae (both msWhite and msAsian)"=Response)
Asian_words %>% setdiff(.,shared_words) %>% rename("Words only in Asian personae (both ethAsian and msAsian)"=Response)
```
```{r}
# Different words
ethAsian_words_only <- setdiff(select(ethAsian_words, Response), select(msAsian_words, Response)) %>% setdiff(., select(msWhite_words, Response)) 
msAsian_words_only <- setdiff(select(msAsian_words, Response), select(ethAsian_words, Response)) %>% setdiff(., select(msWhite_words, Response)) 
msWhite_words_only <- setdiff(select(msWhite_words, Response), select(ethAsian_words, Response)) %>% setdiff(., select(msAsian_words, Response)) 

ethAsian_words_only %>% rename("Words in ethAsian only"=Response)
msAsian_words_only%>% rename("Words in msAsian only"=Response)
msWhite_words_only %>% rename("Words in msWhite only"=Response)
```

```{r}
# Show individual condition top words (excluding single occurances)
ethAsian_words %>% filter(n>1) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")
msAsian_words %>% filter(n>1) %>%ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msAsian adjectives", x= "Responses", y="No. of occurences")
msWhite_words %>% filter(n>1) %>%ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msWhite adjectives", x= "Responses", y="No. of occurences")
```
```{r}
# Show individual condition unique top words (excluding single occurances)
ethAsian_words %>% filter(n>1) %>% filter(Response %in% pull(ethAsian_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="ethAsian adjectives", x= "Responses", y="No. of occurences")
msAsian_words %>% filter(n>1)  %>% filter(Response %in% pull(msAsian_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msAsian adjectives", x= "Responses", y="No. of occurences")
msWhite_words %>% filter(n>1)  %>%  filter(Response %in% pull(msWhite_words_only)) %>% ggplot(aes(x=reorder(Response, n), y=n)) + geom_col() + coord_flip() + labs(title="msWhite adjectives", x= "Responses", y="No. of occurences")
```




### Age
Get proportion selections per age category for each image.
```{r}
 n1b_age <- n1b_responses_selected %>% 
  filter(Question=="Age") %>% 
  pivot_wider(id_cols = c(PROLIFIC_PID, Condition, Image), names_from = Question_Part, values_from = Response) %>% 
  mutate(across(Teens:`Over 40`, ~ as.numeric(as.character(.x)))) %>% 
  # Get proportions out of total responses (props add up to over 100 b/c multiple selections)
  add_count(Image) %>% group_by(Condition, Image, n) %>% 
  summarize(across(Teens:`Over 40`, sum)) %>% 
  mutate(across(Teens:`Over 40`, ~ .x/n)) %>%
  pivot_longer(Teens:`Over 40`, names_to = "Age", values_to = "prop") %>%
  mutate(Age=gsub("Teens", "10s", Age)) %>%
  # Drop values where none selected
  filter(prop>0) %>%  arrange(-prop, .by_group=TRUE) %>% 
  filter(prop>=0.75) # Option 1: Keep values where more than 75% of participants selected
  # slice_max(prop) # Option 2: Keep only highest value (including ties)
n1b_age <- n1b_age %>%
  full_join(
    # Get age consensus based on majority decision
   n1b_age %>% pivot_wider(Image:n, names_from = Age, values_from = prop) %>% 
   # relocate(`10s`, .before=`20s`) %>%
   mutate(across(2:last_col(), ~ ifelse(!is.na(.x), cur_column(), NA) )) %>%
   unite(perceived_age, 3:last_col(), sep=',', na.rm=TRUE)
  ) %>% ungroup
n1b_age
```

### Occupation
```{r}
n1b_occupation <-
  n1b_responses_selected %>% filter(Question=="Occupation") %>% 
  # text cleaning
  mutate(Response = tolower(Response)) 
  
n1b_occupation %>% count(Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_occupation %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

### Activities
```{r}
n1b_activities <-
  n1b_responses_selected %>% filter(Question=="Activities") %>% 
  # text cleaning
  mutate(Response = tolower(Response)) 
  # Split responses by comma, get multiple long responses per row/image, then count
  
n1b_activities %>% count(Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_activities %>% count(Condition, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

```

### (+) GrewUp
```{r}
n1b_grewup <-
  n1b_responses_selected %>% filter(Question=="GrewUp") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = str_trim(tolower(Response))) %>%
  mutate(Response = gsub("(.*)\\s*,\\s*usa", "\\1", Response)) %>% # keep everything before ', usa'
  mutate(Response = mgsub(Response, c("philipinnes|phillippines", "america|^us$|^in the us$|^united states$", "(?:^|\\W)(ca|cali)(?:$|\\W)", "(?:^|\\W)ny(?:$|\\W)", "(?:^|\\W)az(?:$|\\W)", "wisconson"), c("philippines", "usa", "california", "new york", "arizona", "wisconsin"))) %>% 
  # Add categorization into US or Asia or Europe
  mutate(Region = case_when(
    Response %in% c("san francisco", "los angeles", "boston", "beverly hills", "san fernando valley", "las vegas") ~ "USA",
    str_detect(Response, paste0("(?:^|\\W)", paste(tolower(state.name), collapse = '|'), "(?:$|\\W)")) ~ "USA",
    str_detect(Response, "(?:^|\\W)(mid(\\s*|-*)west|pacific northwest|(east|west) coast|the west|(west|east)ern states)(?:$|\\W)") ~ "USA", 
    str_detect(Response, "(?:^|\\W)usa(?:$|\\W)") ~ "USA", # if USA or <other>, defaults to US
    str_detect(Response,"(?:^|\\W)canada(?:$|\\W)") ~ "Canada",
    str_detect(Response, "(?<!\\w)(asia(n*))(?!\\w)") ~ "Asia",
    str_detect(Response, "(?<!\\w)europe(?!\\w)") ~ "Europe",
    TRUE ~ NA
  )) 
n1b_grewup <- n1b_grewup %>%
  mutate(CountryContinent = countrycode(sourcevar = Response,
                             origin = "country.name",
                             destination = "continent")) %>%
  cbind(CityCountry = world.cities[match(n1b_grewup$Response, tolower(world.cities$name)), ][[2]]) %>%
  
  mutate(Response_Cat = coalesce(coalesce(Region, CountryContinent), CityCountry), .after=Response) %>%
  mutate(across(where(is.character), as.factor))


# Check summary
summary(n1b_grewup)

# Check categorization
n1b_grewup %>% filter(Response_Cat == "USA") %>% count(Response_Cat, Response, sort=TRUE)
n1b_grewup %>% filter(Response_Cat == "Asia") %>% count(Response_Cat, Response, sort=TRUE)
n1b_grewup %>% filter(!(Response_Cat == "USA" | Response_Cat == "Asia")) %>% count(Response_Cat, Response, sort=TRUE)
# Check for NAs (uncategorized)
# n1b_grewup %>% filter(is.na(Region) & is.na(CountryContinent) & is.na(CityCountry) )
n1b_grewup %>% filter(is.na(Response_Cat) )
```

```{r}
# Check for errors
# world.cities %>% filter(country.etc=="USA") %>% mutate(name=tolower(name)) %>% filter(str_detect(name, "washington"))
```

```{r}
# Check data overall
# Specific Labels
n1b_grewup %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_grewup %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_grewup %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_grewup %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```
All personae categories sometimes identified as USA

msWhite only USA (or Canada), Europe
msAsian and ethAsian only USA (or Canada) or Asian. More msAsian = USA, and more ethAsian = Asia

```{r}
# Check Location category of photos per condition

n1b_grewup %>% filter(Condition=="msWhite") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_grewup %>% filter(Condition=="msAsian") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_grewup %>% filter(Condition=="ethAsian") %>% count(Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```

**Review Comments:**
msWhite photos were all identified as growing up in USA (or Canada) at least once (only other responses were Europe [n=2]; none were Asia).

msAsian photos were all identified as growing up in USA (or Canada) at least once, while some were also identified as from Asia (none were Europe)

ethAsian photos were variably identified as growing up in the USA or Asia (none were Europe)

**Selection Criteria:**
- msAsian must be at least identified as USA (TO CONSIDER: Mostly, select those ONLY USA because including Asia could bleed into ethAsian category, but this might depend more on the ratings)
- ethAsian must be identified as USA + Asia (not ONLY Asia, b/c that would be Asia-Asian; not ONLY USA, because that would be too msAsian)

### (+) Ethnicity
```{r}
n1b_ethnicity <-
  n1b_responses_selected %>% filter(Question=="Ethnicity") %>% select(-Question_Part) %>%
  # text cleaning
  mutate(Response = tolower(str_trim(Response))) %>%
  mutate(Response = mgsub(Response, c("china|chiense", "flippino|philipino|philipno"), c("chinese", "filipino"))) %>% 

  # Add categorization into race
  mutate(Asian = case_when(
    str_detect(Response, "(?:^|\\W)(asia(n*)|chinese|filipin(o|a))(?:$|\\W)") ~ "Asian",
    str_detect(Response, "(?:^|\\W)(hong kong|singapore|japan|korean|taiwan|vietnam|thai|hmong|lao|malaysian)") ~ "Asian",
    TRUE ~ NA
  )) %>%
  mutate(White = case_when(
    str_detect(Response, "(?:^|\\W)(white|european|caucasian|anglo|english|finnish|german|irish|italian|scandinavian|scottish|swedish)(?:$|\\W)") ~ "White",
    TRUE ~ NA
  )) %>%
  mutate(American = ifelse(str_detect(Response, "(?:^|\\W)american(?:$|\\W)"), "American", NA)) %>%
  # mutate(Other = case_when(
    # str_detect(Response, "(?:^|\\W)(mexi|hispanic)") ~ "Other",
    # str_detect(Response, "(?:^|\\W)(half|and|mixed)(?:$|\\W)") ~ "Mixed",
    # TRUE ~ NA )) %>%
  mutate(Response_Cat = coalesce(coalesce(coalesce(Asian, White), American), "Other"), .after=Response) %>% # if Asian mixed, Asian
  mutate(across(where(is.character), as.factor))
# n1b_ethnicity

# Check for NAs (uncategorized)
n1b_ethnicity %>% filter(is.na(Response_Cat) )

# Check summary
summary(n1b_ethnicity)

# Check categorization
n1b_ethnicity %>% filter(Response_Cat == "Asian") %>% count(Response_Cat, Response, sort=TRUE)
n1b_ethnicity %>% filter(Response_Cat == "White") %>% count(Response_Cat, Response, sort=TRUE)
n1b_ethnicity %>% filter(!(Response_Cat == "Asian" | Response_Cat == "White")) %>% count(Response_Cat, Response, sort=TRUE)
```

```{r}
# Check data overall
# Specific Labels
n1b_ethnicity %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_ethnicity %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_ethnicity %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_ethnicity %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```

```{r}
# Check  category of photos per condition
n1b_ethnicity %>% filter(Condition=="msWhite") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_ethnicity %>% filter(Condition=="msAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_ethnicity %>% filter(Condition=="ethAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```
**Review Comments:**
msWhite photos were all identified as White (or American) at least once (Other responses [n=2]).

msAsian photos were all identified as Asian only (exception: one photo also Other = 'hispanic')

ethAsian photos were  all identified as Asian only

**Selection Criteria:**
- msWhite must be identified as White only
- msAsian and ethAsian must be identified as Asian only
  - PLUS: should additional check East vs. Southeast asian representation within msAsian and ethAsian so that it isn't the difference (e.g., Southeast Asian in msAsian but not ethAsian)

To pick photos: Check a specific photo's words
```{r}
n1b_ethnicity %>% filter(Image=="AAW-12") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)
n1b_ethnicity %>% filter(Image=="AAW-36") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)
n1b_ethnicity %>% filter(Image=="AAW-47") %>% select(Image, Condition, Response_Cat, Response, PROLIFIC_PID)

```

### (+) Speech
```{r}
n1b_speech <- 
  n1b_responses_selected %>% filter(Question=="Speech") %>% select(-Question_Part) %>%
    # text cleaning
  mutate(Response = tolower(str_trim(Response)))  %>%
  # Consider: shorten long statements by extracting the keyword (e.g. straightforward)
# Add categorization into accentedness
  mutate(OtherAccent = case_when(
    str_detect(Response, "(?:^|\\W)((english|southern( drawl)*|mid(\\s*|-*)west(ern)*) accent)(?:$|\\W)") ~ "OtherAccent",
    TRUE ~ NA
  )) %>%
  mutate(Accented = case_when(
    str_detect(Response, "(?:^|\\W)(accented|(with|have) a(.+\\s)accent|(aapi|hawaiian|asian|foreign|heritage('s)*|chinese) accent|(broken|poor(-ish)*) english)(?:$|\\W)") ~ "Accented",
    TRUE ~ NA
  )) %>%
  mutate(Unaccented = case_when(
    str_detect(Response, "(?:^|\\W)(standard|unaccented|without an accent|no accent|(american|canadian|california|neutral) accent|typical american|(california) style|normal|perfect english|native speaker)(?:$|\\W)") ~ "Unaccented",
    # str_detect(Response, "(?:^|\\W)((fluent|clear) english)(?:$|\\W)") ~ "Unaccented",
    TRUE ~ NA
  )) %>%

  mutate(Response_Cat = coalesce(coalesce(coalesce(OtherAccent, Unaccented), Accented), "Other"), .after=Response) %>% 
  mutate(across(where(is.character), as.factor))
# n1b_speech

# Check for NAs (uncategorized)
n1b_speech %>% filter(is.na(Response_Cat) ) %>% select(Response:last_col())

# Check summary
summary(n1b_speech)

# Check categorization
n1b_speech %>% filter(Response_Cat == "Unaccented") %>% count(Response_Cat, Response, sort=TRUE)
n1b_speech %>% filter(Response_Cat == "Accented") %>% count(Response_Cat, Response, sort=TRUE)
n1b_speech %>% filter(!(Response_Cat == "Unaccented" | Response_Cat == "Accented")) %>% count(Response_Cat, Response, sort=TRUE)
```

Accentedness:
```{r}
# Check data overall
# Specific Labels
n1b_speech %>% count(Response_Cat, Response, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Response_Cat, Response, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)

# Categorized Labels
n1b_speech %>% count(Response_Cat, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```

```{r}
# Check category of photos per condition
n1b_speech %>% filter(Condition=="msWhite") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_speech %>% filter(Condition=="msAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)

n1b_speech %>% filter(Condition=="ethAsian") %>% count(Condition, Image, Response_Cat, sort=TRUE) %>% pivot_wider(names_from=Response_Cat, values_from = n)
```

```{r}
# Check specific photos/labels
n1b_speech %>% mutate(Response=as.character(Response)) %>% 
  filter(str_detect(Response, "(?:^|\\W)(south)"))
```
NOTE: WAW-1 was mentioned by 2 participants as possibly having a southern accent == good for my distractor stimuli (or avoid, because this photo will behave differently?)

#### Styles
```{r}
n1b_speech <- n1b_speech %>%
  # Other styles
  mutate(Style = case_when(
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(softly|soft spoken|quiet|mild|high)") ~ "Delicate", # possibly polite
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(deliberate|formal|polite)") ~ "Reserved",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(casual|conversational|slang)") ~ "Casual",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(fast|quick|rapid)") ~ "Fast",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(slow|drawl)") ~ "Slow",
    Response_Cat == "Other" & str_detect(Response, "(?:^|\\W)(direct|straight|straightforward|clear manner|concise)") ~ "Direct",
    TRUE ~ NA
  ))

# Check Styles data
n1b_speech %>% filter(Style == "Delicate") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Reserved") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Fast") %>% count(Style, Response, sort=TRUE)
n1b_speech %>% filter(Style == "Direct") %>% count(Style, Response, sort=TRUE)


# Categorized Labels
n1b_speech %>% count(Style, sort=TRUE) %>% add_count(name="n_unique")
n1b_speech %>% count(Condition, Style, sort=TRUE) %>% pivot_wider(names_from=Condition, values_from = n)
```



### (+) Ratings
```{r}
n1b_ratings <-
  n1b_responses %>%
  select(PROLIFIC_PID, Question_Number, Question, Question_Part, Condition, Image_Cat, Image, Response) %>%
  filter(Question=="Ratings") %>% select(-Question) %>% mutate(Response =  as.numeric(as.character(Response))) %>%
  # Convert scales and responses to consistent direction
  separate(Question_Number, into=c("Question_Number", "Version"), sep="-") %>%
separate(Question_Number, into=c(NA, "Rating_Group"), sep="\\.") %>% 
  mutate(Rating_Group = case_when(Rating_Group == "8" ~ "Style",
                                  Rating_Group == "9" ~ "Traits",
                                  Rating_Group == "10" ~ "Culture",
                                  TRUE ~ NA)) %>%
  separate(Question_Part, into=c("Left", "Right"), sep=":") %>%
  mutate(Rating_Scale = case_when(Rating_Group == "Style" & Version == "A" ~ Left,
                                  Rating_Group == "Traits" & Version == "A" ~ Right,
                                  Rating_Group == "Culture" & Version == "A" ~ Left,
                                  Rating_Group == "Style" & Version == "B" ~ Right,
                                  Rating_Group == "Traits" & Version == "B" ~ Left,
                                  Rating_Group == "Culture" & Version == "B" ~ Right,
                                  TRUE ~ NA), .before=(Rating_Group))  %>%
  mutate(Rating_Value = case_when(Rating_Group == "Style" & Version == "A" ~ abs((Response)-8),
                                  Rating_Group == "Traits" & Version == "B" ~ abs((Response)-8),
                                  Rating_Group == "Culture" & Version == "A" ~ abs((Response)-8),
                                  TRUE ~ Response), .before=(Response)) %>%
  select(-Version, -Left, -Right) %>% relocate(c(Rating_Group,Rating_Scale), .before=Rating_Value) %>%
  # Add z-score transformed ratings by participant (participant-normalized)
  group_by(PROLIFIC_PID) %>% 
  mutate(Rating_ZScore = scale(Rating_Value, center=TRUE, scale=TRUE), .before=Rating_Value) %>%
  mutate(Response_ZScore = scale(Response, center=TRUE, scale=TRUE)) %>% 
  ungroup() %>%
  mutate(across(where(is.character), as.factor))

summary(n1b_ratings)
n1b_ratings
```
Summary stats
```{r}
# Check summary stats of data overall
n1b_ratings %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
# n1b_ratings %>% summarize(min=min(Rating_ZScore), max=max(Rating_ZScore), mean=mean(Rating_ZScore), median=median(Rating_ZScore))

# by rating group
n1b_ratings %>% group_by(Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

# by condition
n1b_ratings %>% group_by(Condition, Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
n1b_ratings %>% group_by(Condition, Rating_Scale) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

# by image
n1b_ratings %>% group_by(Image, Rating_Group) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))
n1b_ratings %>% group_by(Image, Rating_Scale) %>% quick_summarize(Rating_Value) %>% rename_with(~ gsub("Rating_Value", "rating", .x))

```

Means per
```{r}
# Check  category of photos per condition + sd
# Tables
n1b_ratings %>% group_by(Condition, Rating_Group) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd) %>%
  pivot_wider(names_from=Condition, values_from = `Mean (SD)`)
  
n1b_ratings %>% group_by(Condition, Rating_Group, Rating_Scale) %>% 
  summarize(mean=mean(Rating_Value), sd=sd(Rating_Value)) %>% 
  mutate(str_mean = sprintf("%.2f", round(mean,2)), str_sd = sprintf("%.2f", round(sd,2))) %>%
  mutate(`Mean (SD)`=paste(str_mean, " (", str_sd, ")")) %>% # <- swap in paste0 for printing
  select(-mean:-str_sd) %>%
  pivot_wider(names_from=Condition, values_from = `Mean (SD)`)
```



Plot Functions:
```{r}
# Parallel Plot Function
# FUNCTION REF: https://stackoverflow.com/questions/52340768/using-a-custom-function-with-tidyverse 

parallel_plot <- function(df, x, y, group, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <-
    df %>% 
    group_by((!!group), (!!x)) %>% 
    
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group), group=(!!group)))  +
    stat_summary(fun.data=mean_se, geom="pointrange", position=position_dodge(), alpha=1) +
    stat_summary(fun.data=mean_se, geom="line", position=position_dodge(), alpha=1) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    theme_minimal()
    
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}

# Violin Plot Function
# FUNCTION REF: https://stackoverflow.com/questions/52340768/using-a-custom-function-with-tidyverse 

violin_plot <- function(df, x, y, group, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <-
    df %>% group_by((!!group), (!!x)) %>% 
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group)))  +
    geom_violin(alpha=0.5, position=position_dodge(0.95)) +
    geom_boxplot(fill="white", alpha=0.9, width=0.2, position=position_dodge(0.95)) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    scale_fill_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    theme_minimal()
  
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
     theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}
```


Parallel plots show the differences in score (=y, e.g., rating values) across score type (=x, e.g., rating scales) for each group (=series as represented by color and line, e.g., condition).

#### By Condition

Overview of ratings grouping scales under Rating_Group (Culture, Traits, Style)
```{r}
# Condition Mean Plots by Rating_Group
# Get plots: Zoomed in (Raw and ZScore)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition)

# Get plots: Zoomed out (Raw and ZScore)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition, full_scale = TRUE)
parallel_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition, full_scale = TRUE)
```
```{r}
# Condition Distribution + Median Violin Plots by RatingGroup
violin_plot(n1b_ratings, x=Rating_Group, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE)
violin_plot(n1b_ratings, x=Rating_Group, y=Rating_ZScore, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE)

```

Overview of all scales shown individually, reordered into helpful viewing order
```{r}
# Condition Mean Parallel Plots by Rating_Scale
# Get plots: Zoomed in (Raw and ZScore)
n1b_ratings_releveled <-
  n1b_ratings %>% 
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale, 
  "American", "Native speaker of English", "American-accented", "Aligned with American culture",  # Culture
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent",               # Traits
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker"))                         # Style

parallel_plot(n1b_ratings_releveled, x=Rating_Scale, y=Rating_Value, group=Condition, angle_axis_labels=TRUE) 
parallel_plot(n1b_ratings_releveled, x=Rating_Scale, y=Rating_ZScore, group=Condition, angle_axis_labels=TRUE) 
```

Notes:
- msAsian shouldn't be less attractive—pick matching where possible
- msWhite shouldn't be less likable—pick matching where possible
- msAsian shouldn't be more Friendly

- I think Casual shouldn't differ between MS groups, if possible
- I think Feminine shouldn't differ for msAsian only, if possible (but maybe that is part of the persona look?)


```{r}
# Condition Distribution + Median Violin Plots by Rating_Scale
n1b_ratings_releveled %>% filter(Rating_Group == "Culture") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n1b_ratings_releveled %>% filter(Rating_Group == "Traits") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)

n1b_ratings_releveled %>% filter(Rating_Group == "Style") %>%
  violin_plot(x=Rating_Scale, y=Rating_Value, group=Condition, full_scale = TRUE, angle_axis_labels = TRUE) + facet_grid(~Rating_Group)
```

#### By Image
```{r}
# Add all images mean plots to find outliers to drop (or matched pairs to keep)

```


```{r}
# Parallel Plot exploration
current_condition = "msWhite"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)

current_condition = "msAsian"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)

current_condition = "ethAsian"
n1b_ratings_releveled %>% filter(Condition == current_condition) %>%
  parallel_plot(., x=Rating_Scale, y=Rating_Value, group=Image, angle_axis_labels=TRUE) + labs(title=current_condition)
```


```{r}
# Add image direct comparision plots


```

## Process
### Get By-Image Data
Get general screening filters per image, merge into one.
```{r}
n1b_percepts_general <- 
  n1b_descriptors %>% group_by(Image) %>% summarize(sentiment_score=mean(sentiment_score, na.rm=TRUE)) %>%
  full_join(
    n1b_descriptors %>% drop_na(sentiment) %>% add_count(Image, name="group_n") %>% 
  count(Image, group_n, sentiment) %>% mutate(prop = n/group_n) %>% select(-group_n, -n) %>%
  pivot_wider(id_cols=Image, names_from = sentiment, values_from = prop, values_fill = 0, unused_fn=sum) %>%
  rename(positive_prop=positive, negative_prop=negative)
  ) %>%
  full_join(
    n1b_age %>% select(Image, perceived_age)
  )

# n1b_occupation
# n1b_activities
n1b_percepts_general
```

Get persona screening filters per image, merge into one.
```{r}
n1b_percepts_personae <- 
  n1b_grewup %>% count(Image, Response_Cat) %>% drop_na(Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, from_asia_prop=Asia, from_us_prop=USA) %>%
  full_join(
    n1b_ethnicity %>% count(Image, Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, asian_prop=Asian, white_prop=White)
  ) %>%
  full_join(
    n1b_speech %>% count(Image, Response_Cat) %>% add_count(Image, wt=n) %>% mutate(prop=n/nn) %>%
  pivot_wider(id_cols=Image, names_from = Response_Cat, values_from = prop, values_fill=0) %>% select(Image, accented_prop=Accented, unaccented_prop=Unaccented)
  )
n1b_percepts_personae

```


Get mean general+persona rating scores per image, to put through a PCA.
```{r}
# By-Image means: Get across-participant means per Rating_Scale for each Image 
n1b_ratings_means <- n1b_ratings_releveled %>% group_by(Condition, Image_Cat, Image, Rating_Group, Rating_Scale) %>%
  summarize(Rating_Value=mean(Rating_Value), Rating_ZScore = mean(Rating_ZScore)) %>% ungroup()

# Get factors as columns
# Raw rating values
n1b_ratings_vars_raw <-
   n1b_ratings_means %>%  pivot_wider(id_cols=Condition:Image, names_from = Rating_Scale, values_from =  Rating_Value)

# Zscores
n1b_ratings_vars_zscore <-
  n1b_ratings_means %>%  pivot_wider(id_cols=Condition:Image, names_from = Rating_Scale, values_from = Rating_ZScore)

# View data
n1b_ratings_means
n1b_ratings_vars_raw
n1b_ratings_vars_zscore
```

Check correlations between measures to assess collinearity.

Function to get correlation info:
```{r, fig.width=8, fig.height=7.5}
get_correlations <- function(df, columns_to_keep){
  # NOTE: Columns to keep cannot be name of column, only indices or function (e.g. last_col())
  corr_matrix <-
    df %>% select(columns_to_keep) %>%
    cor(., method="pearson")  # Alt: method="spearman"
  # # View the correlation matrix
  corr_df <- corr_matrix %>% as_tibble(rownames = "var") # independent variables correlation matrix 
  print(corr_df)
  # # Visualize
  corr_plot <- corrplot(rating_vars_cor,method='number',is.corr = T)
  print(corr_plot)
}
```

Raw rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n1b_ratings_vars_raw, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```
The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear. (OUTDATED: though Friendly and Likable are similar (>0.8) as well.)

ZScored rating values:
```{r, fig.width=8, fig.height=7.5}
get_correlations(n1b_ratings_vars_zscore, columns_to_keep=4:last_col())
# TBD - Include a correlation scatterplot to visualize specific variables. (see Visualize > Scored Data > Correlation Scatters)
```

The "Culture" Rating_Group, may be highly collinear (>0.8 and >0.9), which will result in a single value. Others are all not highly collinear.


### Run PCA
Notes for PCA Interpretation
```{r}
#############################################
## (1) Checking the number of dimensions

# Parallel Analysis
# Standard way to decide on the number of factors or components needed in an FA or PCA.

# Eigenvalues & percent variance accounted for
# One guideline is to only include dimensions that have an eigenvalue of at least 1, but note that...
# This guideline only applies if using correlation matrix (i.e. scaled units; scale.unit = T)
# Because we don't want a factor that accounts for less than what a single variable accounts for (single variable=1)

# Scree Plot
# One guideline is to check starting with the "elbow" value, plus or minus 1
# Check for where there is an "elbow" where the plot bends, such that subsequent factors don't contribute much

## (2) Interpreting the output factor values

# Factor matrix (raw eigenvectors = the factor score coefficients; sometimes called the factor, but not factor scores)
# To interpret, focus on the most extreme factor values (or loadings, below)
# Higher values means that those variables contribute more to the specific dimension/component/factor
# Dimensions/components/factors can be interpreted based on which variables contribute more

# Factor loadings (eigenvectors scaled by the square root of their associated eigenvalues)
# Provide similar information about which variables contribute to each dimension/component/factor, but also
# Can be interpreted as correlations between each variable and the factor
# One guideline treats all values less than 0.3 as 0, thus drops them from consideration (irrelevant for that factor) 

# Rotated factor matrix
# Orthogonally rotates factor matrix for ease of interpretation of each dimension

#############################################
## (3) Checking the individual coordinate scores
# Individual coordinate scores (principle coordinates)
# Same as factor scores for each subject and dimension (weighted sum of all of a subjects raw scores, where the weights are the eigenvector values)
# i.e. values are calculated from the normalized variable scores (Z-scores) multiplied by the eigenvector weights, then summed
```

#### Select Data
Select data for PCA.
```{r}
# Use Zscores to represent responses without participant bias in scale range 
#   (e.g., never selecting 1 means their lowest value is represented by 2, but their 2 can be equated to another person's 1)
# Use Raw scores to represent responses as representing true value 
#   (e.g., never selecting 1 means the true perceptual range represented by the photos never goes down to 1 for this person, and their 1 is equated to another person's 1)

current_source_data <- n1b_ratings_vars_zscore # Alt: n1b_ratings_vars_raw

## Maximal set
image_vars_pca <- current_source_data %>% select(-Condition:-Image)
image_vars_pca
```

#### Process PCA
Determine number of components via Parallel analysis.
```{r }
## Relevant libraries
# `PCA` command from `FactoMineR` library (see index for more info)
# `paran` command from `paran` library
# `Varimax` command from `GPArotations` library (https://stats.stackexchange.com/questions/59213/how-to-compute-varimax-rotated-principal-components-in-r)

## (1) Run Parallel Analysis with `paran`
# Standard way to decide on the number of factors or components needed in an FA or PCA.
# Prints out a scree plot as well, with the randomized line + unadjusted line
paran(image_vars_pca,
      graph = TRUE, color = TRUE, 
      col = c("black", "red", "blue"), lty = c(1, 2, 3), lwd = 1, legend = TRUE, 
      file = "", width = 640, height = 640, grdevice = "png", seed = 0)
```
Parallel analysis suggests 2 components retained.
Scree plots suggest ~3 components, based on the location of the elbow. Could try 2, 3, or 4 components.
Eigenvalues suggest 2 components, as only the first two comps have a value above 1.
The _difference in_ eigenvalues suggests 3 components, as up to the third comp has a difference greater than 1. (See: https://stats.stackexchange.com/questions/450752/understanding-how-many-components-to-include-for-pca)
Interpretability indicates 3 components, such that FrCA serves as its own component (Dim3).

```{r}
## (2) Run PCA with `FactoMineR`
# ncp = number of components; adjust after checking the parallel analysis output

# FactoMineR PCA Commands
#score_PCA        # lists commands
#score_PCA$var    # variables
#score_PCA$ind    # individuals
#score_PCA$call   # summary stats

# Conduct PCA with scaling/standardizing
score_PCA <- PCA(image_vars_pca, scale.unit = T, ncp =4, graph=T)

## Relevant Raw PCA Output
# Eigenvalues & percent variance accounted for
eigenvalues <- score_PCA$eig
as_tibble(eigenvalues, rownames="components")

# Eigenvectors (=Factor matrix, factor score coefficients, principal directions, principal axes; sometimes called the factor, but NOT factor scores; these are called loadings by some but its incorrect).
eigenvectors <- score_PCA$var$coord
as_tibble(eigenvectors, rownames="Score")

# Factor scores for each subject and dimension (also: Individual coordinate scores; principle coordinates)
rawScores <- score_PCA$ind$coord
as_tibble(rawScores, rownames="Item")

# Factor loadings (=loadings, correlation loadings, or scaled factor coefficients; eigenvectors scaled by the square root of their associated eigenvalues)
# Calculate factor loadings using the output eigenvectors and eigenvalues (i.e. divide each eigenvector column value by the appropriate eigenvalue square root).
rawLoadings <- sweep(eigenvectors,MARGIN=2,STATS=sqrt(eigenvalues[1:ncol(eigenvectors),1]),FUN="/") # margin 1=rows, 2=cols
as_tibble(rawLoadings, rownames="Score")
```




```{r lbqdata-pca2-3}
## (3) Conduct rotation on the PCA factor loadings with `GPArotation`
# Rotations are typically done on the retained component factor loadings, not on all components nor on the eigenvectors
# Performed for ease of interpretation, maximizing factor loadings
rotLoadings <- Varimax(rawLoadings, normalize=T)$loadings
as_tibble(rotLoadings, rownames="Score") 

# Recover Rotation matrix from loadings
# Because the rotLoadings are calculated from rawLoadings %*% rotMatrix, can recover rotMatrix by rotLoadings "divided" by rawLoadings, which in matrix multiplication is multiplying by the inverse (transpose) 
# Note: For some reason, can't call Varimax(rawLoadings)$rotmat (just get NULL); this recreates the same matrix from Varimax(rawLoadings)
rotMatrixL <- t(rawLoadings) %*% rotLoadings
as_tibble(rotMatrixL, rownames="Dimensions")

# Calculate rotated factor scores
# The formula simply multiplies the normalized variable scores with the rotation matrix to get rotated factor scores
# First, z-score the raw scores using base R scale()
# Then, matrix multiply the matrix of zScores with the rotation matrix
# Result is a matrix with columns=components and rows=each subject
zScores <- scale(rawScores)
rotScores <- zScores %*% rotMatrixL
as_tibble(rotScores, rownames="Item")

```

```{r}
# For comparison, a different rotation function. Requires normalization. Still a little different but similar enough. Provides rotmat and SS loadings. 
## Rotate eigenvectors (not typical, according to Stats notes and StackExchange, but reasonable-looking SS loadings+% variance)
# stats::varimax(eigenvectors)
## Rotate factor loadings (typical, but unreasonable(?)-looking SS loadings+% variance)
stats::varimax(rawLoadings)
```

#### Factor Loadings
```{r}
# Replace all factor loading values under |0.3| with 0 for better readability

# Raw Loadings
as_tibble(rawLoadings, rownames="Variable") %>% 
  mutate(across(where(is.numeric), ~ ifelse(abs(.x)<0.3, 0, .x)))

# Rotated Loadings
as_tibble(rotLoadings, rownames="Variable") %>% 
  mutate(across(where(is.numeric), ~ ifelse(abs(.x)<0.3, 0, .x)))
```

#### PCA: Raw Plots
```{r lbqdata-pca2-4, echo=F}
## (4) Data Visualization of Raw Scores with `factoextra`

# Plot individual factor scores
fviz_pca_ind(score_PCA, col.ind = "#00AFBB", repel = TRUE)

# Biplot, including individual scores and factor vectors
fviz_pca_biplot(score_PCA, label = "all", col.ind = "#00AFBB", col.var="black", ggtheme = theme_minimal())
```

#### PCA: Rotated Plots
```{r plbqdata-pca2-5, echo=F}
## (5) Manual Plots of Rotated Scores with `ggplot`

## Create dataframes of the rotated factor loading and factor score matrices

# Convert rotated factor loadings matrix to data frame; add variable number
rotLoadingsData <- as.data.frame(rotLoadings)
rotLoadingsData <- mutate(rotLoadingsData, variable = row.names(rotLoadings))
rotLoadingsData <- mutate(rotLoadingsData, variable = factor(variable))
#rotLoadingsData

# Convert rotated factor score matrix to data frame; add subject number
rotScoreData <- as.data.frame(rotScores)
rotScoreData <- mutate(rotScoreData, subject = 1:n())
rotScoreData

## Create base plots
# Loading plot
loadingplot <- ggplot(rotLoadingsData, aes(x=Dim.1, y=Dim.2))+
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4, y=Dim.2*4, label=variable), color="red",check_overlap=T) +
  scale_x_continuous(lim=c(-2.5, 2.5),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-2, 3),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Variables - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))
loadingplot


# Scatter plot of Individual factor scores
dimplot = ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  scale_x_continuous(lim=c(-2.5, 2.5),breaks=seq(-3,3,1)) +
  scale_y_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Individuals - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))
dimplot

## Merge loading and score plot = Biplot

# Biplot of factor loadings + ind factor scores
ggplot(rotScoreData, aes(x=Dim.1, y=Dim.2))+
  geom_point(na.rm=TRUE, color="#00AFBB") +
  geom_text(aes(label=subject),hjust=1.5,vjust=1.5, color="#00AFBB", check_overlap=T)+
  
  # Overlay loading plot (i.e. arrows)
  geom_segment(data=rotLoadingsData, mapping=aes(x=0, y=0, xend=Dim.1*4, yend=Dim.2*4), arrow=arrow(), size=0.5, color="black") +
  geom_text(data=rotLoadingsData, aes(x=Dim.1*4.5, y=Dim.2*4.5, label=variable), color="red",check_overlap=T, nudge_y = 0)+

  # scale_x_continuous(lim=c(-3.5, 3.5),breaks=seq(-3,3,1)) +
  # scale_y_continuous(lim=c(-4.5, 4.5),breaks=seq(-3,3,1)) +
  geom_hline(yintercept=0, linetype="dashed") +
  geom_vline(xintercept=0, linetype="dashed") +
  labs(title="Biplot - PCA", x="Dim 1", y="Dim 2") +
  theme_minimal()+
  theme(plot.title=element_text(size=15),
        plot.subtitle=element_text(size=15, face="italic"),
        axis.title=element_text(size=15),
        axis.text=element_text(size=14),
        strip.background =element_rect(fill="white"),
        strip.text = element_text(size=14))+
  theme(legend.title = element_text(size=16),
        legend.text=element_text(size=14))


```
~ PILOT NOTES as sample ~
NOTE: Both ZScore rotated and unrotated are very similar (for pilot data; however, big difference for Raw ratings)

The results of the Zscore rotated PCA with three components retained suggest that:
* Dim1 is the "culture" or "American" component, with all four of those American-related ratings being highly correlated.

* Dim2 is the "social attractiveness" or "likeability" component, linked to positive traits like Attractive, Likeable (strongest), Friendly, Confident, and Enthusiastic (when rotated), as well as confidence-related stylistic features like Cool and Fast speaker (i.e., non-Slow speaker)

* The other features that act more like their factors rather than a grouped/similar/collinear unit are:
  * Intelligent
  * Nerdy (vs Enthusiastic, when not rotated)
  * Casual
  * Feminine — definitely seems more like a third dimension
  

Based on the biplots, where is the best slice to take matched participants from? 
* Since Dim1 is the relevant distinguishing component, we'd want to take different personae across this dimension — i.e., this should differ significantly across Personae conditions (ethAsian vs. msAsian / msWhite)
* Since Dim2 (and others) are the "flavor" non-target characteristics to ensure social perceptual similarity on, those should be similar, maybe towards the mean around 0 (i.e., visually, along the horizontal x axis line is where we should pick from).

In addition, use the Ethnicity, GrewUp, and Speech scores to screen for outliers or non-matching aspects. So, we don't want to include even if they might match in ratings but: 
* e.g., differ in being East Asian vs. Southeast Asian or 
* e.g., differ in where they are from like always California vs. never California

#### Combine Data
```{r}
# Merge PCA factor scores back to image number and raw scores
n1b_ratings_vars_pca <-
  n1b_ratings_vars_zscore %>%
  cbind(
    as_tibble(rotScores, rownames="Item"), . # alternatively rawScores
  ) %>%
relocate(Condition:Image,
       Dim1=Dim.1, Dim2=Dim.2, Dim3=Dim.3, Dim4=Dim.4
       ) %>% select(-Item)
n1b_ratings_vars_pca
```

```{r}
# Summary stats of each PC/Dimension
quick_summarize(n1b_ratings_vars_pca, Dim1)
quick_summarize(n1b_ratings_vars_pca, Dim2)
quick_summarize(n1b_ratings_vars_pca, Dim3)
quick_summarize(n1b_ratings_vars_pca, Dim4)
```

#### Visualize Data
Function for labelled scatterplot of variables
```{r}
labeled_scatterplot <- function(df, x, y, label, group, show_points=TRUE, full_scale=FALSE, angle_axis_labels=FALSE) {
  x = enquo(x)
  y = enquo(y)
  label = enquo(label)
  group = enquo(group)
  min_y <- min(df %>% pull(!!y))
  max_y <- max(df %>% pull(!!y))

  plot <- df %>% group_by((!!group), (!!x)) %>% 
    ggplot(aes(x=(!!x), y=(!!y), color=(!!group), fill=(!!group), label=(!!label)))
  if (show_points == TRUE){
    plot <- plot + geom_point(alpha=0.7)
  }
  plot <- plot +
    geom_text(alpha=0.9, nudge_x = 0.2, nudge_y = 0.2) +
    scale_color_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    scale_fill_viridis(option="viridis", discrete=TRUE) + # <- UNCOMMENT to get colorblind-friendly palette
    gg_theme() + theme_minimal()
  
  if (full_scale == TRUE){
  plot <- plot + scale_y_continuous(limits = c(min_y, max_y), breaks = seq(floor(min_y), ceiling(max_y), 1)) 
  }
  if (angle_axis_labels == TRUE){
   plot <- plot + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
     theme(plot.margin = margin(0,0,0,40, unit = "pt"))  # pad left side of plot to read angled labels (t,r,b,l)  
  }
  plot
}
```

```{r}
# Get variable names
colnames(n1b_ratings_vars_pca)

# Double check raw zscore correlations for select variables
n1b_ratings_vars_pca %>% filter(Condition=="ethAsian") %>%
  ggplot(aes(y=American, x=`American-accented`)) +  geom_point() + geom_smooth(method="lm") + theme_minimal()

n1b_ratings_vars_pca %>% filter(Condition=="msAsian") %>%
  ggplot(aes(y=American, x=Likeable)) +  geom_point() + geom_smooth(method="lm") + theme_minimal()
```


```{r}
# Double check PCA correlations
n1b_ratings_vars_pca %>%
  ggplot() +
  # facet_wrap(~Image) +
  geom_point(aes(y=Dim1, x=American)) +
  geom_point(aes(y=Dim1, x=`Native speaker of English`), col="blue", alpha=0.7) +
  geom_point(aes(y=Dim1, x=`American-accented`), col="red", alpha=0.7) +
  geom_point(aes(y=Dim1, x=`Aligned with American culture`), col="orange", alpha=0.7) +
  theme_minimal()

```

Get plot of main Dimensions with the labelled images to review the selections below
```{r}
# Main Dimensions
# Specific Images colored by Condition
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim2, label=Image, group=Condition, show_points=FALSE)
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim3, label=Image, group=Condition, show_points=FALSE)
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Dim4, label=Image, group=Condition, show_points=FALSE)

n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim2, y=Dim3, label=Image, group=Condition, show_points=FALSE)
n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim2, y=Dim4, label=Image, group=Condition, show_points=FALSE)

n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim3, y=Dim4, label=Image, group=Condition, show_points=FALSE)

```
```{r}
# Other
# Specific Images colored by Condition
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Feminine, label=Image, group=Condition, show_points=FALSE)
# n1b_ratings_vars_pca %>% labeled_scatterplot(x=Dim1, y=Intelligent, label=Image, group=Condition, show_points=FALSE)
```

### Merge By-Image Data
```{r}
# Merge finalized ratings data with other processed and finalized data per image
n1b_byimage_data <-  n1b_ratings_vars_pca %>% full_join(n1b_percepts_personae) %>% full_join(n1b_percepts_general)
n1b_byimage_data
```
```{r}
colnames(n1b_byimage_data)
```


## Assess

### Find Outliers: Percepts
```{r}
# Ethnicity
n1b_byimage_data %>% filter(asian_prop > 0.5)
n1b_byimage_data %>% filter(white_prop >= 0.5)

# GrewUp
n1b_byimage_data %>% filter(from_asia_prop > 0.5)
n1b_byimage_data %>% filter(from_us_prop >= 0.5)

# Speech
n1b_byimage_data %>% filter(accented_prop > 0.2)
n1b_byimage_data %>% filter(unaccented_prop > 0.2)

# Descriptors
n1b_byimage_data %>% select(Condition:Image, sentiment_score) %>% mutate(mean=mean(sentiment_score), `sd`=sd(sentiment_score)) %>% filter(sentiment_score < mean-(3*sd) | sentiment_score > mean+(3*sd))
# n1b_byimage_data %>% select(Condition:Image, positive_prop) %>% mutate(mean=mean(positive_prop), `sd`=sd(positive_prop)) %>% filter(positive_prop < mean-(3*sd) | positive_prop > mean+(3*sd))
# n1b_byimage_data %>% select(Condition:Image, negative_prop) %>% mutate(mean=mean(negative_prop), `sd`=sd(negative_prop)) %>% filter(negative_prop < mean-(3*sd) | negative_prop > mean+(3*sd))
```

```{r}
# Ethnicity x Grewup x Speech
n1b_byimage_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, size=accented_prop, color=Condition)) + geom_point(alpha=0.7) + gg_theme()
n1b_byimage_data %>% ggplot(aes(x=asian_prop, y=from_asia_prop, color=Condition, label=Image)) + geom_text(alpha=0.7) + gg_theme() +
  scale_x_continuous(limits=c(-0.1, 1.1), breaks = c(0,0.25, 0.5, 0.75, 1))

# Descriptor Sentiment
n1b_byimage_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Condition)) + geom_point() + gg_theme()
n1b_byimage_data %>% ggplot(aes(x=sentiment_score, y=positive_prop, color=Condition, label=Image)) + geom_text() + gg_theme() 
```

### Find Outliers: Ratings
Check for Outliers with Mahalanobis distances for numerical ratings

* Use the Mahalanobis distance to assess outliers/matches via visual inspection and selecting a threshold cut-off

```{r}
# Get mahalnobis distances Function
get_mdist <- function(scores_df, index_df){
  # Finding the center point 
center  = colMeans(scores_df)
# Finding the covariance matrix
cov     = cov(scores_df)
# Calculate Mahalnobis distance + identify outliers
mdist_df <- index_df %>% 
  cbind(m_dist = mahalanobis(scores_df, center, cov))  %>%
  mutate(outlier = ifelse(m_dist > 6, TRUE, FALSE))
  # mutate(pvalue = pchisq(m_dist, df=3, lower.tail=FALSE)) #pval<0.001 outlier
return(mdist_df)
}
```

```{r}
# All orthogonal/unique variables
scores_df <- n1b_ratings_vars_pca %>% select(-Condition:-Image) %>% 
  select(Dim1, Dim2, Dim3, Dim4) #, Feminine, Casual, Intelligent, Nerdy, `Clear speaker`)
index_df <- n1b_ratings_vars_pca %>% select(Condition:Image)

# Get mahalnobis distances
mdist_df <- get_mdist(scores_df, index_df)

# Most different considering all variables
# See results
mdist_df %>% arrange(desc(m_dist))
# Get list of potential outliers to check
mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat)
```

```{r}
# All orthogonal/unique variables, excluding American
scores_df <- n1b_ratings_vars_pca %>% select(-Condition:-Image) %>% 
  select(Dim2, Dim3, Dim4) #, Feminine, Casual, Intelligent, Nerdy, `Clear speaker`)
index_df <- n1b_ratings_vars_pca %>% select(Condition:Image)

# Get mahalnobis distances
mdist_df <- get_mdist(scores_df, index_df)

# Most different (across conditions) excluding American
# See results
mdist_df %>% arrange(desc(m_dist))
# Get list of potential outliers to check
mdist_df %>% filter(outlier==TRUE) %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat)
```
```{r}
# Most similar (across conditions) excluding American
# See results
mdist_df %>% arrange((m_dist))
# Get list of potential matches to check
mdist_ranks <- mdist_df %>% select(Condition, Image, m_dist) %>% inner_join(n1b_ratings_vars_pca) %>% select(-Image_Cat) %>% 
  mutate(m_dist_bins = case_when(
    m_dist < 1 ~ "0-1", # 0-1
    m_dist < 2 ~ "1-2", # 1-2
    m_dist < 4 ~ "2-4", # 2-4
    m_dist < 6 ~ "4-6", # 4-6
    m_dist >= 6 ~ "6+", # 6+
    TRUE ~ NA
  ), .after=m_dist) %>%
  arrange((m_dist)) %>% group_by(Condition) %>%
  mutate(ingroup_ranks = order(order(m_dist, decreasing=FALSE)), .after=m_dist_bins) #%>% arrange(ingroup_ranks)
mdist_ranks

# Option 1: Pivot to matched rank columns, image value for condition based on rank
mdist_ranks %>%  pivot_wider(id_cols = ingroup_ranks, names_from = Condition, values_from = c(Image, m_dist))

# Option 2: Bin columns, image value for condition based on rank
mdist_ranks %>% count(Condition, m_dist_bins) %>% pivot_wider(names_from = Condition, values_from = n)
mdist_ranks %>% pivot_wider(id_cols = c(m_dist_bins, Image), names_from = Condition, values_from = c(m_dist))
```

### Get Narrowed Conditions

Select 4-5 photos 
* Based on Dim1 scores difference
* Based on outliers (mahalanobis scores)


TODO:
- Pull in screener categories; filter out
- How to get most similar rows across categories? Mahalanobis but smallest values
  - TODO: Try with fewer unique cols, just the most relevant/orthogonal seeming


```{r}
# # WIP simplified slice_mean function
# slice_mean <- function(df, col, n, mean_group=NULL, filter_group=NULL, filter_string=NULL){
#   col <- enquo(col)
#   if (!is.null(mean_group)){    mean_group <- enquo(mean_group)  }
#   if (!is.null(filter_group)){    filter_group <- enquo(filter_group)  }
#   if (is.null(mean_group)){
#     df <- df %>% mutate(mean=round(mean(!!col),7), .before=!!col) 
#   } else {
#     df <- df %>% group_by(!!mean_group) %>% mutate(mean=round(mean(!!col),7), .before=!!col) %>% ungroup()
#   }
#   if (!is.null(filter_group)){
#     df <- df %>% filter(!!filter_group==filter_string)
#   }
#   df %>% mutate(dist_mean = abs(mean-!!col), .before=!!col) %>%
#   slice_min(dist_mean, n=n)
# }
# 
# n1b_byimage_data %>% slice_mean(Dim1, n=5)
# n1b_byimage_data %>% slice_mean(Dim1, n=5, mean_group=Condition)
# n1b_byimage_data %>% slice_mean(Dim1, n=5, filter_group=Condition, filter_string="msAsian")
# n1b_byimage_data %>% slice_mean(Dim1, n=5, mean_group=Condition, filter_group=Condition, filter_string="msAsian")

```

```{r}
# rightmost = msWhite
n1b_msWhite <- n1b_byimage_data %>% filter(Condition=="msWhite") %>% slice_max(Dim1, n=10)

# leftmost = ethAsian
n1b_ethAsian <- n1b_byimage_data %>% filter(Condition=="ethAsian") %>% slice_min(Dim1, n=5)

# Options for msAsian
# leftmost within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% filter(Condition=="msAsian") %>% slice_min(Dim1, n=5)
# rightmost within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% filter(Condition=="msAsian") %>% slice_max(Dim1, n=5)
# closest to group mean  within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% 
  group_by(Condition) %>% mutate(mean=round(mean(Dim1),7)) %>% ungroup() %>% mutate(dist_mean = abs(mean-Dim1)) %>%
  filter(Condition=="msAsian") %>%
  slice_min(dist_mean, n=5) %>% select(-mean:-dist_mean)
# closest to grand mean  within category for msAsian
n1b_msAsian <- n1b_byimage_data %>% 
  mutate(mean=round(mean(Dim1),7)) %>% mutate(dist_mean = abs(mean-Dim1)) %>%
  filter(Condition=="msAsian") %>%
  slice_min(dist_mean, n=5) %>% select(-mean:-dist_mean)

n1b_selected_images <-  n1b_msWhite %>% rbind(n1b_ethAsian) %>% rbind(n1b_msAsian)
n1b_selected_images
```
 
```{r}
# Condition Mean Parallel Plots by Rating_Scale
n1b_selected_images %>% pivot_longer(American:Intelligent, names_to = "Rating_Scale", values_to = "Rating_ZScore") %>%
  # reorder factors  
  mutate(Rating_Scale = fct_relevel(Rating_Scale,
  "American", "Native speaker of English", "American-accented", "Aligned with American culture",  # Culture
  "Enthusiastic", "Confident", "Friendly", "Likeable", "Attractive", "Intelligent",               # Traits
  "Feminine", "Clear speaker","Cool", "Casual",  "Nerdy", "Slow speaker"))    %>%                      # Style
  parallel_plot(x=Rating_Scale, y=Rating_ZScore, group=Condition, angle_axis_labels=TRUE) 

# Dimensions
n1b_selected_images %>% pivot_longer(Dim1:Dim4, names_to = "Dimension", values_to = "Score") %>%
  parallel_plot(x=Dimension, y=Score, group=Condition, angle_axis_labels=FALSE) 
```
 
 
### Examine Distinctiveness / Test Differences
Function to get linear model output and pairwise comparisons
```{r}
# LM Output Comparisons Function
# **Reference Code:** 
# - https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html 
# - https://broom.tidymodels.org/reference/tidy.summary_emm.html 

lm_emms_pairs <- function(df, formula){
  # run linear model
  lm_out <- lm(formula, data=df)
  print( summary(lm_out) )
  print( lm_df <- tidy(lm_out) )
  
  # get marginal averages (estimated marginal means / Least-squares means)
  emms <- emmeans(lm_out, "Condition")
  print( emms_df <- tidy(emms, conf.int = TRUE) )
  
  # get contrasts
  # (?) tidy(contrast(emms)) # contrast from grand mean(?)
  # pwcs = contrast between paired groups; equivalent to: contrast(emms, method="pairwise") 
  #   +  bonferroni adjusted for multiple comparisons
  pwcs <- tidy(pairs(emms)) %>% mutate(bfrn.adj.p.value = tidy(pairs(emms, adjust="bonferroni")) %>% pull(adj.p.value))
  print(pwcs)
  
  # get effect sizes
  print( tidy(eff_size(emms, sigma = sigma(lm_out), edf = Inf)) )
  
  # plot confidence intervals
  #Option 1
  print( plot(emms, comparisons = TRUE))
  #Option 2
  confint_plot <- ggplot(emms_df, aes(Condition, estimate)) +
    geom_point() + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=0.5) + gg_theme()
  print( confint_plot )
}

# Output:
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
# (*) not currently sure about the `edf` parameter, currently speifying Inf, narrowing confint unrealistically
```

#### Dim1
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim1 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim1 ~ Condition)
```
#### Dim2
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim2 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim2 ~ Condition)
```
#### Dim3
Good, Dim3 are NOT different across conditions
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim3 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim3 ~ Condition)
```
#### Dim4
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Dim4 ~ Condition)
lm_emms_pairs(n1b_selected_images, Dim4 ~ Condition)
```

#### Feminine
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Feminine ~ Condition)
lm_emms_pairs(n1b_selected_images, Feminine ~ Condition)
```
#### Feminine
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Intelligent ~ Condition)
lm_emms_pairs(n1b_selected_images, Intelligent ~ Condition)
```

#### Nerdy
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Nerdy ~ Condition)
lm_emms_pairs(n1b_selected_images, Nerdy ~ Condition)
```
#### Casual
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, Casual ~ Condition)
lm_emms_pairs(n1b_selected_images, Casual ~ Condition)
```
#### Clear speaker
```{r}
# (1) LM summary, (2) tidy LM, (3) EMMs w/ CIs, (4) Pairwise comp, (5) Effect sizes*, (6) EMM comp plot, (7) EMM CI plot
lm_emms_pairs(n1b_ratings_vars_pca, `Clear speaker` ~ Condition)
lm_emms_pairs(n1b_selected_images, `Clear speaker` ~ Condition)
```
